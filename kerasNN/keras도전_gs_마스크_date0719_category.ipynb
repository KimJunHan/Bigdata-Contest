{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5bd838f861d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 인공신경망관련 패키지\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# 인공신경망관련 패키지\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "\n",
    "# 회귀분석 관련 패키지\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from scipy import stats\n",
    "\n",
    "# OLS회귀분석\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 시각화 패키지\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "# 데이터 처리를 위한 패키지\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# 다중공선성(multicollinearity) 처리를 위한VIF 확인 패키지\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# 한글 처리\n",
    "from matplotlib import rc, font_manager\n",
    "font_name = font_manager.FontProperties(fname='C:/Windows/Fonts/NanumGothicCoding.ttf').get_name()\n",
    "rc('font',family=font_name)\n",
    "\n",
    "# from matplotlib import rcParams\n",
    "# rcParams['font.family'] = 'NanumGothicCoding'\n",
    "\n",
    "# - 마이너스 사인 처리\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# DeprecationWarning경고 무시\n",
    "# 향후 안쓰일 함수들을 이용해서 만들어져 있기 때문에 필요하다 없으면, 사방이 붉어진다.\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# MAD 기반 예제코드\n",
    "def mad_based_outlier(points, thresh=3.5):\n",
    "    if len(points.shape) == 1:\n",
    "        points = points[:,None]\n",
    "    median = np.median(points, axis=0)\n",
    "    diff = np.sum((points - median)**2, axis=-1)\n",
    "    diff = np.sqrt(diff)\n",
    "    med_abs_deviation = np.median(diff)\n",
    "    modified_z_score = 0.6745 * diff / med_abs_deviation\n",
    "    return modified_z_score > thresh \n",
    "\n",
    "# 출처: https://pythonanalysis.tistory.com/7 [Python 데이터 분석]\n",
    "#########################################################################\n",
    "\n",
    "# 소셜 데이터 처리를 위한 함수\n",
    "# 1. 모든 소셜 데이터 column들의 첫번째는 : 날짜다.\n",
    "# 2. 각 소셜데이터는 social_키워드.블로그/트위터/뉴스/총합 으로 되어 있다.\n",
    "def changeColNames(d,before, after) : \n",
    "    # 컬럼이름 시리즈로 만들어 반환\n",
    "    # 통합하기 쉽게, 모든 데이터들의 날짜컬럼 이름을 date로 통일\n",
    "    new_col_names = ['date']\n",
    "    new_col_names.extend(list(d.columns)[1:])\n",
    "    d.columns = new_col_names\n",
    "    return pd.Series(d.columns).apply(lambda x : x.replace(before,after))\n",
    "\n",
    "#########################################################################\n",
    "# modeling 함수로 만들어 처리하기\n",
    "def linReg(df, item, cols_using):\n",
    "    cols = cols_using\n",
    "    X = df.loc[df['category']==item, cols]\n",
    "    y = df.loc[df['category']==item,'qty']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=0)\n",
    "\n",
    "    model = LinearRegression().fit(X_train, y_train)\n",
    "  \n",
    "    print('LinearRegression을 이용한 %s의 회귀분석 결과 :'%item)\n",
    "    print('훈련세트점수 : {:.2f}'.format(model.score(X_train, y_train)))\n",
    "    print('검증세트점수 : {:.2f}'.format(model.score(X_test, y_test)))\n",
    "\n",
    "    \n",
    "def ridgeReg(df, item, cols_using):\n",
    "    cols = cols_using\n",
    "    X = df.loc[df['category']==item,cols]\n",
    "    y = df.loc[df['category']==item,'qty']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=0)\n",
    "\n",
    "    ridge = Ridge(alpha=0.1, normalize=True, random_state=0, tol=0.001).fit(X_train, y_train)\n",
    "    \n",
    "    print('RidgeRegression을 이용한 %s의 회귀분석 결과 :'%item)\n",
    "    print('훈련세트점수 : {:.2f}'.format(ridge.score(X_train, y_train)))\n",
    "    print('검증세트점수 : {:.2f}'.format(ridge.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "def lassoReg(df, item, cols_using):\n",
    "    cols = cols_using\n",
    "    X = df.loc[df['category']==item,cols]\n",
    "    y = df.loc[df['category']==item,'qty']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=0)\n",
    "\n",
    "    lasso = Lasso(alpha=0.1, max_iter=1000).fit(X=X_train, y=y_train)\n",
    "  \n",
    "    print('LassoRegression을 이용한 %s의 회귀분석 결과 :'%item)\n",
    "    print('훈련세트점수 : {:.2f}'.format(lasso.score(X_train, y_train)) )\n",
    "    print('검증세트점수 : {:.2f}'.format(lasso.score(X_test, y_test)) )\n",
    "\n",
    "    #사용한 특성수\n",
    "    print('사용한 특성수 : {}'.format(np.sum(lasso.coef_ != 0)) )\n",
    "#########################################################################\n",
    "\n",
    "# 자료가 1일 1행이라는 전제하에\n",
    "# df길이를 이용하여 날짜수를 계산, 이후 2016년 1월1일을 1번째주 1일이라 기준하에\n",
    "# 몇번째 주인지 알려주는 컬럼 추가. 향후 주단위로 종합할때 스인다.\n",
    "def addDayWeek(df):\n",
    "    df_work = df.copy()\n",
    "    df_work['day'] = pd.Series(range(1,df_work.shape[0]+1)).astype('int64')\n",
    "    df_work['week'] = df_work['day'].apply(lambda x : math.ceil(x/7))\n",
    "    return df_work\n",
    "#########################################################################\n",
    "# 자료를 병합해주는 함수, 어떤 item인지 어느 컬럼을 기준으로 할지 받아서 병합\n",
    "def mergeForAnalysis(df1, df2, df3, item, on_what='date'):\n",
    "    merged_df = pd.merge(df1.loc[df1.category==item], df2, on=on_what, how='left')\n",
    "    merged_df = pd.merge(merged_df, df3, on=on_what, how='left')\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "def lowVIF(df, n=7, cols_using =['temp', 'cloud', 'wind','humid', 'hpa', 'sun_time', 'lgt_time', \n",
    "       'SO2', 'CO', 'O3', 'NO2', 'PM10', 'PM25'] ):\n",
    "    col_to_use = cols_using\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"VIF_Factor\"] = [variance_inflation_factor(\n",
    "        df[col_to_use].values, i) for i in range(df[col_to_use].shape[1])]\n",
    "    vif[\"features\"] = col_to_use\n",
    "    vif.sort_values(\"VIF_Factor\")\n",
    "    lowest_vif = vif.sort_values(\"VIF_Factor\")[:n].reset_index()\n",
    "    lowest_vif.drop(columns='index', inplace=True)\n",
    "    return lowest_vif\n",
    "\n",
    "#########################################################################\n",
    "# ols모델용 formula 생성\n",
    "def formulaGen(target, ind_features):\n",
    "    '''\n",
    "    formulaGen(목표컬럼명,[변수컬럼명1, 변수컬럼명2,...])\n",
    "    '''\n",
    "    custom_formula = target + \" ~ \"\n",
    "    for f in range(len(ind_features)):\n",
    "        custom_formula += ind_features[f]\n",
    "        if f!=(len(ind_features)-1):\n",
    "            custom_formula += \" + \"\n",
    "    return custom_formula\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기 (전처리 된 GS, 랄라블라, 날씨)\n",
    "gs = pd.read_csv('d:/project/contest/data/processed/p_gs.csv', parse_dates=['date'])\n",
    "lv = pd.read_csv('d:/project/contest/data/processed/p_lavla.csv', parse_dates=['date'])\n",
    "w = pd.read_csv('d:/project/contest/data/processed/p_wUVair_seoul_category.csv', parse_dates=['date'], index_col=0)\n",
    "sns_all = pd.read_csv('d:/project/contest/data/processed/social_all.csv', parse_dates=['date'])\n",
    "\n",
    "# GS/lv 서울시만\n",
    "gs_seoul = gs.loc[gs.pvn_nm =='서울특별시']\n",
    "lv_seoul = lv.loc[lv.pvn_nm =='서울특별시']\n",
    "w_seoul = w.loc[w['loc']==108]\n",
    "\n",
    "cols_to_keep = ['date','bor_nm','gender','age_cd','category','qty']\n",
    "\n",
    "# 일일, 구단위, 상품별 판매량 종합\n",
    "gs_grouped = gs_seoul[cols_to_keep].groupby(by=['date','bor_nm','category']).sum().reset_index()\n",
    "\n",
    "# 일단위로 자료 종합(qty는 일일 합계)\n",
    "day_gs_grouped = gs_grouped.groupby(by=['date','category']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '마스크'만 빼서 df생성\n",
    "item = '마스크'\n",
    "grouped_by = 'date'\n",
    "day_gs_grouped_w_item = pd.merge(day_gs_grouped.loc[day_gs_grouped.category==item],w_seoul,on='date',how='left')\n",
    "# day_gs_grouped_w_item.head(3)\n",
    "day_gs_grouped_w_sns_item = pd.merge(day_gs_grouped_w_item, sns_all,on='date',how='left')\n",
    "\n",
    "# 일단 uv = 자외선 지수는 결측치가 많아서 제외\n",
    "selected_cols = ['date', 'category', 'qty', 'temp', 'rain', 'cloud', 'wind','humid', 'hpa',\n",
    "                 'sun_time', 'lgt_time', 'snow','rain_or_not','snow_or_not',\n",
    "                 'SO2', 'CO', 'O3', 'NO2', 'PM10', 'PM25',\n",
    "                 'pm_blog', 'pm_twitter', 'pm_news', 'pm_total',\n",
    "                 'health_blog', 'health_twitter', 'health_news', 'health_total',\n",
    "                 'date_blog', 'date_twitter', 'date_news', 'date_total',\n",
    "                 'br_blog', 'br_twitter', 'br_news', 'br_total',\n",
    "                 'hobby_blog', 'hobby_twitter', 'hobby_news', 'hobby_total']\n",
    "gs_day_w = day_gs_grouped_w_sns_item[selected_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAD기법 적용할꺼면....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAD적용\n",
    "gs_day_w['outlier'] = pd.DataFrame(mad_based_outlier(gs_day_w['qty']))\n",
    "gs_day_w = gs_day_w.loc[gs_day_w.outlier==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VIF_Factor</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.715302</td>\n",
       "      <td>temp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.682044</td>\n",
       "      <td>PM10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.314379</td>\n",
       "      <td>cloud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.886587</td>\n",
       "      <td>lgt_time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.692697</td>\n",
       "      <td>wind</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VIF_Factor  features\n",
       "0    3.715302      temp\n",
       "1    4.682044      PM10\n",
       "2    6.314379     cloud\n",
       "3    7.886587  lgt_time\n",
       "4    8.692697      wind"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_col = ['temp', 'cloud', 'wind', 'lgt_time', 'PM10'] # +,'rain_or_not','snow_or_not'\n",
    "lowVIF(w,40,list_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = gs_day_w.loc[gs_day_w.date.between('2016-01-01','2017-12-31')]\n",
    "test_data = gs_day_w.loc[gs_day_w.date.between('2018-01-01','2018-12-31')]\n",
    "\n",
    "# 3년치 데이터 분리 : 종속변수('qty': 판매량)와 독립변수(판매량 제외 나머지 전부)\n",
    "# 날씨 데이터만, \n",
    "# 'temp', 'rain', 'cloud', 'wind','humid', 'hpa',\n",
    "# 'sun_time', 'lgt_time', 'snow','rain_or_not','snow_or_not',\n",
    "# 'SO2', 'CO', 'O3', 'NO2', 'PM10', 'PM25',\n",
    "combined = gs_day_w.loc[:,list_col+['rain_or_not','snow_or_not']]\n",
    "target = gs_day_w.loc[:,'qty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>cloud</th>\n",
       "      <th>wind</th>\n",
       "      <th>lgt_time</th>\n",
       "      <th>PM10</th>\n",
       "      <th>rain_or_not</th>\n",
       "      <th>snow_or_not</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>68.696500</td>\n",
       "      <td>비x</td>\n",
       "      <td>눈x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.7</td>\n",
       "      <td>7.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>53.618667</td>\n",
       "      <td>비x</td>\n",
       "      <td>눈x</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   temp  cloud  wind  lgt_time       PM10 rain_or_not snow_or_not\n",
       "0   1.2    7.0   1.6       2.1  68.696500          비x          눈x\n",
       "1   5.7    7.8   2.0       3.6  53.618667          비x          눈x"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy = pd.concat([target,combined], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cols_with_no_nans(df,col_type):\n",
    "    '''\n",
    "    Arguments :\n",
    "    df : The dataframe to process\n",
    "    col_type : \n",
    "          num : to only get numerical columns with no nans\n",
    "          no_num : to only get nun-numerical columns with no nans\n",
    "          all : to get any columns with no nans    \n",
    "    '''\n",
    "    if (col_type == 'num'):\n",
    "        predictors = df.select_dtypes(exclude=['object'])\n",
    "    elif (col_type == 'no_num'):\n",
    "        predictors = df.select_dtypes(include=['object'])\n",
    "    elif (col_type == 'all'):\n",
    "        predictors = df\n",
    "    else :\n",
    "        print('Error : choose a type (num, no_num, all)')\n",
    "        return 0\n",
    "    cols_with_no_nans = []\n",
    "    for col in predictors.columns:\n",
    "        if not df[col].isnull().any():\n",
    "            cols_with_no_nans.append(col)\n",
    "    return cols_with_no_nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수치형 자료 컬럼 갯수 : 5\n",
      "오브젝트형 자료 컬럼 갯수 : 2\n"
     ]
    }
   ],
   "source": [
    "num_cols = get_cols_with_no_nans(combined , 'num')\n",
    "cat_cols = get_cols_with_no_nans(combined , 'no_num')\n",
    "\n",
    "print ('수치형 자료 컬럼 갯수 :',len(num_cols))\n",
    "print ('오브젝트형 자료 컬럼 갯수 :',len(cat_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined = combined[num_cols + cat_cols]\n",
    "combined.hist(figsize = (12,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAykAAAM9CAYAAACYLTznAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3de5DlaVkn+O9DAw2jzaVRcERlcWUQpV03RJDt5uqINAgzC8ZyR9GxxhhgIHTdNdaR1V10dnGdARUGCnVkpBGM5RJcmos23TQiNrSAIxeBlUGFCUAaGwWZprry2T/y1FAUlZlvVeX55ZuZn0/EiTy/zF+e81SfONn5zed53191dwAAAGZxk70uAAAA4GRCCgAAMBUhBQAAmIqQAgAATEVIAQAApnLTvS4AAAAOi2Of/sjUW+ve7Gu+ufa6hkQnBQAAmIyQAgAATEVIAQAApmJNCgAALGXj+F5XsC/opAAAAFMRUgAAgKkY9wIAgKX0xl5XsC/opAAAAFMRUgAAgKkY9wIAgKVsGPcaoZMCAABMRUgBAACmIqQAAABTsSYFAAAW0rYgHqKTAgAATEVIAQAApmLcCwAAlmIL4iE6KQAAwFSEFAAAYCrGvQAAYCl29xqikwIAAExFSAEAAKZi3AsAAJaycXyvK9gXdFIAAICpCCkAAMBUjHsBAMBS7O41RCcFAACYipACAABMRUgBAACmYk0KAAAsZcOalBE6KQAAwFSEFAAAYCrGvQAAYCFtC+IhOikAAMBUhBQAAGAqxr0AAGApdvcaopMCAABMRUgBAACmYtwLAACWYnevITopAADAVIQUAABgKsa9AABgKRvH97qCfUEnBQAAmIqQAgAATMW4FwAALMXuXkN0UgAAgKkIKQAAwFSEFAAAYCrWpAAAwFI2rEkZoZMCAABMRUgBAACmYtwLAACWYgviITopAADAVIQUAABgKsa9AABgKXb3GqKTAgAATEVIAQAApmLcCwAAFtJ9fK9L2Bd0UgAAgKkIKQAAwFSMewEAwFJczHGITgoAADAVIQUAAJiKkAIAAEzFmhQAAFiKK84P0UkBAACmIqQAAABTMe4FAABLsQXxEJ0UAABgKkIKAAAwFeNeAACwlI3je13BvqCTAgAATGWtnZRjn/5Ir/PxWZ8bfvHpe10C5+Blv3urvS6Bc/Bd5/3tXpfAWXr3jd57+9mNtdcVcC5+7GMv9goeIMa9AABgKXb3GmLcCwAAmIqQAgAATMW4FwAALGXDuNcInRQAAGAqQgoAADAVIQUAAJiKNSkAALAUWxAP0UkBAACmIqQAAABTMe4FAABLsQXxEJ0UAABgKkIKAAAwFeNeAACwFONeQ3RSAACAqQgpAADAVIx7AQDAQrqP73UJ+4JOCgAAMBUhBQAAmIpxLwAAWIrdvYbopAAAAFMRUgAAgKkY9wIAgKW0ca8ROikAAMBUhBQAAGAqQgoAADCsqp5VVddU1fO2OefhVfWHVXVVVd31TJ/DmhQAAFjKPt+CuKouSnJed9+rqp5RVRd399tOOeeOSR6R5L7dfePZPI9OCgAAMOqSJJdX1WVJXr86PtVjk3w8yVuq6hfO5kmEFAAAIElSVUeq6tqTbkdOOeXCJJ/NZo64PsntTvMwd05yq+6+OMmNVfWgM63DuBcAACxl8i2Iu/tokqPbnHJ9klt392Oq6rtXx6f6XJLLV/dfneSBSd50JnXopAAAAKPemeQhq/uXro5P9UdJ7rO6f58k7z/TJxFSAACAId39jiQ3r6q3JrlTkitOc9ork3zz6py75ktdlWHGvQAAYCn7fHevJOnup558XFXnJ3lfkrt197Hu7iRPOpfn0EkBAADOWnffkOSe3X1stx5TSAEAAM5Jd39mNx/PuBcAACxl8t29ZqGTAgAATEVIAQAApmLcCwAAlnIAdvdagk4KAAAwFSEFAACYipACAABMxZoUAABYijUpQ3RSAACAqQgpAADAVIx7AQDAUlxxfohOCgAAMBUhBQAAmIpxLwAAWIrdvYbopAAAAFMRUgAAgKkY9wIAgKXY3WuITgoAADAVIQUAAJjK0LhXVX1VkqckuSjJh5I8p7s/u87CAADgwLG715DRTspvJ3l3kh9N8odJLtvqxKo6UlXXVtW1v/4ffmcXSgQAAA6T0YXzX9vdb1rd//2q+tmtTuzuo0mOJsmxT3+kz7E+AADgkBkNKR+qqucleVuSi5N8uKoenuR4d79ubdUBAMBBYnevIaMh5cok561u1yTpJLdNcuOa6gIAAA6p0ZDyF0kekuQWq+Mbu/un1lMSAABwmI2GlOckeUKSv1sdH19POQAAwGE3GlJek83F8J9MUkm+mORR6yoKAAAOJFsQDxkNKRcleVySv1kd+68LAACsxWhIuS7JM046vjHJkd0vBwAAOOxGQ8qTkzwmye27+5eq6g5rrAkAAA4m415DRq84/++zuVj+4avj31pLNQAAwKE3GlLu0N0vTnJsdXyL7U4GAAA4W6PjXh+tqkcnuUVVPTbJX62xJgAAOJi697qCfWG0k/KqJBckeXeSC5NctraKAACAQ220k/IT3f3AEwdVdWWSN66nJAAA4DDbNqRU1UuTnJ/koqp6xerTN0/y6XUXBgAAB47dvYZsG1K6+9HJZuekux+xTEkAAMBhNrom5ZfXWgUAAMDK0JqU7n7tugsBAIADz7jXkNFOCgAAwCKEFAAAYCpCCgAAMJXR66QAAADnqq1JGaGTAgAATEVIAQAApmLcCwAAlmIL4iE6KQAAwFSEFAAAYCrGvQAAYCnde13BvqCTAgAATEVIAQAApmLcCwAAlmJ3ryE6KQAAwFSEFAAAYCrGvQAAYCnGvYbopAAAAFMRUgAAgKkIKQAAwFSsSQEAgKW0NSkjdFIAAICpCCkAAMBUjHsBAMBCeqP3uoR9QScFAACYipACAABMxbgXAAAsxRXnh+ikAAAAUxFSAACAqRj3AgCApbiY4xCdFAAAYCpCCgAAMBXjXgAAsBQXcxyikwIAAExFSAEAAKZi3AsAAJbiYo5DdFIAAICprLWTcsMvPn2dD88anf+/PXuvS+Ac3P4lP7PXJXAO7vGpP97rEjhLr77tffa6BM5Bp/a6BGBFJwUAAJiKNSkAALAUa1KG6KQAAABTEVIAAICpGPcCAICltCvOj9BJAQAApiKkAAAAUzHuBQAAS7G71xCdFAAAYCpCCgAAMBXjXgAAsJQNu3uN0EkBAACmIqQAAABTMe4FAABLabt7jdBJAQAApiKkAAAAUxFSAACAqViTAgAAS7EF8RCdFAAAYCpCCgAAMBXjXgAAsJDesAXxCJ0UAABgKkIKAAAwFeNeAACwFLt7DdFJAQAApiKkAAAAUzHuBQAAS2m7e43QSQEAAKYipAAAAFMx7gUAAEuxu9cQnRQAAGAqQgoAADAV414AALCUDbt7jdBJAQAApiKkAAAAUxFSAACAqViTAgAAS7EF8RCdFAAAYCpCCgAAMBXjXgAAsJS2BfEInRQAAGAqQgoAADAV414AALAUu3sN0UkBAACmIqQAAABTMe4FAAAL6Q27e43QSQEAAKYipAAAAFMx7gUAAEuxu9cQnRQAAGAqQgoAADAVIQUAAJiKNSkAALAUa1KG6KQAAABTEVIAAICpbDvuVVVPyemDzPHufu56SgIAgAOqXXF+xE6dlD9Z3b41yTcm+WiSb0hy962+oaqOVNW1VXXtb/7Hv9itOgEAgENi205Kd781Sarqmd19n9WnX11Vb9nme44mOZokn/uJh1sZBAAAnJHR3b0+XVWPTPKWJPdL8sn1lQQAAAeU3b2GjC6cf3ySb0ry80nukOSJa6sIAAA41EZDypOSbCT5UDa7Lz+6tooAAIBDbXTc6z1Jzkty8yQPzGZgAQAAzkAb9xoyFFK6+w9OOvy9qnrRmuoBAAAOuaGQUlUPzWYnJdncgvjCtVUEAAAcaqPjXrfNl9avfCzJo9ZTDgAAHGDGvYaMLpz/3SSV5K5JLkhyw9oqAgAADrXRkHI0yS2T/HaSr07ygrVVBAAATKuqnlVV11TV87b4+q2q6veq6sqqek1V3e5Mn2M0pHxTdz+/u/+su1+Q5M5n+kQAAMD+VlUXJTmvu++V5BNVdfGp53T33yZ5cHc/IMlzk/yLM32e0TUpn6mqH8yXrjj/12f6RAAAcOht7PsreVyS5PKquizJs7N5eZK3nXpSdx+vqpuvzv+PZ/oko52UH07yjdm84vzXJ/mRM30iAABgblV1pKquPel25JRTLkzy2WzmiOuTnHaUq6r+aZK/zGZ2ePmZ1jF6nZTPJfm3Jz3pbZL8/Zk+GQAAMK/uPprN9ehbuT7Jrbv7MVX13avj0z3Oq5K8qqoeluRpSf7NmdQx2kk51RmnIQAAOPQ2eu7bzt6Z5CGr+5eujr9MVdVJh8eyRbdlO9t2UqrqrfnK9SeV5KIzfSIAAGB/6+53VNUTVjnhQ0meeZrTHlBVP5dkI8mnkvzYmT7PTuNeX+zuR5z6yaq68kyfCAAA2P+6+6knH1fV+Unel+Ru3X2su9+c5M3n8hw7hZQfWD3xrVZbiZ3wjHN5UgAAOJQO4BXnu/uGqrpndx/brcfcdk1Kd39hdfeVp3zp53erAAAAYH/r7s/s5uONLpy/+SnHt9jNIgAAAE4YvZjjFVX1rGx2VB6V5E3rKwkAAA6m7oM37rUOo9dJ+bmqenA2rxj5uu7+vfWWBQAAHFajnZR09xuSvGGNtQAAAIyHFAAA4BwdwN291uFsrzgPAACwFkIKAAAwFeNeAACwFONeQ3RSAACAqQgpAADAVIQUAABgKtakAADAQtqalCE6KQAAwFSEFAAAYCrGvQAAYCnGvYbopAAAAFMRUgAAgKkY9wIAgKVs7HUB+4NOCgAAMBUhBQAAmIpxLwAAWIiLOY7RSQEAAKYipAAAAFMx7gUAAEsx7jVEJwUAAJiKkAIAAExFSAEAAKZiTQoAACzFFeeH6KQAAABTEVIAAICpGPcCAICFuOL8GJ0UAABgKkIKAAAwFeNeAACwFLt7DdFJAQAAprLWTsrLfvdW63x41uj2L/mZvS6Bc/Dg9/7CXpfAOfjzBx3Z6xI4S+/5z/72t599+4XX7XUJwIpxLwAAWIjdvcb4kw8AADAVIQUAAJiKcS8AAFiK3b2G6KQAAABTEVIAAICpGPcCAICFtHGvITopAADAVIQUAABgKkIKAAAwFWtSAABgKdakDNFJAQAApiKkAAAAUzHuBQAAC7EF8RidFAAAYCpCCgAAMBXjXgAAsBTjXkN0UgAAgKkIKQAAwFSMewEAwELs7jVGJwUAAJiKkAIAAEzFuBcAACzEuNcYnRQAAGAqQgoAADAVIQUAAJiKNSkAALAQa1LG6KQAAABTEVIAAICpGPcCAICldO11BfuCTgoAADAVIQUAAJiKcS8AAFiI3b3G6KQAAABTEVIAAICpGPcCAICF9IbdvUbopAAAAFMRUgAAgKkY9wIAgIXY3WuMTgoAADAVIQUAAJiKkAIAAEzFmhQAAFhIty2IR+ikAAAAUxFSAACAqRj3AgCAhdiCeIxOCgAAMBUhBQAAmIpxLwAAWEhv2N1rhE4KAAAwFSEFAACYinEvAABYSPdeV7A/6KQAAABTEVIAAICpGPcCAICF2N1rzJYhpaqektN3Wo5393O3+b4jSY4kyRNuc8/c76vucs5FAgAAh8d2415/srp9a5JvTPLRJN+Q5O7bPWB3H+3ue3T3PQQUAADgTG3ZSenutyZJVT2zu++z+vSrq+oti1QGAAAHjHGvMSML5z9dVY+sqq+pqkcm+eS6iwIAAA6vkZDy+CTflOTnk9whyRPXWhEAAHCojYSUY0k+nOSaJJ9P8si1VgQAABxqI1sQvyrJ5dlcON9Jjq+zIAAAOKhccX7MSEi5SXf/2torAQAAyFhI+WhV/VySd2fVSenu1621KgAA4NAaCSlvX328zerjjWuqBQAADjRbEI/ZMaR094tOPq6qb19fOQAAwGE3srvXqX5116sAAABY2bKTUlXP6e6nVdWHk/zpiU8nufsilQEAwAHTbdxrxJYhpbuftrr7l939iBOfr6or114VAABwaI2Me11TVU+oqntU1Vcneey6iwIAAA6vkd29XpjkHyV5RJIHJPnaJN+yzqIAAOAg6o29rmB/GAkpP5nk0iRvSfIrSd621ooAAIBDbWTc66eTPDXJdUn+5yTvWmtFAADAoTbSSTma5GNJPpDk5auPAADAGdqwu9eQkYs5WigPAAAs5mwu5ggAALA2QgoAADCVkTUpAADALnDF+TE6KQAAwFSEFAAAYCrGvQAAYCG9YdxrhE4KAAAwFSEFAACYinEvAABYSPdeV7A/6KQAAABTEVIAAICpGPcCAICF2N1rjE4KAAAwFSEFAACYinEvAABYyEYb9xqhkwIAAExFSAEAAKZi3AsAABbSxr2G6KQAAABTEVIAAICpCCkAAMBUrEkBAICFdO91BfuDTgoAADAVIQUAAJiKcS8AAFiIK86P0UkBAACmIqQAAABTMe4FAAALccX5MTopAADAVIQUAABgKsa9AABgIS7mOEYnBQAAGFZVz6qqa6rqeedyznaEFAAAYEhVXZTkvO6+V5JPVNXFZ3POToQUAABYyEbX1LcBlyS5vKouS/L61fHZnLMtIQUAAEiSVNWRqrr2pNuRU065MMlns5kjrk9yu9M8zMg527JwHgAASJJ099EkR7c55fokt+7ux1TVd6+Oz+acba01pHzXeX+7zodnje7xqT/e6xI4B3/+oFP/6MF+8vVv2u7/Dczs7+/9lL0ugXPwxRv87RYGvDPJo5JckeTSJG8/y3O2ZdwLAAAW0l1T33auv9+R5OZV9dYkd8pmEDnjc3biTwYAAMCw7n7qycdVdX6S9yW5W3cfO905Z0onBQAAOGvdfUOSe54IKLtBJwUAABYyuM3vvtPdn9nNx9NJAQAApiKkAAAAUzHuBQAAC+m9LmCf0EkBAACmIqQAAABTMe4FAAALOai7e+02nRQAAGAqQgoAADAV414AALCQNu41RCcFAACYipACAABMRUgBAACmYk0KAAAsZGOvC9gndFIAAICpCCkAAMBUjHsBAMBCOrYgHqGTAgAATEVIAQAApmLcCwAAFrLRe13B/qCTAgAATEVIAQAApmLcCwAAFrJhd68hOikAAMBUhBQAAGAqxr0AAGAhLuY4RicFAACYipACAABMxbgXAAAsZGOvC9gndFIAAICpCCkAAMBUhBQAAGAq1qQAAMBCbEE8RicFAACYipACAABMxbgXAAAsxBbEY3RSAACAqQgpAADAVIx7AQDAQox7jdFJAQAApiKkAAAAUzHuBQAAC3ExxzE6KQAAwFSEFAAAYCrGvQAAYCEbpr2G6KQAAABTEVIAAICpCCkAAMBUrEkBAICFbNiCeIhOCgAAMBUhBQAAmMpQSKmqm1XVD1XVT62Ob7/esgAA4ODpyW+zGO2k/FaS40kevjp+0VYnVtWRqrq2qq59+ef+4hzLAwAADpvRkHKH7n5xkmOr41tsdWJ3H+3ue3T3PR751Xc65wIBAIDDZXR3r49W1aOT3LKqHpvkr9ZYEwAAHEgbe13APjHaSfnxJBckeVeS2yb5Z2urCAAAONRGQ8q9k3xzki8m+ZYkv7C2igAAgENtdNzrOUmekOTvVsfH11MOAAAcXBvlYo4jRkPKa5IcTfLJJJXNjsqj1lUUAABweI2GlIuSPC7J36yOrfkBAADWYjSkXJfkGScd35jkyO6XAwAAB9dMF0yc2VBI6e4fO/m4qu6+nnIAAIDDbnR3r1P9yq5WAQAAsLJtJ6WqntPdT6uqDyf50xOfTqKTAgAAZ8jC7jHbhpTuftrq7l929yNOfL6qrlxrVQAAwKE1Ou516SnHj9vtQgAAAJLxkPJ9pxxftNuFAAAAJOMh5SdPOf7p3S4EAAAOuo2a+zaLnRbOvzTJ+UnuXlWvyOai+Zsl+fQCtQEAAIfQTgvnH51sLpQ/eeE8AADAuoxecf6X11oFAAAcAhuZaKZqYkNrUrr7taf7fFVdvrvlAAAAh93ZXnH+hFvuShUAAAAro+NeW+ldqQIAAA4BvzyPOddOCgAAwK4615Dy97tSBQAAwMpQSKmqh55y/P1J0t0/sI6iAADgINrrizXul4s5uuI8AAAwFVecBwAApuKK8wAAsJCNvS5gnxgd93LFeQAAYBGj10m5TVU98aTjzya5trs/voaaAACAQ2w0pNwzyQVJrknyPUlukeSxVXVVd/+7dRUHAAAcPqMh5Tu6+/6r+8+vqjd194Oq6uokQgoAAAxwxfkxo2tSPlNVD6mq262umXLiv+9EuykDAAAHwWhIeWKSuyR5ZpKLkjy6qm6S5GnrKgwAADichsa9uvtzSZ5z4riqruju703yrnUVBgAAB81MV3Wf2U4Xc3xrkr8+9dPZ7KYAAADsup06KV883UUcq+rKNdUDAAAccjuFlB/Y4vP/+24XAgAAB50rzo/ZduF8d39hi89fvZ5yAACAw250dy8AAIBFjF7MEQAAOEfGvcbopAAAAFMRUgAAgKkY9wIAgIW0izkO0UkBAACmIqQAAABTEVIAAICpWJMCAAALsQXxGJ0UAABgKkIKAAAwFeNeAACwEONeY3RSAACAqQgpAADAVIx7AQDAQnqvC9gndFIAAICprLWT8u4bb7XOh2eNXn3b++x1CZyD9/xnf3/Yz/7+3k/Z6xI4S9/y9l/b6xI4Bze+/+q9LgFYMe4FAAAL2ai9rmB/8OdWAABgKkIKAAAwFeNeAACwEBdzHKOTAgAATEVIAQAApmLcCwAAFmLca4xOCgAAMBUhBQAAmIqQAgAATMWaFAAAWEjvdQH7hE4KAAAwFSEFAACYinEvAABYyEbtdQX7g04KAAAwFSEFAACYinEvAABYiCvOj9FJAQAApiKkAAAAUzHuBQAAC3ExxzE6KQAAwFSEFAAAYCrGvQAAYCEbBr6G6KQAAABTEVIAAICpCCkAAMBUrEkBAICFuOL8GJ0UAABgKkIKAAAwFeNeAACwEBsQj9FJAQAApiKkAAAAUzHuBQAAC7G71xidFAAAYCpCCgAAMBXjXgAAsJCN2usKdl9VPSvJ/ZL8cXf/iy3OuVWSl2czf3wuyQ9393VbPaZOCgAAcFaq6qIk53X3vZJ8oqouPt153f23SR7c3Q9I8twkpw0zJwgpAADA2bokyeVVdVmS16+OT6u7j1fVzVfnfHC7BxVSAABgIRvpqW9VdaSqrj3pduTk+qvq6VV11YlbkguTfDabueL6JLfb6t9eVf80yV8m+fpsjn5tSUgBAACSJN19tLvvcdLt6Clff3Z33//ELZvB5Nbd/Zgkt1kdb/XYr+rur0vyyiRP264OIQUAADhb70zykNX9S1fHX6GqTt4y4Fi26bgkQgoAACymJ7+d8b+n+x1Jbl5Vb01ypyRXbHHqA6rq6tWI2I8kedZ2j2sLYgAA4Kx191NP/VxVnZ/kfUnu1t3HuvvNSd48+pg6KQAAwK7q7huS3LO7j53N9wspAADAruvuz5zt9xr3AgCAhWzsdQH7hE4KAAAwFSEFAACYinEvAABYyMZZbfR7+OikAAAAUxFSAACAqRj3AgCAhRj2GqOTAgAATEVIAQAApmLcCwAAFuJijmN2DClVde8k90lyYZLrk7yzu69Yd2EAAMDhtO24V1X9WpLvTXJFkhcm+f0k/0NVvXCb7zlSVddW1bVXff7Du1osAABw8O3USfmu7r53Vb04SXX345JcW1VXb/UN3X00ydEk+fd3fLwNDAAAYMXFHMfsFFKurqpfSvIbSf62qh6Q5KFJ3rP2ygAAgENp25DS3f9rVd09ySX50pqUl3X3O5coDgAAOHx2XDjf3e9N8t4FagEAANg+pFTVt21xzvHuft96SgIAgIPJipQxO3VS/t8kr03y+VM+f2MSIQUAANh1O4WUf5Lkad39vyxRDAAAwE4L5z9cVf9yqWIAAOAgc8X5MTutSfmZJOdV1Zd9OsmN3f0L6ywMAAA4nHYa97r/6pzPJvlwkj9L8qEkn1pvWQAAwGG107jX9yVJVf3DJA9K8uAkv5jkpUmetvbqAADgAGn7ew3ZadzrD1bnXJfNDsrVSX49ySfWXxoAAHAY7TTu9YYk35bkc0lukuT2q5stiAEAgLXYKaRcnOTpSb4+yeO624gXAACcJbt7jbnJDl+/aXd/sLuvTHLnJQoCAAAOt506KV9XVQ/P5rbDt1/dT5Lj3f269ZYGAAAcRjuFlP8ryW1X9/+fk+7fuLaKAADggNqwu9eQnbYgvmypQgAAAJKd16QAAAAsSkgBAACmstOaFAAAYJdYkTJGJwUAAJiKkAIAAEzFuBcAACzEFsRjdFIAAICpCCkAAMBUjHsBAMBCNva6gH1CJwUAAJiKkAIAAEzFuBcAACyk7e41RCcFAACYipACAABMxbgXAAAsxO5eY3RSAACAqQgpAADAVIx7AQDAQuzuNUYnBQAAmIqQAgAATEVIAQAApmJNCgAALMQWxGN0UgAAgKkIKQAAwFSMewEAwEI22hbEI3RSAACAqQgpAADAVIx7AQDAQgx7jdFJAQAApiKkAAAAUzHuBQAAC9kw8DVEJwUAAJiKkAIAAEzFuBcAACykjXsN0UkBAACmIqQAAABTWeu41421zkdnnTpevP3s2y+8bq9L4Bx88QaTuPvVje+/eq9L4Bzc9Nvuu9clACv+TwgAAAvZ2OsC9gnjXgAAwFSEFAAAYCrGvQAAYCGuOD9GJwUAAJiKkAIAAEzFuBcAACzEFefH6KQAAABTEVIAAICpGPcCAICFuJjjGJ0UAABgKkIKAAAwFeNeAACwkG67e43QSQEAAKYipAAAAFMx7gUAAAvZcDHHITopAADAVIQUAABgKkIKAAAwFWtSAABgIa44P0YnBQAAmIqQAgAATMW4FwAALKRtQTxEJwUAAJiKkAIAAEzFuBcAACzEFefH6KQAAABTEVIAAICpGPcCAICFdBv3GqGTAgAATEVIAQAApmLcCwAAFrKx1wXsEzopAADAVIQUAABgKkIKAAAwFWtSAOvFUMMAAA0JSURBVABgIe2K80N0UgAAgKkIKQAAwFSMewEAwEI2jHsN0UkBAACmIqQAAABTMe4FAAAL6TbuNUInBQAAmIqQAgAATMW4FwAALMTuXmN0UgAAgKkIKQAAwFSMewEAwELauNcQnRQAAGAqQgoAADAVIQUAAJiKNSkAALCQDVecH6KTAgAATEVIAQAAprJjSKmqh55y/P3rKwcAAA6unvw2i5FOyk+ecvzT251cVUeq6tqquvbqz3/47CsDAAAOpS0XzlfVS5Ocn+TuVfWKJJXkZkk+vd0DdvfRJEeT5IXf8PiZAhkAALAPbBlSuvvRSVJVV3b3I5YrCQAADqaNqYaq5jUy7vXLa68CAABgZSSkvLGqfqiqfipJqur2a64JAAA4xEZCym8lOZ7k4avjF62tGgAAOMA20lPfZjESUu7Q3S9Ocmx1fIs11gMAABxyIyHlo1X16CS3rKrHJvmrNdcEAAAcYlvu7nWSH0/ypCTvSnLbJP9srRUBAMAB1T3PSNXMRjopj0pyQ5J3JPm7JJdW1R3XWhUAAHBojXRS7pXkgiTXJPmebK5JeWxVXdXd/26dxQEAAIfPSEj5ju6+/+r+86vqTd39oKq6OomQAgAAg2baQWtmI+Nen6mqh1TV7arqocl//S9ba6wLAAA4pEZCyhOT3CXJM5NclOQxVXWTJE9bZ2EAAMDhNDLu9YLuftxpPv+u3S4GAABgJKTc6eSDqvqG7v7YmuoBAIADq61JGTIy7vXyqvrnVfW1VXXrJC9Zd1EAAMDhNdJJeXKSt2Rz++Fkc30KAADAWoyElP+zu1904qCq7rvGegAA4MByxfkxIyHlFVX1pGxe0DFJjie5en0lAQAAh9nImpTnJ3lKko8kuXeSf7DWigAAgH2jqp5VVddU1fN2OO/hVfWHVXVVVd11u3NHQsodklyX5PXd/Zgk/2S8ZAAA4ISN9NS3M1VVFyU5r7vvleQTVXXxFufdMckjkty3u+/f3R/c7nFHQsonkrw2yS9V1YOSbJxZ6QAAwAF1SZLLq+qyJK9fHZ/OY5N8PMlbquoXdnrQLUNKVf2b1d0f6u5fyeY6lIuSPOpMqgYAAPaHqjpSVdeedDtyytefvhrXuqqqrkpyYZLPZjNXXJ/kdls89J2T3Kq7L05y46r5saXtFs5/Z5J09/HVx1eN/MMAAIDTm313r+4+muToNl9/dpJnnziuqicnuXV3P6aqvjubQeV0Ppfk8tX9Vyd5YJI3bfU824WUi6rqFUkq+bIBtWPdrZsCAAC8M5uTVlckuTTJ27c474+S3CfJVauP79/uQbcLKe/t7keccZkAAMCh0N3vqKonVNVbk3woyTO3OPWVSR62Ou+9SZ6z3eNuF1K2XXEPAACcmbPZQWt23f3UUz9XVecneV+Su3X3sd6cc3vS6GNuuXC+u3/8rKoEAAAOte6+Ick9u/vY2Xz/yBbEAAAAZ6S7P3O23yukAAAAU9luTQoAALCL+gCuSVkHnRQAAGAqQgoAADAV414AALCQjcmvOD8LnRQAAGAqQgoAADAV414AALAQu3uN0UkBAACmIqQAAABTMe4FAAALsbvXGJ0UAABgKkIKAAAwFeNeAACwELt7jdFJAQAApiKkAAAAUzHuBQAAC7G71xidFAAAYCpCCgAAMBUhBQAAmIo1KQAAsBBbEI/RSQEAAKYipAAAAFMx7gUAAAuxBfEYnRQAAGAqQgoAADCVai2ns1ZVR7r76F7Xwdnx+u1fXrv9zeu3f3nt9jev3xy++Wv++6l/+f7Ip99de11DopNyro7sdQGcE6/f/uW129+8fvuX125/8/qxbwgpAADAVOzuBQAAC+ne2OsS9gWdlHNjrnN/8/rtX167/c3rt3957fY3rx/7hoXzAACwkDvf7r+b+pfv/3Tdn0yxcN64FwAALGQjU2eUaRj3AgAApiKk7KCqXrbXNXB2quofVNWbq+pt5/g4962qp+5WXZyZ2vSrZ3C+12uXncl7afRn5qnnnenrzO6rqsdX1Xur6oqqemNV/eyJ17yqfr+qfnZ1/y5V9YGquvtJ3/usqrqmqp63V/UfFlX1gqq6qqo+vfr47DU9j99/2FNCys7O3+sCODvd/ffd/cAkf32OD3WTJOftQkmchd50JqHD67XLzvC9NPoz88vOO4vXmd130yT/qru/N8nTk3xbkpud9LXzquq81ddet/pcquqiJOd1972SfKKqLl688kOku/95d98/yR909/27++lreiq//7CnhJQtVNXDquqqJJes/lLxsKr6jdVfE3+nqs6vql+vqldV1f9RVe+pqvtV1ROq6jer6g9WX7tgr/8th0lV/fzqv/2VVXWb03z9gqp6+eo1/Q+r1/EJVfWw1dfvU1VPraqvXr1+b0vyqMX/IYfU6r11QVU9t6q+uar+9eo99+cnnfPDq78kvrGq3lBVt/B6LWv1Gr2yqq6uql898R469WfmFt97up+t55/mdT7dz9ebnfpzeJl/8aH0D5N8Lsl1VfX1ST6bJN19vLufvPraCZckubyqLkvy+tUxC6mqm1bVZasO2Euq6tarn5N/XlU/UVVvO+ln66nv2694T42+lzl73T31bRZCyha6+zUn/6Uimz+wr1j9NfHyJI/J5l+RfiLJP07yhCSXZvMvuF/s7kuSPD/Jk5ev/nCqqkuTfFV3X9LdD+ju609z2o8lednqNf2jbL6O5+VLf3k/cf9Iksu6++Ikn1978Zzw8ST3TfKQJHdN8mer99yfnnLeJ7v7+7P5Xvy+eL2W9qNJfqO775vkgmz+Ff3LfmZ292tO942nO6+7bzjN63y6n69Pylf+HGZ3PbOqrkzy+CT/JcmHkjwoyYe3+Z4LsxlibpLk+iS3W3eRfEl335jkHUk6yd2S/GB3/1aStyT5Qndf3N2PyWnetznNe2r0vQzrJqSM+84k/3L114UfT3KH1ec/k+RTSf4uyS1Wnzsxt31Vku9YrsRD76Ikr9zhnLtk8wd3klyZ5B+d8vWbn+a8t+5KdYz4QJIHZPO//X1Xx6fz7tXH/5TNX5C8Xsv61iTXrO5fs92J5+jUn69b/Rxm9/yr1R95fiTJddkMKQ9O8sFtvuf6JLde/SJ8m9UxC6mq/zHJnbMZ5J+RzQCSbP6O94qTTj3d+9Z7imkJKTs7MY/7gSS/tvqrwsXd/X9v8z0n5nEvyeYPeJbxniT/0w7nfDCbv/wmyf2y+bpen+SOq889cPXxfSedZ3RhOe/P5mvwe9kMK3+2xXl90seK12tpf5nkXqv733PK126WMaPnnexMfg6zOz6Y5Huz/f/L3pnN7mey+YvyO9ddFF/mv03yhu4+luRxp3zthpPun+59u9176mzeowzYSE99m4WQsrMvVNVbkvx/Sb5/NZ/5xqr6uiQ3JtlIcizJ8dUtSW5WVVdnc1ThOXtR9GHU3W9Kcn1VvX31Ot3vpLnaK6vqwiQvTPKDq3GGeyZ5aTZ/Ib60qn47m7/0Hk/ym0keVVVvTXKrfOm1Zb0+kOS/yWY38vZJLjhlNvqifPl77cR9r9canfpeSvKCbP719feT3DKr9QorX6iqt6zGL7fzX8+rqjue5nU+3c/XF+Yrfw6ze05+b504/mQ2f05+6jRfO54k3f2OJDdfvf/ulOSKRarl2OrjS5L8TFX9YTa7zMer6n7Z7IC9tqq+aXXe6d63272nRt/LsBauOL/LquqHk1zf3a/a61oA1qGqzuvu41VVSX43yU9091/tdV3A1rxv5/ENF9596l++P/aZ97ri/AF16l+iAA6a76yqE13i39nqF52qel2SrzrpU/+lux+89uqA0xl637J+GgRjdFIAAGAhd7ztt0/9y/fH/+Z9U3RSrEkBAACmYtwLAAAWsmGKaYhOCgAAMBUhBQAAmIpxLwAAWEhPdMHEmemkAAAAUxFSAACAqQgpAADAVKxJAQCAhbiQ+hidFAAAYCpCCgAAMBXjXgAAsJANWxAP0UkBAACmIqQAAABTMe4FAAALsbvXGJ0UAABgKkIKAAAwFeNeAACwkA3jXkN0UgAAgKkIKQAAwFSMewEAwELs7jVGJwUAAJiKkAIAAEzFuBcAACxkI8a9RuikAAAAUxFSAACAqQgpAADAVKxJAQCAhdiCeIxOCgAAMBUhBQAAmIpxLwAAWMiGca8hOikAAMBUhBQAAGAqxr0AAGAh7YrzQ3RSAACAqQgpAADAVIx7AQDAQuzuNUYnBQAAmIqQAgAATMW4FwAALKSNew3RSQEAAKYipAAAAFMRUgAAgKlYkwIAAAtxxfkxOikAAMBUhBQAAGAqxr0AAGAhtiAeo5MCAABMRUgBAACmYtwLAAAWYtxrjE4KAAAwFSEFAACYinEvAABYiGGvMTopAADAVIQUAABgKmWHAQAAYCY6KQAAwFSEFAAAYCpCCgAAMBUhBQAAmIqQAgAATEVIAQAApvL/A/o3LgbQZ5XFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x1080 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = train_data[num_cols + cat_cols]\n",
    "train_data['Target'] = target\n",
    "\n",
    "C_mat = train_data.corr()\n",
    "fig = plt.figure(figsize = (15,15))\n",
    "\n",
    "sb.heatmap(C_mat, vmax = .8, square = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류형(category형) 컬럼 수정 전, 총 7 개의 columns이 있었습니다.\n",
      "분류형(category형) 컬럼 수정 후, 총 9 개의 columns이 있었습니다.\n"
     ]
    }
   ],
   "source": [
    "def oneHotEncode(df,colNames):\n",
    "    for col in colNames:\n",
    "        # 해당 컬럼의 데이터 타입이 object란 소리는 숫자가 아니다 = 분류형 데이터\n",
    "        if( df[col].dtype == np.dtype('object')):\n",
    "            # 더미 컬럼 생성\n",
    "            dummies = pd.get_dummies(df[col],prefix=col)\n",
    "            # 원본 데이터에 이어 붙이기 axis=1 컬럼방향으로 \n",
    "            df = pd.concat([df,dummies],axis=1)\n",
    "\n",
    "            # 기존의 str형 컬럼 삭제\n",
    "            df.drop([col],axis = 1 , inplace=True)\n",
    "    return df\n",
    "    \n",
    "\n",
    "print('분류형(category형) 컬럼 수정 전, 총 {} 개의 columns이 있었습니다.'.format(combined.shape[1]))\n",
    "combined = oneHotEncode(combined, cat_cols)\n",
    "print('분류형(category형) 컬럼 수정 후, 총 {} 개의 columns이 있었습니다.'.format(combined.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>cloud</th>\n",
       "      <th>wind</th>\n",
       "      <th>lgt_time</th>\n",
       "      <th>PM10</th>\n",
       "      <th>rain_or_not_비o</th>\n",
       "      <th>snow_or_not_눈o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>68.6965</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   temp  cloud  wind  lgt_time     PM10  rain_or_not_비o  snow_or_not_눈o\n",
       "0   1.2    7.0   1.6       2.1  68.6965               0               0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 비o,비x는 하나가 1이면 다른하나는 0은 완저히 베타적인 관계이므로 한개를 삭제\n",
    "# 눈o눈x,도 비와 같다.\n",
    "combined.drop(['rain_or_not_비x'],axis = 1 , inplace=True)\n",
    "combined.drop(['snow_or_not_눈x'],axis = 1 , inplace=True)\n",
    "combined.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.columns = ['temp', 'cloud', 'wind', 'lgt_time', 'PM10', 'rain_or_not_o',\n",
    "       'snow_or_not_o']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qty</th>\n",
       "      <th>temp</th>\n",
       "      <th>cloud</th>\n",
       "      <th>wind</th>\n",
       "      <th>lgt_time</th>\n",
       "      <th>PM10</th>\n",
       "      <th>rain_or_not_o</th>\n",
       "      <th>snow_or_not_o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>266</td>\n",
       "      <td>1.2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>68.6965</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qty  temp  cloud  wind  lgt_time     PM10  rain_or_not_o  snow_or_not_o\n",
       "0  266   1.2    7.0   1.6       2.1  68.6965              0              0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy = pd.concat([target,combined], axis=1)\n",
    "Xy.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(730, 7)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (combined.shape[0] * 2)//3 첫 2/3 : 730\n",
    "# 하루정도 차이 있겠지만, 2년/1년\n",
    "cut_line = (combined.shape[0] * 2)//3\n",
    "def split_combined():\n",
    "    global combined\n",
    "    train = combined[:cut_line]\n",
    "    test = combined[cut_line:]\n",
    "\n",
    "    return train , test \n",
    "  \n",
    "train, test = split_combined()\n",
    "\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 128)               1024      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 165,889\n",
      "Trainable params: 165,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 신경망 모델 생성\n",
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = train.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "# optimizer에 여러 방식이 있다.\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GS-마스크date-Weights-{epoch:03d}--{val_loss:.5f}-cat02-vf05.hdf5'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인공 신경망에 의해 생성된 weight 자료를 저장하기 위해서\n",
    "checkpoint_name = 'GS-'+item+grouped_by+'-Weights-{epoch:03d}--{val_loss:.5f}-cat02-vf05.hdf5' \n",
    "\n",
    "# save_best_only값이 저장되어, 모든 weight값을 저장하지 않고, val_loss값이 줄어들때마다(적을수록 좋다.) 저장\n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "checkpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xxxyyzzz.hdf5 자료가 주워 졌음 ↓ 요건 실행 안해도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 549 samples, validate on 138 samples\n",
      "Epoch 1/500\n",
      "549/549 [==============================] - 0s 632us/step - loss: 324.9247 - mean_absolute_error: 324.9247 - val_loss: 219.6005 - val_mean_absolute_error: 219.6005\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 219.60052, saving model to GS-mad적용마스크date-Weights-001--219.60052-cat02-vf05.hdf5\n",
      "Epoch 2/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 174.5848 - mean_absolute_error: 174.5848 - val_loss: 161.0678 - val_mean_absolute_error: 161.0678\n",
      "\n",
      "Epoch 00002: val_loss improved from 219.60052 to 161.06776, saving model to GS-mad적용마스크date-Weights-002--161.06776-cat02-vf05.hdf5\n",
      "Epoch 3/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 150.3362 - mean_absolute_error: 150.3362 - val_loss: 151.5492 - val_mean_absolute_error: 151.5492\n",
      "\n",
      "Epoch 00003: val_loss improved from 161.06776 to 151.54916, saving model to GS-mad적용마스크date-Weights-003--151.54916-cat02-vf05.hdf5\n",
      "Epoch 4/500\n",
      "549/549 [==============================] - 0s 74us/step - loss: 140.4942 - mean_absolute_error: 140.4942 - val_loss: 147.2286 - val_mean_absolute_error: 147.2286\n",
      "\n",
      "Epoch 00004: val_loss improved from 151.54916 to 147.22855, saving model to GS-mad적용마스크date-Weights-004--147.22855-cat02-vf05.hdf5\n",
      "Epoch 5/500\n",
      "549/549 [==============================] - 0s 98us/step - loss: 134.7406 - mean_absolute_error: 134.7406 - val_loss: 132.4524 - val_mean_absolute_error: 132.4524\n",
      "\n",
      "Epoch 00005: val_loss improved from 147.22855 to 132.45243, saving model to GS-mad적용마스크date-Weights-005--132.45243-cat02-vf05.hdf5\n",
      "Epoch 6/500\n",
      "549/549 [==============================] - 0s 80us/step - loss: 133.1256 - mean_absolute_error: 133.1256 - val_loss: 143.3769 - val_mean_absolute_error: 143.3769\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 132.45243\n",
      "Epoch 7/500\n",
      "549/549 [==============================] - 0s 78us/step - loss: 132.1828 - mean_absolute_error: 132.1828 - val_loss: 166.2093 - val_mean_absolute_error: 166.2093\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 132.45243\n",
      "Epoch 8/500\n",
      "549/549 [==============================] - 0s 78us/step - loss: 130.3264 - mean_absolute_error: 130.3264 - val_loss: 142.4320 - val_mean_absolute_error: 142.4320\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 132.45243\n",
      "Epoch 9/500\n",
      "549/549 [==============================] - 0s 82us/step - loss: 128.8552 - mean_absolute_error: 128.8552 - val_loss: 130.4264 - val_mean_absolute_error: 130.4264\n",
      "\n",
      "Epoch 00009: val_loss improved from 132.45243 to 130.42640, saving model to GS-mad적용마스크date-Weights-009--130.42640-cat02-vf05.hdf5\n",
      "Epoch 10/500\n",
      "549/549 [==============================] - 0s 82us/step - loss: 128.3435 - mean_absolute_error: 128.3435 - val_loss: 126.6479 - val_mean_absolute_error: 126.6479\n",
      "\n",
      "Epoch 00010: val_loss improved from 130.42640 to 126.64790, saving model to GS-mad적용마스크date-Weights-010--126.64790-cat02-vf05.hdf5\n",
      "Epoch 11/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 126.5100 - mean_absolute_error: 126.5100 - val_loss: 134.8493 - val_mean_absolute_error: 134.8493\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 126.64790\n",
      "Epoch 12/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 127.0707 - mean_absolute_error: 127.0707 - val_loss: 148.1973 - val_mean_absolute_error: 148.1973\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 126.64790\n",
      "Epoch 13/500\n",
      "549/549 [==============================] - 0s 78us/step - loss: 127.0932 - mean_absolute_error: 127.0932 - val_loss: 142.4289 - val_mean_absolute_error: 142.4289\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 126.64790\n",
      "Epoch 14/500\n",
      "549/549 [==============================] - 0s 74us/step - loss: 125.2408 - mean_absolute_error: 125.2408 - val_loss: 138.9702 - val_mean_absolute_error: 138.9702\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 126.64790\n",
      "Epoch 15/500\n",
      "549/549 [==============================] - 0s 78us/step - loss: 127.1189 - mean_absolute_error: 127.1189 - val_loss: 110.8054 - val_mean_absolute_error: 110.8054\n",
      "\n",
      "Epoch 00015: val_loss improved from 126.64790 to 110.80544, saving model to GS-mad적용마스크date-Weights-015--110.80544-cat02-vf05.hdf5\n",
      "Epoch 16/500\n",
      "549/549 [==============================] - 0s 77us/step - loss: 129.0775 - mean_absolute_error: 129.0775 - val_loss: 109.9397 - val_mean_absolute_error: 109.9397\n",
      "\n",
      "Epoch 00016: val_loss improved from 110.80544 to 109.93975, saving model to GS-mad적용마스크date-Weights-016--109.93975-cat02-vf05.hdf5\n",
      "Epoch 17/500\n",
      "549/549 [==============================] - 0s 88us/step - loss: 123.5577 - mean_absolute_error: 123.5577 - val_loss: 126.7923 - val_mean_absolute_error: 126.7923\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 109.93975\n",
      "Epoch 18/500\n",
      "549/549 [==============================] - 0s 76us/step - loss: 123.9567 - mean_absolute_error: 123.9567 - val_loss: 136.9059 - val_mean_absolute_error: 136.9059\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 109.93975\n",
      "Epoch 19/500\n",
      "549/549 [==============================] - 0s 82us/step - loss: 124.2276 - mean_absolute_error: 124.2276 - val_loss: 105.5280 - val_mean_absolute_error: 105.5280\n",
      "\n",
      "Epoch 00019: val_loss improved from 109.93975 to 105.52804, saving model to GS-mad적용마스크date-Weights-019--105.52804-cat02-vf05.hdf5\n",
      "Epoch 20/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 126.5341 - mean_absolute_error: 126.5341 - val_loss: 99.1561 - val_mean_absolute_error: 99.1561\n",
      "\n",
      "Epoch 00020: val_loss improved from 105.52804 to 99.15612, saving model to GS-mad적용마스크date-Weights-020--99.15612-cat02-vf05.hdf5\n",
      "Epoch 21/500\n",
      "549/549 [==============================] - 0s 73us/step - loss: 128.1275 - mean_absolute_error: 128.1275 - val_loss: 121.8869 - val_mean_absolute_error: 121.8869\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 99.15612\n",
      "Epoch 22/500\n",
      "549/549 [==============================] - 0s 76us/step - loss: 123.5172 - mean_absolute_error: 123.5172 - val_loss: 122.9177 - val_mean_absolute_error: 122.9177\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 99.15612\n",
      "Epoch 23/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 125.2108 - mean_absolute_error: 125.2108 - val_loss: 124.9250 - val_mean_absolute_error: 124.9250\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 99.15612\n",
      "Epoch 24/500\n",
      "549/549 [==============================] - 0s 93us/step - loss: 122.2103 - mean_absolute_error: 122.2103 - val_loss: 120.5859 - val_mean_absolute_error: 120.5859\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 99.15612\n",
      "Epoch 25/500\n",
      "549/549 [==============================] - 0s 91us/step - loss: 124.3299 - mean_absolute_error: 124.3299 - val_loss: 134.7793 - val_mean_absolute_error: 134.7793\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 99.15612\n",
      "Epoch 26/500\n",
      "549/549 [==============================] - 0s 80us/step - loss: 123.9252 - mean_absolute_error: 123.9252 - val_loss: 108.9843 - val_mean_absolute_error: 108.9843\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 99.15612\n",
      "Epoch 27/500\n",
      "549/549 [==============================] - 0s 73us/step - loss: 125.3277 - mean_absolute_error: 125.3277 - val_loss: 128.5971 - val_mean_absolute_error: 128.5971\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 99.15612\n",
      "Epoch 28/500\n",
      "549/549 [==============================] - 0s 89us/step - loss: 122.4544 - mean_absolute_error: 122.4544 - val_loss: 124.5073 - val_mean_absolute_error: 124.5073\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 99.15612\n",
      "Epoch 29/500\n",
      "549/549 [==============================] - 0s 78us/step - loss: 121.9269 - mean_absolute_error: 121.9269 - val_loss: 157.6720 - val_mean_absolute_error: 157.6720\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 99.15612\n",
      "Epoch 30/500\n",
      "549/549 [==============================] - 0s 82us/step - loss: 124.7972 - mean_absolute_error: 124.7972 - val_loss: 152.3930 - val_mean_absolute_error: 152.3930\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 99.15612\n",
      "Epoch 31/500\n",
      "549/549 [==============================] - 0s 91us/step - loss: 122.1342 - mean_absolute_error: 122.1342 - val_loss: 130.1081 - val_mean_absolute_error: 130.1081\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 99.15612\n",
      "Epoch 32/500\n",
      "549/549 [==============================] - 0s 80us/step - loss: 120.5272 - mean_absolute_error: 120.5272 - val_loss: 116.0691 - val_mean_absolute_error: 116.0691\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 99.15612\n",
      "Epoch 33/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549/549 [==============================] - 0s 91us/step - loss: 120.1805 - mean_absolute_error: 120.1805 - val_loss: 127.3941 - val_mean_absolute_error: 127.3941\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 99.15612\n",
      "Epoch 34/500\n",
      "549/549 [==============================] - 0s 80us/step - loss: 121.1873 - mean_absolute_error: 121.1873 - val_loss: 141.8023 - val_mean_absolute_error: 141.8023\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 99.15612\n",
      "Epoch 35/500\n",
      "549/549 [==============================] - ETA: 0s - loss: 123.2555 - mean_absolute_error: 123.255 - 0s 82us/step - loss: 125.8193 - mean_absolute_error: 125.8193 - val_loss: 158.8762 - val_mean_absolute_error: 158.8762\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 99.15612\n",
      "Epoch 36/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 124.4873 - mean_absolute_error: 124.4873 - val_loss: 116.8525 - val_mean_absolute_error: 116.8525\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 99.15612\n",
      "Epoch 37/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 120.3490 - mean_absolute_error: 120.3490 - val_loss: 105.8156 - val_mean_absolute_error: 105.8156\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 99.15612\n",
      "Epoch 38/500\n",
      "549/549 [==============================] - 0s 80us/step - loss: 122.0094 - mean_absolute_error: 122.0094 - val_loss: 121.4760 - val_mean_absolute_error: 121.4760\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 99.15612\n",
      "Epoch 39/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 119.0092 - mean_absolute_error: 119.0092 - val_loss: 126.7858 - val_mean_absolute_error: 126.7858\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 99.15612\n",
      "Epoch 40/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 120.6896 - mean_absolute_error: 120.6896 - val_loss: 121.7602 - val_mean_absolute_error: 121.7602\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 99.15612\n",
      "Epoch 41/500\n",
      "549/549 [==============================] - 0s 82us/step - loss: 120.2158 - mean_absolute_error: 120.2158 - val_loss: 124.4571 - val_mean_absolute_error: 124.4571\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 99.15612\n",
      "Epoch 42/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 119.0542 - mean_absolute_error: 119.0542 - val_loss: 129.6097 - val_mean_absolute_error: 129.6097\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 99.15612\n",
      "Epoch 43/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 119.2980 - mean_absolute_error: 119.2980 - val_loss: 151.1594 - val_mean_absolute_error: 151.1594\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 99.15612\n",
      "Epoch 44/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 120.3341 - mean_absolute_error: 120.3341 - val_loss: 124.0722 - val_mean_absolute_error: 124.0722\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 99.15612\n",
      "Epoch 45/500\n",
      "549/549 [==============================] - 0s 93us/step - loss: 118.2306 - mean_absolute_error: 118.2306 - val_loss: 113.6892 - val_mean_absolute_error: 113.6892\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 99.15612\n",
      "Epoch 46/500\n",
      "549/549 [==============================] - 0s 95us/step - loss: 117.0432 - mean_absolute_error: 117.0432 - val_loss: 140.4953 - val_mean_absolute_error: 140.4953\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 99.15612\n",
      "Epoch 47/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 119.1413 - mean_absolute_error: 119.1413 - val_loss: 126.6589 - val_mean_absolute_error: 126.6589\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 99.15612\n",
      "Epoch 48/500\n",
      "549/549 [==============================] - 0s 89us/step - loss: 117.2445 - mean_absolute_error: 117.2445 - val_loss: 115.8574 - val_mean_absolute_error: 115.8574\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 99.15612\n",
      "Epoch 49/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 115.9861 - mean_absolute_error: 115.9861 - val_loss: 119.3331 - val_mean_absolute_error: 119.3331\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 99.15612\n",
      "Epoch 50/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 118.3479 - mean_absolute_error: 118.3479 - val_loss: 108.2189 - val_mean_absolute_error: 108.2189\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 99.15612\n",
      "Epoch 51/500\n",
      "549/549 [==============================] - 0s 78us/step - loss: 117.9217 - mean_absolute_error: 117.9217 - val_loss: 118.6536 - val_mean_absolute_error: 118.6536\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 99.15612\n",
      "Epoch 52/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 117.5622 - mean_absolute_error: 117.5622 - val_loss: 117.5397 - val_mean_absolute_error: 117.5397\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 99.15612\n",
      "Epoch 53/500\n",
      "549/549 [==============================] - 0s 78us/step - loss: 116.1844 - mean_absolute_error: 116.1844 - val_loss: 111.1295 - val_mean_absolute_error: 111.1295\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 99.15612\n",
      "Epoch 54/500\n",
      "549/549 [==============================] - 0s 82us/step - loss: 116.1547 - mean_absolute_error: 116.1547 - val_loss: 132.7406 - val_mean_absolute_error: 132.7406\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 99.15612\n",
      "Epoch 55/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 116.7370 - mean_absolute_error: 116.7370 - val_loss: 125.7627 - val_mean_absolute_error: 125.7627\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 99.15612\n",
      "Epoch 56/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 115.6294 - mean_absolute_error: 115.6294 - val_loss: 127.8113 - val_mean_absolute_error: 127.8113\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 99.15612\n",
      "Epoch 57/500\n",
      "549/549 [==============================] - 0s 94us/step - loss: 114.4041 - mean_absolute_error: 114.4041 - val_loss: 137.6182 - val_mean_absolute_error: 137.6182\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 99.15612\n",
      "Epoch 58/500\n",
      "549/549 [==============================] - 0s 74us/step - loss: 118.2218 - mean_absolute_error: 118.2218 - val_loss: 120.0247 - val_mean_absolute_error: 120.0247\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 99.15612\n",
      "Epoch 59/500\n",
      "549/549 [==============================] - 0s 82us/step - loss: 114.4160 - mean_absolute_error: 114.4160 - val_loss: 116.1063 - val_mean_absolute_error: 116.1063\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 99.15612\n",
      "Epoch 60/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 118.5608 - mean_absolute_error: 118.5608 - val_loss: 133.4637 - val_mean_absolute_error: 133.4637\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 99.15612\n",
      "Epoch 61/500\n",
      "549/549 [==============================] - 0s 80us/step - loss: 124.1460 - mean_absolute_error: 124.1460 - val_loss: 130.3206 - val_mean_absolute_error: 130.3206\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 99.15612\n",
      "Epoch 62/500\n",
      "549/549 [==============================] - 0s 96us/step - loss: 114.9729 - mean_absolute_error: 114.9729 - val_loss: 128.6734 - val_mean_absolute_error: 128.6734\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 99.15612\n",
      "Epoch 63/500\n",
      "549/549 [==============================] - 0s 80us/step - loss: 115.3604 - mean_absolute_error: 115.3604 - val_loss: 117.6704 - val_mean_absolute_error: 117.6704\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 99.15612\n",
      "Epoch 64/500\n",
      "549/549 [==============================] - 0s 78us/step - loss: 120.5235 - mean_absolute_error: 120.5235 - val_loss: 127.2531 - val_mean_absolute_error: 127.2531\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 99.15612\n",
      "Epoch 65/500\n",
      "549/549 [==============================] - 0s 80us/step - loss: 114.3350 - mean_absolute_error: 114.3350 - val_loss: 137.0471 - val_mean_absolute_error: 137.0471\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 99.15612\n",
      "Epoch 66/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 114.8191 - mean_absolute_error: 114.8191 - val_loss: 140.4857 - val_mean_absolute_error: 140.4857\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 99.15612\n",
      "Epoch 67/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 113.6047 - mean_absolute_error: 113.6047 - val_loss: 132.6135 - val_mean_absolute_error: 132.6135\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 99.15612\n",
      "Epoch 68/500\n",
      "549/549 [==============================] - 0s 74us/step - loss: 117.1338 - mean_absolute_error: 117.1338 - val_loss: 148.9614 - val_mean_absolute_error: 148.9614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00068: val_loss did not improve from 99.15612\n",
      "Epoch 69/500\n",
      "549/549 [==============================] - 0s 78us/step - loss: 116.1744 - mean_absolute_error: 116.1744 - val_loss: 122.5253 - val_mean_absolute_error: 122.5253\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 99.15612\n",
      "Epoch 70/500\n",
      "549/549 [==============================] - ETA: 0s - loss: 145.4629 - mean_absolute_error: 145.462 - 0s 86us/step - loss: 114.1227 - mean_absolute_error: 114.1227 - val_loss: 112.5300 - val_mean_absolute_error: 112.5300\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 99.15612\n",
      "Epoch 71/500\n",
      "549/549 [==============================] - 0s 80us/step - loss: 119.1558 - mean_absolute_error: 119.1558 - val_loss: 111.3177 - val_mean_absolute_error: 111.3177\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 99.15612\n",
      "Epoch 72/500\n",
      "549/549 [==============================] - 0s 78us/step - loss: 113.3745 - mean_absolute_error: 113.3745 - val_loss: 130.5307 - val_mean_absolute_error: 130.5307\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 99.15612\n",
      "Epoch 73/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 117.7408 - mean_absolute_error: 117.7408 - val_loss: 112.6699 - val_mean_absolute_error: 112.6699\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 99.15612\n",
      "Epoch 74/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 117.0763 - mean_absolute_error: 117.0763 - val_loss: 99.1541 - val_mean_absolute_error: 99.1541\n",
      "\n",
      "Epoch 00074: val_loss improved from 99.15612 to 99.15409, saving model to GS-mad적용마스크date-Weights-074--99.15409-cat02-vf05.hdf5\n",
      "Epoch 75/500\n",
      "549/549 [==============================] - 0s 78us/step - loss: 118.8893 - mean_absolute_error: 118.8893 - val_loss: 102.5979 - val_mean_absolute_error: 102.5979\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 99.15409\n",
      "Epoch 76/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 117.5399 - mean_absolute_error: 117.5399 - val_loss: 128.0346 - val_mean_absolute_error: 128.0346\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 99.15409\n",
      "Epoch 77/500\n",
      "549/549 [==============================] - 0s 76us/step - loss: 112.6022 - mean_absolute_error: 112.6022 - val_loss: 123.6691 - val_mean_absolute_error: 123.6691\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 99.15409\n",
      "Epoch 78/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 111.8056 - mean_absolute_error: 111.8056 - val_loss: 122.6610 - val_mean_absolute_error: 122.6610\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 99.15409\n",
      "Epoch 79/500\n",
      "549/549 [==============================] - 0s 78us/step - loss: 112.4870 - mean_absolute_error: 112.4870 - val_loss: 131.4404 - val_mean_absolute_error: 131.4404\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 99.15409\n",
      "Epoch 80/500\n",
      "549/549 [==============================] - 0s 89us/step - loss: 112.2162 - mean_absolute_error: 112.2162 - val_loss: 133.2784 - val_mean_absolute_error: 133.2784\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 99.15409\n",
      "Epoch 81/500\n",
      "549/549 [==============================] - 0s 93us/step - loss: 114.4115 - mean_absolute_error: 114.4115 - val_loss: 140.3037 - val_mean_absolute_error: 140.3037\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 99.15409\n",
      "Epoch 82/500\n",
      "549/549 [==============================] - 0s 93us/step - loss: 114.6628 - mean_absolute_error: 114.6628 - val_loss: 130.7076 - val_mean_absolute_error: 130.7076\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 99.15409\n",
      "Epoch 83/500\n",
      "549/549 [==============================] - 0s 73us/step - loss: 112.4204 - mean_absolute_error: 112.4204 - val_loss: 122.7228 - val_mean_absolute_error: 122.7228\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 99.15409\n",
      "Epoch 84/500\n",
      "549/549 [==============================] - 0s 80us/step - loss: 112.4669 - mean_absolute_error: 112.4669 - val_loss: 133.9484 - val_mean_absolute_error: 133.9484\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 99.15409\n",
      "Epoch 85/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 111.5198 - mean_absolute_error: 111.5198 - val_loss: 140.0363 - val_mean_absolute_error: 140.0363\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 99.15409\n",
      "Epoch 86/500\n",
      "549/549 [==============================] - 0s 74us/step - loss: 114.9264 - mean_absolute_error: 114.9264 - val_loss: 129.9691 - val_mean_absolute_error: 129.9691\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 99.15409\n",
      "Epoch 87/500\n",
      "549/549 [==============================] - 0s 74us/step - loss: 110.6741 - mean_absolute_error: 110.6741 - val_loss: 130.8342 - val_mean_absolute_error: 130.8342\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 99.15409\n",
      "Epoch 88/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 109.7291 - mean_absolute_error: 109.7291 - val_loss: 117.7590 - val_mean_absolute_error: 117.7590\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 99.15409\n",
      "Epoch 89/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 112.9496 - mean_absolute_error: 112.9496 - val_loss: 114.7885 - val_mean_absolute_error: 114.7885\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 99.15409\n",
      "Epoch 90/500\n",
      "549/549 [==============================] - 0s 89us/step - loss: 110.3740 - mean_absolute_error: 110.3740 - val_loss: 136.4922 - val_mean_absolute_error: 136.4922\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 99.15409\n",
      "Epoch 91/500\n",
      "549/549 [==============================] - 0s 94us/step - loss: 111.8581 - mean_absolute_error: 111.8581 - val_loss: 129.8716 - val_mean_absolute_error: 129.8716\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 99.15409\n",
      "Epoch 92/500\n",
      "549/549 [==============================] - 0s 89us/step - loss: 110.3743 - mean_absolute_error: 110.3743 - val_loss: 109.5220 - val_mean_absolute_error: 109.5220\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 99.15409\n",
      "Epoch 93/500\n",
      "549/549 [==============================] - 0s 74us/step - loss: 113.3232 - mean_absolute_error: 113.3232 - val_loss: 121.7333 - val_mean_absolute_error: 121.7333\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 99.15409\n",
      "Epoch 94/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 110.1759 - mean_absolute_error: 110.1759 - val_loss: 122.8540 - val_mean_absolute_error: 122.8540\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 99.15409\n",
      "Epoch 95/500\n",
      "549/549 [==============================] - 0s 86us/step - loss: 109.7440 - mean_absolute_error: 109.7440 - val_loss: 127.6027 - val_mean_absolute_error: 127.6027\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 99.15409\n",
      "Epoch 96/500\n",
      "549/549 [==============================] - 0s 78us/step - loss: 110.0284 - mean_absolute_error: 110.0284 - val_loss: 122.0311 - val_mean_absolute_error: 122.0311\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 99.15409\n",
      "Epoch 97/500\n",
      "549/549 [==============================] - 0s 72us/step - loss: 109.8059 - mean_absolute_error: 109.8059 - val_loss: 124.2523 - val_mean_absolute_error: 124.2523\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 99.15409\n",
      "Epoch 98/500\n",
      "549/549 [==============================] - 0s 91us/step - loss: 111.7887 - mean_absolute_error: 111.7887 - val_loss: 108.5851 - val_mean_absolute_error: 108.5851\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 99.15409\n",
      "Epoch 99/500\n",
      "549/549 [==============================] - 0s 80us/step - loss: 113.2775 - mean_absolute_error: 113.2775 - val_loss: 106.6386 - val_mean_absolute_error: 106.6386\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 99.15409\n",
      "Epoch 100/500\n",
      "549/549 [==============================] - 0s 72us/step - loss: 111.6335 - mean_absolute_error: 111.6335 - val_loss: 119.7378 - val_mean_absolute_error: 119.7378\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 99.15409\n",
      "Epoch 101/500\n",
      "549/549 [==============================] - 0s 78us/step - loss: 109.3910 - mean_absolute_error: 109.3910 - val_loss: 145.5917 - val_mean_absolute_error: 145.5917\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 99.15409\n",
      "Epoch 102/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 118.9319 - mean_absolute_error: 118.9319 - val_loss: 134.2002 - val_mean_absolute_error: 134.2002\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 99.15409\n",
      "Epoch 103/500\n",
      "549/549 [==============================] - 0s 82us/step - loss: 109.3973 - mean_absolute_error: 109.3973 - val_loss: 116.2048 - val_mean_absolute_error: 116.2048\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 99.15409\n",
      "Epoch 104/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 109.3595 - mean_absolute_error: 109.3595 - val_loss: 111.7393 - val_mean_absolute_error: 111.7393\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 99.15409\n",
      "Epoch 105/500\n",
      "549/549 [==============================] - 0s 82us/step - loss: 110.5933 - mean_absolute_error: 110.5933 - val_loss: 134.7081 - val_mean_absolute_error: 134.7081\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 99.15409\n",
      "Epoch 106/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 114.0032 - mean_absolute_error: 114.0032 - val_loss: 118.8510 - val_mean_absolute_error: 118.8510\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 99.15409\n",
      "Epoch 107/500\n",
      "549/549 [==============================] - 0s 98us/step - loss: 109.9062 - mean_absolute_error: 109.9062 - val_loss: 105.0775 - val_mean_absolute_error: 105.0775\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 99.15409\n",
      "Epoch 108/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 112.2681 - mean_absolute_error: 112.2681 - val_loss: 119.6294 - val_mean_absolute_error: 119.6294\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 99.15409\n",
      "Epoch 109/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 110.0991 - mean_absolute_error: 110.0991 - val_loss: 134.3712 - val_mean_absolute_error: 134.3712\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 99.15409\n",
      "Epoch 110/500\n",
      "549/549 [==============================] - 0s 82us/step - loss: 109.5664 - mean_absolute_error: 109.5664 - val_loss: 125.2503 - val_mean_absolute_error: 125.2503\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 99.15409\n",
      "Epoch 111/500\n",
      "549/549 [==============================] - 0s 93us/step - loss: 109.4376 - mean_absolute_error: 109.4376 - val_loss: 126.9548 - val_mean_absolute_error: 126.9548\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 99.15409\n",
      "Epoch 112/500\n",
      "549/549 [==============================] - 0s 78us/step - loss: 109.4289 - mean_absolute_error: 109.4289 - val_loss: 123.3074 - val_mean_absolute_error: 123.3074\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 99.15409\n",
      "Epoch 113/500\n",
      "549/549 [==============================] - 0s 89us/step - loss: 108.8278 - mean_absolute_error: 108.8278 - val_loss: 122.8670 - val_mean_absolute_error: 122.8670\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 99.15409\n",
      "Epoch 114/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 108.5751 - mean_absolute_error: 108.5751 - val_loss: 119.1815 - val_mean_absolute_error: 119.1815\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 99.15409\n",
      "Epoch 115/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 109.7884 - mean_absolute_error: 109.7884 - val_loss: 114.1735 - val_mean_absolute_error: 114.1735\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 99.15409\n",
      "Epoch 116/500\n",
      "549/549 [==============================] - 0s 80us/step - loss: 107.8302 - mean_absolute_error: 107.8302 - val_loss: 112.2757 - val_mean_absolute_error: 112.2757\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 99.15409\n",
      "Epoch 117/500\n",
      "549/549 [==============================] - 0s 76us/step - loss: 110.7323 - mean_absolute_error: 110.7323 - val_loss: 124.7399 - val_mean_absolute_error: 124.7399\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 99.15409\n",
      "Epoch 118/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 108.7966 - mean_absolute_error: 108.7966 - val_loss: 117.6567 - val_mean_absolute_error: 117.6567\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 99.15409\n",
      "Epoch 119/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 108.9338 - mean_absolute_error: 108.9338 - val_loss: 112.2031 - val_mean_absolute_error: 112.2031\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 99.15409\n",
      "Epoch 120/500\n",
      "549/549 [==============================] - 0s 80us/step - loss: 108.1954 - mean_absolute_error: 108.1954 - val_loss: 122.6260 - val_mean_absolute_error: 122.6260\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 99.15409\n",
      "Epoch 121/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 109.8455 - mean_absolute_error: 109.8455 - val_loss: 133.4092 - val_mean_absolute_error: 133.4092\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 99.15409\n",
      "Epoch 122/500\n",
      "549/549 [==============================] - 0s 76us/step - loss: 110.7568 - mean_absolute_error: 110.7568 - val_loss: 135.1489 - val_mean_absolute_error: 135.1489\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 99.15409\n",
      "Epoch 123/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 108.7069 - mean_absolute_error: 108.7069 - val_loss: 115.3862 - val_mean_absolute_error: 115.3862\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 99.15409\n",
      "Epoch 124/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 108.3572 - mean_absolute_error: 108.3572 - val_loss: 129.5192 - val_mean_absolute_error: 129.5192\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 99.15409\n",
      "Epoch 125/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 109.3202 - mean_absolute_error: 109.3202 - val_loss: 138.5605 - val_mean_absolute_error: 138.5605\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 99.15409\n",
      "Epoch 126/500\n",
      "549/549 [==============================] - 0s 78us/step - loss: 109.5291 - mean_absolute_error: 109.5291 - val_loss: 142.0893 - val_mean_absolute_error: 142.0893\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 99.15409\n",
      "Epoch 127/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 110.5503 - mean_absolute_error: 110.5503 - val_loss: 113.9287 - val_mean_absolute_error: 113.9287\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 99.15409\n",
      "Epoch 128/500\n",
      "549/549 [==============================] - 0s 76us/step - loss: 111.8419 - mean_absolute_error: 111.8419 - val_loss: 115.1964 - val_mean_absolute_error: 115.1964\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 99.15409\n",
      "Epoch 129/500\n",
      "549/549 [==============================] - 0s 89us/step - loss: 108.0641 - mean_absolute_error: 108.0641 - val_loss: 122.5839 - val_mean_absolute_error: 122.5839\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 99.15409\n",
      "Epoch 130/500\n",
      "549/549 [==============================] - 0s 86us/step - loss: 108.7047 - mean_absolute_error: 108.7047 - val_loss: 134.2359 - val_mean_absolute_error: 134.2359\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 99.15409\n",
      "Epoch 131/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 107.9915 - mean_absolute_error: 107.9915 - val_loss: 135.5704 - val_mean_absolute_error: 135.5704\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 99.15409\n",
      "Epoch 132/500\n",
      "549/549 [==============================] - ETA: 0s - loss: 108.4941 - mean_absolute_error: 108.494 - 0s 75us/step - loss: 109.5167 - mean_absolute_error: 109.5167 - val_loss: 120.5035 - val_mean_absolute_error: 120.5035\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 99.15409\n",
      "Epoch 133/500\n",
      "549/549 [==============================] - 0s 82us/step - loss: 107.2838 - mean_absolute_error: 107.2838 - val_loss: 129.4812 - val_mean_absolute_error: 129.4812\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 99.15409\n",
      "Epoch 134/500\n",
      "549/549 [==============================] - 0s 83us/step - loss: 107.3965 - mean_absolute_error: 107.3965 - val_loss: 120.6067 - val_mean_absolute_error: 120.6067\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 99.15409\n",
      "Epoch 135/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 107.3756 - mean_absolute_error: 107.3756 - val_loss: 131.4724 - val_mean_absolute_error: 131.4724\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 99.15409\n",
      "Epoch 136/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 107.4495 - mean_absolute_error: 107.4495 - val_loss: 120.9631 - val_mean_absolute_error: 120.9631\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 99.15409\n",
      "Epoch 137/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 108.5950 - mean_absolute_error: 108.5950 - val_loss: 120.2530 - val_mean_absolute_error: 120.2530\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 99.15409\n",
      "Epoch 138/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 107.3449 - mean_absolute_error: 107.3449 - val_loss: 109.3709 - val_mean_absolute_error: 109.3709\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 99.15409\n",
      "Epoch 139/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 107.6899 - mean_absolute_error: 107.6899 - val_loss: 120.4815 - val_mean_absolute_error: 120.4815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00139: val_loss did not improve from 99.15409\n",
      "Epoch 140/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 107.8216 - mean_absolute_error: 107.8216 - val_loss: 117.0780 - val_mean_absolute_error: 117.0780\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 99.15409\n",
      "Epoch 141/500\n",
      "549/549 [==============================] - 0s 82us/step - loss: 108.5203 - mean_absolute_error: 108.5203 - val_loss: 124.4156 - val_mean_absolute_error: 124.4156\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 99.15409\n",
      "Epoch 142/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 109.8969 - mean_absolute_error: 109.8969 - val_loss: 131.7935 - val_mean_absolute_error: 131.7935\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 99.15409\n",
      "Epoch 143/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 107.9022 - mean_absolute_error: 107.9022 - val_loss: 124.0298 - val_mean_absolute_error: 124.0298\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 99.15409\n",
      "Epoch 144/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 107.4999 - mean_absolute_error: 107.4999 - val_loss: 130.3938 - val_mean_absolute_error: 130.3938\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 99.15409\n",
      "Epoch 145/500\n",
      "549/549 [==============================] - 0s 78us/step - loss: 108.1296 - mean_absolute_error: 108.1296 - val_loss: 112.2590 - val_mean_absolute_error: 112.2590\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 99.15409\n",
      "Epoch 146/500\n",
      "549/549 [==============================] - 0s 80us/step - loss: 108.0536 - mean_absolute_error: 108.0536 - val_loss: 121.1016 - val_mean_absolute_error: 121.1016\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 99.15409\n",
      "Epoch 147/500\n",
      "549/549 [==============================] - 0s 82us/step - loss: 108.9793 - mean_absolute_error: 108.9793 - val_loss: 126.9996 - val_mean_absolute_error: 126.9996\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 99.15409\n",
      "Epoch 148/500\n",
      "549/549 [==============================] - 0s 80us/step - loss: 107.3791 - mean_absolute_error: 107.3791 - val_loss: 130.0611 - val_mean_absolute_error: 130.0611\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 99.15409\n",
      "Epoch 149/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 107.6190 - mean_absolute_error: 107.6190 - val_loss: 132.3993 - val_mean_absolute_error: 132.3993\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 99.15409\n",
      "Epoch 150/500\n",
      "549/549 [==============================] - 0s 82us/step - loss: 107.2615 - mean_absolute_error: 107.2615 - val_loss: 115.4188 - val_mean_absolute_error: 115.4188\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 99.15409\n",
      "Epoch 151/500\n",
      "549/549 [==============================] - 0s 78us/step - loss: 106.5126 - mean_absolute_error: 106.5126 - val_loss: 112.6784 - val_mean_absolute_error: 112.6784\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 99.15409\n",
      "Epoch 152/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 111.3952 - mean_absolute_error: 111.3952 - val_loss: 111.9146 - val_mean_absolute_error: 111.9146\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 99.15409\n",
      "Epoch 153/500\n",
      "549/549 [==============================] - 0s 78us/step - loss: 108.2056 - mean_absolute_error: 108.2056 - val_loss: 123.8386 - val_mean_absolute_error: 123.8386\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 99.15409\n",
      "Epoch 154/500\n",
      "549/549 [==============================] - 0s 74us/step - loss: 109.3252 - mean_absolute_error: 109.3252 - val_loss: 126.7252 - val_mean_absolute_error: 126.7252\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 99.15409\n",
      "Epoch 155/500\n",
      "549/549 [==============================] - 0s 89us/step - loss: 109.3597 - mean_absolute_error: 109.3597 - val_loss: 129.0337 - val_mean_absolute_error: 129.0337\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 99.15409\n",
      "Epoch 156/500\n",
      "549/549 [==============================] - 0s 86us/step - loss: 110.4299 - mean_absolute_error: 110.4299 - val_loss: 106.3835 - val_mean_absolute_error: 106.3835\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 99.15409\n",
      "Epoch 157/500\n",
      "549/549 [==============================] - 0s 80us/step - loss: 111.3444 - mean_absolute_error: 111.3444 - val_loss: 103.5023 - val_mean_absolute_error: 103.5023\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 99.15409\n",
      "Epoch 158/500\n",
      "549/549 [==============================] - 0s 83us/step - loss: 111.4218 - mean_absolute_error: 111.4218 - val_loss: 113.9112 - val_mean_absolute_error: 113.9112\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 99.15409\n",
      "Epoch 159/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 107.0730 - mean_absolute_error: 107.0730 - val_loss: 132.5780 - val_mean_absolute_error: 132.5780\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 99.15409\n",
      "Epoch 160/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 106.5465 - mean_absolute_error: 106.5465 - val_loss: 121.5705 - val_mean_absolute_error: 121.5705\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 99.15409\n",
      "Epoch 161/500\n",
      "549/549 [==============================] - 0s 78us/step - loss: 107.7564 - mean_absolute_error: 107.7564 - val_loss: 120.6692 - val_mean_absolute_error: 120.6692\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 99.15409\n",
      "Epoch 162/500\n",
      "549/549 [==============================] - 0s 89us/step - loss: 107.4087 - mean_absolute_error: 107.4087 - val_loss: 127.0346 - val_mean_absolute_error: 127.0346\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 99.15409\n",
      "Epoch 163/500\n",
      "549/549 [==============================] - 0s 77us/step - loss: 108.8520 - mean_absolute_error: 108.8520 - val_loss: 139.6829 - val_mean_absolute_error: 139.6829\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 99.15409\n",
      "Epoch 164/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 108.4528 - mean_absolute_error: 108.4528 - val_loss: 133.8511 - val_mean_absolute_error: 133.8511\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 99.15409\n",
      "Epoch 165/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 108.1348 - mean_absolute_error: 108.1348 - val_loss: 112.5046 - val_mean_absolute_error: 112.5046\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 99.15409\n",
      "Epoch 166/500\n",
      "549/549 [==============================] - 0s 93us/step - loss: 106.4757 - mean_absolute_error: 106.4757 - val_loss: 119.0213 - val_mean_absolute_error: 119.0213\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 99.15409\n",
      "Epoch 167/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 107.3621 - mean_absolute_error: 107.3621 - val_loss: 120.7613 - val_mean_absolute_error: 120.7613\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 99.15409\n",
      "Epoch 168/500\n",
      "549/549 [==============================] - 0s 76us/step - loss: 107.3394 - mean_absolute_error: 107.3394 - val_loss: 113.7730 - val_mean_absolute_error: 113.7730\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 99.15409\n",
      "Epoch 169/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 106.3392 - mean_absolute_error: 106.3392 - val_loss: 122.4625 - val_mean_absolute_error: 122.4625\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 99.15409\n",
      "Epoch 170/500\n",
      "549/549 [==============================] - 0s 82us/step - loss: 106.4725 - mean_absolute_error: 106.4725 - val_loss: 113.9381 - val_mean_absolute_error: 113.9381\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 99.15409\n",
      "Epoch 171/500\n",
      "549/549 [==============================] - 0s 77us/step - loss: 106.3082 - mean_absolute_error: 106.3082 - val_loss: 114.7137 - val_mean_absolute_error: 114.7137\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 99.15409\n",
      "Epoch 172/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 105.7040 - mean_absolute_error: 105.7040 - val_loss: 121.3012 - val_mean_absolute_error: 121.3012\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 99.15409\n",
      "Epoch 173/500\n",
      "549/549 [==============================] - ETA: 0s - loss: 100.7679 - mean_absolute_error: 100.767 - 0s 89us/step - loss: 106.6852 - mean_absolute_error: 106.6852 - val_loss: 120.8303 - val_mean_absolute_error: 120.8303\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 99.15409\n",
      "Epoch 174/500\n",
      "549/549 [==============================] - 0s 82us/step - loss: 109.9972 - mean_absolute_error: 109.9972 - val_loss: 109.7831 - val_mean_absolute_error: 109.7831\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 99.15409\n",
      "Epoch 175/500\n",
      "549/549 [==============================] - 0s 89us/step - loss: 108.4215 - mean_absolute_error: 108.4215 - val_loss: 116.5325 - val_mean_absolute_error: 116.5325\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 99.15409\n",
      "Epoch 176/500\n",
      "549/549 [==============================] - 0s 78us/step - loss: 110.7942 - mean_absolute_error: 110.7942 - val_loss: 113.6769 - val_mean_absolute_error: 113.6769\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 99.15409\n",
      "Epoch 177/500\n",
      "549/549 [==============================] - 0s 80us/step - loss: 106.3778 - mean_absolute_error: 106.3778 - val_loss: 113.7364 - val_mean_absolute_error: 113.7364\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 99.15409\n",
      "Epoch 178/500\n",
      "549/549 [==============================] - 0s 83us/step - loss: 105.5573 - mean_absolute_error: 105.5573 - val_loss: 136.5377 - val_mean_absolute_error: 136.5377\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 99.15409\n",
      "Epoch 179/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 106.4508 - mean_absolute_error: 106.4508 - val_loss: 125.3195 - val_mean_absolute_error: 125.3195\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 99.15409\n",
      "Epoch 180/500\n",
      "549/549 [==============================] - 0s 83us/step - loss: 106.4936 - mean_absolute_error: 106.4936 - val_loss: 121.8736 - val_mean_absolute_error: 121.8736\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 99.15409\n",
      "Epoch 181/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 106.8386 - mean_absolute_error: 106.8386 - val_loss: 118.4970 - val_mean_absolute_error: 118.4970\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 99.15409\n",
      "Epoch 182/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 106.2646 - mean_absolute_error: 106.2646 - val_loss: 118.2598 - val_mean_absolute_error: 118.2598\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 99.15409\n",
      "Epoch 183/500\n",
      "549/549 [==============================] - 0s 82us/step - loss: 105.7810 - mean_absolute_error: 105.7810 - val_loss: 114.3572 - val_mean_absolute_error: 114.3572\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 99.15409\n",
      "Epoch 184/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 105.9107 - mean_absolute_error: 105.9107 - val_loss: 117.2545 - val_mean_absolute_error: 117.2545\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 99.15409\n",
      "Epoch 185/500\n",
      "549/549 [==============================] - 0s 93us/step - loss: 106.1491 - mean_absolute_error: 106.1491 - val_loss: 125.4770 - val_mean_absolute_error: 125.4770\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 99.15409\n",
      "Epoch 186/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 106.4594 - mean_absolute_error: 106.4594 - val_loss: 121.8169 - val_mean_absolute_error: 121.8169\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 99.15409\n",
      "Epoch 187/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 105.7803 - mean_absolute_error: 105.7803 - val_loss: 124.8523 - val_mean_absolute_error: 124.8523\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 99.15409\n",
      "Epoch 188/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 106.5694 - mean_absolute_error: 106.5694 - val_loss: 117.7099 - val_mean_absolute_error: 117.7099\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 99.15409\n",
      "Epoch 189/500\n",
      "549/549 [==============================] - 0s 91us/step - loss: 106.2126 - mean_absolute_error: 106.2126 - val_loss: 115.6506 - val_mean_absolute_error: 115.6506\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 99.15409\n",
      "Epoch 190/500\n",
      "549/549 [==============================] - 0s 78us/step - loss: 106.5895 - mean_absolute_error: 106.5895 - val_loss: 124.9585 - val_mean_absolute_error: 124.9585\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 99.15409\n",
      "Epoch 191/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 107.8827 - mean_absolute_error: 107.8827 - val_loss: 126.0061 - val_mean_absolute_error: 126.0061\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 99.15409\n",
      "Epoch 192/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 104.6877 - mean_absolute_error: 104.6877 - val_loss: 120.1953 - val_mean_absolute_error: 120.1953\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 99.15409\n",
      "Epoch 193/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 105.6922 - mean_absolute_error: 105.6922 - val_loss: 109.9950 - val_mean_absolute_error: 109.9950\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 99.15409\n",
      "Epoch 194/500\n",
      "549/549 [==============================] - 0s 80us/step - loss: 111.0109 - mean_absolute_error: 111.0109 - val_loss: 106.4607 - val_mean_absolute_error: 106.4607\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 99.15409\n",
      "Epoch 195/500\n",
      "549/549 [==============================] - 0s 77us/step - loss: 107.5390 - mean_absolute_error: 107.5390 - val_loss: 119.8151 - val_mean_absolute_error: 119.8151\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 99.15409\n",
      "Epoch 196/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 107.0847 - mean_absolute_error: 107.0847 - val_loss: 118.1847 - val_mean_absolute_error: 118.1847\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 99.15409\n",
      "Epoch 197/500\n",
      "549/549 [==============================] - 0s 74us/step - loss: 106.6802 - mean_absolute_error: 106.6802 - val_loss: 124.1445 - val_mean_absolute_error: 124.1445\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 99.15409\n",
      "Epoch 198/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 107.3757 - mean_absolute_error: 107.3757 - val_loss: 116.2524 - val_mean_absolute_error: 116.2524\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 99.15409\n",
      "Epoch 199/500\n",
      "549/549 [==============================] - 0s 91us/step - loss: 107.7994 - mean_absolute_error: 107.7994 - val_loss: 127.2271 - val_mean_absolute_error: 127.2271\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 99.15409\n",
      "Epoch 200/500\n",
      "549/549 [==============================] - 0s 78us/step - loss: 108.6734 - mean_absolute_error: 108.6734 - val_loss: 129.0746 - val_mean_absolute_error: 129.0746\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 99.15409\n",
      "Epoch 201/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 107.2497 - mean_absolute_error: 107.2497 - val_loss: 119.8576 - val_mean_absolute_error: 119.8576\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 99.15409\n",
      "Epoch 202/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 108.4618 - mean_absolute_error: 108.4618 - val_loss: 139.0937 - val_mean_absolute_error: 139.0937\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 99.15409\n",
      "Epoch 203/500\n",
      "549/549 [==============================] - 0s 80us/step - loss: 108.6387 - mean_absolute_error: 108.6387 - val_loss: 130.0604 - val_mean_absolute_error: 130.0604\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 99.15409\n",
      "Epoch 204/500\n",
      "549/549 [==============================] - 0s 74us/step - loss: 105.7900 - mean_absolute_error: 105.7900 - val_loss: 127.4009 - val_mean_absolute_error: 127.4009\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 99.15409\n",
      "Epoch 205/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 106.4976 - mean_absolute_error: 106.4976 - val_loss: 117.8597 - val_mean_absolute_error: 117.8597\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 99.15409\n",
      "Epoch 206/500\n",
      "549/549 [==============================] - 0s 91us/step - loss: 106.5629 - mean_absolute_error: 106.5629 - val_loss: 120.5358 - val_mean_absolute_error: 120.5358\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 99.15409\n",
      "Epoch 207/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 106.9467 - mean_absolute_error: 106.9467 - val_loss: 112.9156 - val_mean_absolute_error: 112.9156\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 99.15409\n",
      "Epoch 208/500\n",
      "549/549 [==============================] - 0s 88us/step - loss: 105.8504 - mean_absolute_error: 105.8504 - val_loss: 117.9425 - val_mean_absolute_error: 117.9425\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 99.15409\n",
      "Epoch 209/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 105.2021 - mean_absolute_error: 105.2021 - val_loss: 123.9769 - val_mean_absolute_error: 123.9769\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 99.15409\n",
      "Epoch 210/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 107.0452 - mean_absolute_error: 107.0452 - val_loss: 116.0037 - val_mean_absolute_error: 116.0037\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 99.15409\n",
      "Epoch 211/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549/549 [==============================] - 0s 89us/step - loss: 107.6117 - mean_absolute_error: 107.6117 - val_loss: 110.6212 - val_mean_absolute_error: 110.6212\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 99.15409\n",
      "Epoch 212/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 113.8550 - mean_absolute_error: 113.8550 - val_loss: 106.4901 - val_mean_absolute_error: 106.4901\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 99.15409\n",
      "Epoch 213/500\n",
      "549/549 [==============================] - 0s 89us/step - loss: 107.8991 - mean_absolute_error: 107.8991 - val_loss: 100.6759 - val_mean_absolute_error: 100.6759\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 99.15409\n",
      "Epoch 214/500\n",
      "549/549 [==============================] - 0s 80us/step - loss: 109.0437 - mean_absolute_error: 109.0437 - val_loss: 105.9222 - val_mean_absolute_error: 105.9222\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 99.15409\n",
      "Epoch 215/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 113.6447 - mean_absolute_error: 113.6447 - val_loss: 104.9172 - val_mean_absolute_error: 104.9172\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 99.15409\n",
      "Epoch 216/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 109.6786 - mean_absolute_error: 109.6786 - val_loss: 102.6978 - val_mean_absolute_error: 102.6978\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 99.15409\n",
      "Epoch 217/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 107.2568 - mean_absolute_error: 107.2568 - val_loss: 99.0841 - val_mean_absolute_error: 99.0841\n",
      "\n",
      "Epoch 00217: val_loss improved from 99.15409 to 99.08407, saving model to GS-mad적용마스크date-Weights-217--99.08407-cat02-vf05.hdf5\n",
      "Epoch 218/500\n",
      "549/549 [==============================] - 0s 82us/step - loss: 109.9094 - mean_absolute_error: 109.9094 - val_loss: 107.7330 - val_mean_absolute_error: 107.7330\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 99.08407\n",
      "Epoch 219/500\n",
      "549/549 [==============================] - 0s 76us/step - loss: 105.1183 - mean_absolute_error: 105.1183 - val_loss: 130.3643 - val_mean_absolute_error: 130.3643\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 99.08407\n",
      "Epoch 220/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 108.0622 - mean_absolute_error: 108.0622 - val_loss: 109.8075 - val_mean_absolute_error: 109.8075\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 99.08407\n",
      "Epoch 221/500\n",
      "549/549 [==============================] - 0s 82us/step - loss: 106.0578 - mean_absolute_error: 106.0578 - val_loss: 117.7336 - val_mean_absolute_error: 117.7336\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 99.08407\n",
      "Epoch 222/500\n",
      "549/549 [==============================] - 0s 78us/step - loss: 104.6293 - mean_absolute_error: 104.6293 - val_loss: 131.0333 - val_mean_absolute_error: 131.0333\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 99.08407\n",
      "Epoch 223/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 105.8317 - mean_absolute_error: 105.8317 - val_loss: 137.6728 - val_mean_absolute_error: 137.6728\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 99.08407\n",
      "Epoch 224/500\n",
      "549/549 [==============================] - 0s 76us/step - loss: 107.7529 - mean_absolute_error: 107.7529 - val_loss: 131.2068 - val_mean_absolute_error: 131.2068\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 99.08407\n",
      "Epoch 225/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 111.9372 - mean_absolute_error: 111.9372 - val_loss: 143.3488 - val_mean_absolute_error: 143.3488\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 99.08407\n",
      "Epoch 226/500\n",
      "549/549 [==============================] - 0s 89us/step - loss: 105.8089 - mean_absolute_error: 105.8089 - val_loss: 128.4352 - val_mean_absolute_error: 128.4352\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 99.08407\n",
      "Epoch 227/500\n",
      "549/549 [==============================] - 0s 91us/step - loss: 106.7118 - mean_absolute_error: 106.7118 - val_loss: 136.0250 - val_mean_absolute_error: 136.0250\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 99.08407\n",
      "Epoch 228/500\n",
      "549/549 [==============================] - 0s 89us/step - loss: 107.3373 - mean_absolute_error: 107.3373 - val_loss: 124.7412 - val_mean_absolute_error: 124.7412\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 99.08407\n",
      "Epoch 229/500\n",
      "549/549 [==============================] - 0s 89us/step - loss: 105.7591 - mean_absolute_error: 105.7591 - val_loss: 120.7366 - val_mean_absolute_error: 120.7366\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 99.08407\n",
      "Epoch 230/500\n",
      "549/549 [==============================] - 0s 91us/step - loss: 105.2631 - mean_absolute_error: 105.2631 - val_loss: 129.8789 - val_mean_absolute_error: 129.8789\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 99.08407\n",
      "Epoch 231/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 104.2003 - mean_absolute_error: 104.2003 - val_loss: 120.6392 - val_mean_absolute_error: 120.6392\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 99.08407\n",
      "Epoch 232/500\n",
      "549/549 [==============================] - 0s 93us/step - loss: 104.2410 - mean_absolute_error: 104.2410 - val_loss: 121.7346 - val_mean_absolute_error: 121.7346\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 99.08407\n",
      "Epoch 233/500\n",
      "549/549 [==============================] - 0s 100us/step - loss: 104.3633 - mean_absolute_error: 104.3633 - val_loss: 113.7710 - val_mean_absolute_error: 113.7710\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 99.08407\n",
      "Epoch 234/500\n",
      "549/549 [==============================] - 0s 83us/step - loss: 106.6732 - mean_absolute_error: 106.6732 - val_loss: 123.9504 - val_mean_absolute_error: 123.9504\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 99.08407\n",
      "Epoch 235/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 105.1119 - mean_absolute_error: 105.1119 - val_loss: 110.1618 - val_mean_absolute_error: 110.1618\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 99.08407\n",
      "Epoch 236/500\n",
      "549/549 [==============================] - 0s 97us/step - loss: 104.9991 - mean_absolute_error: 104.9991 - val_loss: 113.0314 - val_mean_absolute_error: 113.0314\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 99.08407\n",
      "Epoch 237/500\n",
      "549/549 [==============================] - 0s 93us/step - loss: 104.9875 - mean_absolute_error: 104.9875 - val_loss: 103.9854 - val_mean_absolute_error: 103.9854\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 99.08407\n",
      "Epoch 238/500\n",
      "549/549 [==============================] - 0s 90us/step - loss: 111.1912 - mean_absolute_error: 111.1912 - val_loss: 114.0422 - val_mean_absolute_error: 114.0422\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 99.08407\n",
      "Epoch 239/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 108.3262 - mean_absolute_error: 108.3262 - val_loss: 110.5923 - val_mean_absolute_error: 110.5923\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 99.08407\n",
      "Epoch 240/500\n",
      "549/549 [==============================] - 0s 97us/step - loss: 104.8861 - mean_absolute_error: 104.8861 - val_loss: 112.3615 - val_mean_absolute_error: 112.3615\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 99.08407\n",
      "Epoch 241/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 104.8658 - mean_absolute_error: 104.8658 - val_loss: 110.5625 - val_mean_absolute_error: 110.5625\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 99.08407\n",
      "Epoch 242/500\n",
      "549/549 [==============================] - 0s 96us/step - loss: 104.9070 - mean_absolute_error: 104.9070 - val_loss: 121.7342 - val_mean_absolute_error: 121.7342\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 99.08407\n",
      "Epoch 243/500\n",
      "549/549 [==============================] - 0s 89us/step - loss: 103.9461 - mean_absolute_error: 103.9461 - val_loss: 124.9062 - val_mean_absolute_error: 124.9062\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 99.08407\n",
      "Epoch 244/500\n",
      "549/549 [==============================] - 0s 93us/step - loss: 105.7689 - mean_absolute_error: 105.7689 - val_loss: 107.8081 - val_mean_absolute_error: 107.8081\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 99.08407\n",
      "Epoch 245/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 105.8619 - mean_absolute_error: 105.8619 - val_loss: 112.9441 - val_mean_absolute_error: 112.9441\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 99.08407\n",
      "Epoch 246/500\n",
      "549/549 [==============================] - 0s 91us/step - loss: 104.9861 - mean_absolute_error: 104.9861 - val_loss: 115.2261 - val_mean_absolute_error: 115.2261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00246: val_loss did not improve from 99.08407\n",
      "Epoch 247/500\n",
      "549/549 [==============================] - 0s 100us/step - loss: 104.5236 - mean_absolute_error: 104.5236 - val_loss: 115.9573 - val_mean_absolute_error: 115.9573\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 99.08407\n",
      "Epoch 248/500\n",
      "549/549 [==============================] - 0s 131us/step - loss: 103.6001 - mean_absolute_error: 103.6001 - val_loss: 125.2332 - val_mean_absolute_error: 125.2332\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 99.08407\n",
      "Epoch 249/500\n",
      "549/549 [==============================] - 0s 98us/step - loss: 109.9420 - mean_absolute_error: 109.9420 - val_loss: 127.4065 - val_mean_absolute_error: 127.4065\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 99.08407\n",
      "Epoch 250/500\n",
      "549/549 [==============================] - 0s 89us/step - loss: 104.7821 - mean_absolute_error: 104.7821 - val_loss: 117.4475 - val_mean_absolute_error: 117.4475\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 99.08407\n",
      "Epoch 251/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 104.7507 - mean_absolute_error: 104.7507 - val_loss: 129.5402 - val_mean_absolute_error: 129.5402\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 99.08407\n",
      "Epoch 252/500\n",
      "549/549 [==============================] - 0s 84us/step - loss: 108.6128 - mean_absolute_error: 108.6128 - val_loss: 128.6339 - val_mean_absolute_error: 128.6339\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 99.08407\n",
      "Epoch 253/500\n",
      "549/549 [==============================] - 0s 89us/step - loss: 104.3204 - mean_absolute_error: 104.3204 - val_loss: 109.5726 - val_mean_absolute_error: 109.5726\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 99.08407\n",
      "Epoch 254/500\n",
      "549/549 [==============================] - 0s 93us/step - loss: 104.4185 - mean_absolute_error: 104.4185 - val_loss: 110.7386 - val_mean_absolute_error: 110.7386\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 99.08407\n",
      "Epoch 255/500\n",
      "549/549 [==============================] - 0s 91us/step - loss: 104.5511 - mean_absolute_error: 104.5511 - val_loss: 119.8296 - val_mean_absolute_error: 119.8296\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 99.08407\n",
      "Epoch 256/500\n",
      "549/549 [==============================] - 0s 91us/step - loss: 103.3071 - mean_absolute_error: 103.3071 - val_loss: 115.2260 - val_mean_absolute_error: 115.2260\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 99.08407\n",
      "Epoch 257/500\n",
      "549/549 [==============================] - 0s 98us/step - loss: 103.8674 - mean_absolute_error: 103.8674 - val_loss: 121.9990 - val_mean_absolute_error: 121.9990\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 99.08407\n",
      "Epoch 258/500\n",
      "549/549 [==============================] - 0s 94us/step - loss: 103.5485 - mean_absolute_error: 103.5485 - val_loss: 117.8408 - val_mean_absolute_error: 117.8408\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 99.08407\n",
      "Epoch 259/500\n",
      "549/549 [==============================] - 0s 96us/step - loss: 103.8994 - mean_absolute_error: 103.8994 - val_loss: 132.8761 - val_mean_absolute_error: 132.8761\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 99.08407\n",
      "Epoch 260/500\n",
      "549/549 [==============================] - 0s 94us/step - loss: 104.3125 - mean_absolute_error: 104.3125 - val_loss: 134.4806 - val_mean_absolute_error: 134.4806\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 99.08407\n",
      "Epoch 261/500\n",
      "549/549 [==============================] - 0s 94us/step - loss: 106.9810 - mean_absolute_error: 106.9810 - val_loss: 130.0246 - val_mean_absolute_error: 130.0246\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 99.08407\n",
      "Epoch 262/500\n",
      "549/549 [==============================] - 0s 89us/step - loss: 105.1671 - mean_absolute_error: 105.1671 - val_loss: 120.7135 - val_mean_absolute_error: 120.7135\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 99.08407\n",
      "Epoch 263/500\n",
      "549/549 [==============================] - 0s 89us/step - loss: 103.3448 - mean_absolute_error: 103.3448 - val_loss: 131.0514 - val_mean_absolute_error: 131.0514\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 99.08407\n",
      "Epoch 264/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 106.3593 - mean_absolute_error: 106.3593 - val_loss: 139.2983 - val_mean_absolute_error: 139.2983\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 99.08407\n",
      "Epoch 265/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 106.4628 - mean_absolute_error: 106.4628 - val_loss: 124.8568 - val_mean_absolute_error: 124.8568\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 99.08407\n",
      "Epoch 266/500\n",
      "549/549 [==============================] - 0s 98us/step - loss: 104.3113 - mean_absolute_error: 104.3113 - val_loss: 117.8205 - val_mean_absolute_error: 117.8205\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 99.08407\n",
      "Epoch 267/500\n",
      "549/549 [==============================] - 0s 102us/step - loss: 102.4353 - mean_absolute_error: 102.4353 - val_loss: 115.2675 - val_mean_absolute_error: 115.2675\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 99.08407\n",
      "Epoch 268/500\n",
      "549/549 [==============================] - 0s 85us/step - loss: 102.3744 - mean_absolute_error: 102.3744 - val_loss: 107.4027 - val_mean_absolute_error: 107.4027\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 99.08407\n",
      "Epoch 269/500\n",
      "549/549 [==============================] - 0s 95us/step - loss: 103.6366 - mean_absolute_error: 103.6366 - val_loss: 116.7872 - val_mean_absolute_error: 116.7872\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 99.08407\n",
      "Epoch 270/500\n",
      "549/549 [==============================] - 0s 93us/step - loss: 103.8409 - mean_absolute_error: 103.8409 - val_loss: 119.2940 - val_mean_absolute_error: 119.2940\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 99.08407\n",
      "Epoch 271/500\n",
      "549/549 [==============================] - 0s 90us/step - loss: 104.6140 - mean_absolute_error: 104.6140 - val_loss: 127.0667 - val_mean_absolute_error: 127.0667\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 99.08407\n",
      "Epoch 272/500\n",
      "549/549 [==============================] - 0s 104us/step - loss: 105.3373 - mean_absolute_error: 105.3373 - val_loss: 120.5093 - val_mean_absolute_error: 120.5093\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 99.08407\n",
      "Epoch 273/500\n",
      "549/549 [==============================] - 0s 103us/step - loss: 102.5695 - mean_absolute_error: 102.5695 - val_loss: 130.0171 - val_mean_absolute_error: 130.0171\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 99.08407\n",
      "Epoch 274/500\n",
      "549/549 [==============================] - 0s 102us/step - loss: 103.9604 - mean_absolute_error: 103.9604 - val_loss: 130.8997 - val_mean_absolute_error: 130.8997\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 99.08407\n",
      "Epoch 275/500\n",
      "549/549 [==============================] - 0s 94us/step - loss: 103.5735 - mean_absolute_error: 103.5735 - val_loss: 127.6398 - val_mean_absolute_error: 127.6398\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 99.08407\n",
      "Epoch 276/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 103.0960 - mean_absolute_error: 103.0960 - val_loss: 131.7742 - val_mean_absolute_error: 131.7742\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 99.08407\n",
      "Epoch 277/500\n",
      "549/549 [==============================] - 0s 107us/step - loss: 107.2196 - mean_absolute_error: 107.2196 - val_loss: 114.6233 - val_mean_absolute_error: 114.6233\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 99.08407\n",
      "Epoch 278/500\n",
      "549/549 [==============================] - 0s 96us/step - loss: 105.0213 - mean_absolute_error: 105.0213 - val_loss: 114.9286 - val_mean_absolute_error: 114.9286\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 99.08407\n",
      "Epoch 279/500\n",
      "549/549 [==============================] - 0s 87us/step - loss: 104.5630 - mean_absolute_error: 104.5630 - val_loss: 110.9276 - val_mean_absolute_error: 110.9276\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 99.08407\n",
      "Epoch 280/500\n",
      "549/549 [==============================] - 0s 107us/step - loss: 103.5079 - mean_absolute_error: 103.5079 - val_loss: 107.9601 - val_mean_absolute_error: 107.9601\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 99.08407\n",
      "Epoch 281/500\n",
      "549/549 [==============================] - 0s 100us/step - loss: 104.7147 - mean_absolute_error: 104.7147 - val_loss: 105.3654 - val_mean_absolute_error: 105.3654\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 99.08407\n",
      "Epoch 282/500\n",
      "549/549 [==============================] - 0s 98us/step - loss: 107.3404 - mean_absolute_error: 107.3404 - val_loss: 102.6749 - val_mean_absolute_error: 102.6749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00282: val_loss did not improve from 99.08407\n",
      "Epoch 283/500\n",
      "549/549 [==============================] - 0s 104us/step - loss: 109.2831 - mean_absolute_error: 109.2831 - val_loss: 94.4948 - val_mean_absolute_error: 94.4948\n",
      "\n",
      "Epoch 00283: val_loss improved from 99.08407 to 94.49476, saving model to GS-mad적용마스크date-Weights-283--94.49476-cat02-vf05.hdf5\n",
      "Epoch 284/500\n",
      "549/549 [==============================] - 0s 89us/step - loss: 109.1213 - mean_absolute_error: 109.1213 - val_loss: 111.0509 - val_mean_absolute_error: 111.0509\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 94.49476\n",
      "Epoch 285/500\n",
      "549/549 [==============================] - 0s 94us/step - loss: 107.4850 - mean_absolute_error: 107.4850 - val_loss: 119.3360 - val_mean_absolute_error: 119.3360\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 94.49476\n",
      "Epoch 286/500\n",
      "549/549 [==============================] - 0s 98us/step - loss: 104.8458 - mean_absolute_error: 104.8458 - val_loss: 110.0876 - val_mean_absolute_error: 110.0876\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 94.49476\n",
      "Epoch 287/500\n",
      "549/549 [==============================] - 0s 94us/step - loss: 104.9978 - mean_absolute_error: 104.9978 - val_loss: 110.9613 - val_mean_absolute_error: 110.9613\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 94.49476\n",
      "Epoch 288/500\n",
      "549/549 [==============================] - 0s 91us/step - loss: 104.2346 - mean_absolute_error: 104.2346 - val_loss: 123.7116 - val_mean_absolute_error: 123.7116\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 94.49476\n",
      "Epoch 289/500\n",
      "549/549 [==============================] - 0s 91us/step - loss: 101.9888 - mean_absolute_error: 101.9888 - val_loss: 108.6426 - val_mean_absolute_error: 108.6426\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 94.49476\n",
      "Epoch 290/500\n",
      "549/549 [==============================] - 0s 89us/step - loss: 104.7444 - mean_absolute_error: 104.7444 - val_loss: 110.5228 - val_mean_absolute_error: 110.5228\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 94.49476\n",
      "Epoch 291/500\n",
      "549/549 [==============================] - 0s 93us/step - loss: 101.5451 - mean_absolute_error: 101.5451 - val_loss: 111.0685 - val_mean_absolute_error: 111.0685\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 94.49476\n",
      "Epoch 292/500\n",
      "549/549 [==============================] - 0s 94us/step - loss: 103.0863 - mean_absolute_error: 103.0863 - val_loss: 129.4168 - val_mean_absolute_error: 129.4168\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 94.49476\n",
      "Epoch 293/500\n",
      "549/549 [==============================] - 0s 91us/step - loss: 102.3938 - mean_absolute_error: 102.3938 - val_loss: 113.8700 - val_mean_absolute_error: 113.8700\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 94.49476\n",
      "Epoch 294/500\n",
      "549/549 [==============================] - 0s 104us/step - loss: 102.7757 - mean_absolute_error: 102.7757 - val_loss: 123.2273 - val_mean_absolute_error: 123.2273\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 94.49476\n",
      "Epoch 295/500\n",
      "549/549 [==============================] - 0s 105us/step - loss: 102.6793 - mean_absolute_error: 102.6793 - val_loss: 124.0543 - val_mean_absolute_error: 124.0543\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 94.49476\n",
      "Epoch 296/500\n",
      "549/549 [==============================] - 0s 104us/step - loss: 101.7250 - mean_absolute_error: 101.7250 - val_loss: 122.2324 - val_mean_absolute_error: 122.2324\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 94.49476\n",
      "Epoch 297/500\n",
      "549/549 [==============================] - 0s 93us/step - loss: 101.2242 - mean_absolute_error: 101.2242 - val_loss: 119.1956 - val_mean_absolute_error: 119.1956\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 94.49476\n",
      "Epoch 298/500\n",
      "549/549 [==============================] - 0s 103us/step - loss: 101.3882 - mean_absolute_error: 101.3882 - val_loss: 125.1306 - val_mean_absolute_error: 125.1306\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 94.49476\n",
      "Epoch 299/500\n",
      "549/549 [==============================] - 0s 104us/step - loss: 103.6452 - mean_absolute_error: 103.6452 - val_loss: 134.8737 - val_mean_absolute_error: 134.8737\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 94.49476\n",
      "Epoch 300/500\n",
      "549/549 [==============================] - 0s 104us/step - loss: 104.3042 - mean_absolute_error: 104.3042 - val_loss: 122.6596 - val_mean_absolute_error: 122.6596\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 94.49476\n",
      "Epoch 301/500\n",
      "549/549 [==============================] - 0s 120us/step - loss: 102.0974 - mean_absolute_error: 102.0974 - val_loss: 116.9878 - val_mean_absolute_error: 116.9878\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 94.49476\n",
      "Epoch 302/500\n",
      "549/549 [==============================] - 0s 95us/step - loss: 101.7708 - mean_absolute_error: 101.7708 - val_loss: 112.6404 - val_mean_absolute_error: 112.6404\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 94.49476\n",
      "Epoch 303/500\n",
      "549/549 [==============================] - 0s 93us/step - loss: 105.1737 - mean_absolute_error: 105.1737 - val_loss: 109.0036 - val_mean_absolute_error: 109.0036\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 94.49476\n",
      "Epoch 304/500\n",
      "549/549 [==============================] - 0s 100us/step - loss: 100.9494 - mean_absolute_error: 100.9494 - val_loss: 113.2378 - val_mean_absolute_error: 113.2378\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 94.49476\n",
      "Epoch 305/500\n",
      "549/549 [==============================] - 0s 93us/step - loss: 100.7085 - mean_absolute_error: 100.7085 - val_loss: 110.0142 - val_mean_absolute_error: 110.0142\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 94.49476\n",
      "Epoch 306/500\n",
      "549/549 [==============================] - 0s 96us/step - loss: 101.3962 - mean_absolute_error: 101.3962 - val_loss: 119.0603 - val_mean_absolute_error: 119.0603\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 94.49476\n",
      "Epoch 307/500\n",
      "549/549 [==============================] - 0s 120us/step - loss: 101.7435 - mean_absolute_error: 101.7435 - val_loss: 117.1005 - val_mean_absolute_error: 117.1005\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 94.49476\n",
      "Epoch 308/500\n",
      "549/549 [==============================] - 0s 98us/step - loss: 102.3170 - mean_absolute_error: 102.3170 - val_loss: 126.6381 - val_mean_absolute_error: 126.6381\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 94.49476\n",
      "Epoch 309/500\n",
      "549/549 [==============================] - 0s 95us/step - loss: 103.0023 - mean_absolute_error: 103.0023 - val_loss: 117.0310 - val_mean_absolute_error: 117.0310\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 94.49476\n",
      "Epoch 310/500\n",
      "549/549 [==============================] - 0s 138us/step - loss: 103.2786 - mean_absolute_error: 103.2786 - val_loss: 109.6387 - val_mean_absolute_error: 109.6387\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 94.49476\n",
      "Epoch 311/500\n",
      "549/549 [==============================] - 0s 133us/step - loss: 101.5187 - mean_absolute_error: 101.5187 - val_loss: 126.4707 - val_mean_absolute_error: 126.4707\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 94.49476\n",
      "Epoch 312/500\n",
      "549/549 [==============================] - 0s 105us/step - loss: 101.1481 - mean_absolute_error: 101.1481 - val_loss: 108.9285 - val_mean_absolute_error: 108.9285\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 94.49476\n",
      "Epoch 313/500\n",
      "549/549 [==============================] - 0s 96us/step - loss: 101.9140 - mean_absolute_error: 101.9140 - val_loss: 113.3325 - val_mean_absolute_error: 113.3325\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 94.49476\n",
      "Epoch 314/500\n",
      "549/549 [==============================] - 0s 110us/step - loss: 100.9643 - mean_absolute_error: 100.9643 - val_loss: 105.6817 - val_mean_absolute_error: 105.6817\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 94.49476\n",
      "Epoch 315/500\n",
      "549/549 [==============================] - 0s 102us/step - loss: 103.5389 - mean_absolute_error: 103.5389 - val_loss: 116.5783 - val_mean_absolute_error: 116.5783\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 94.49476\n",
      "Epoch 316/500\n",
      "549/549 [==============================] - 0s 94us/step - loss: 102.0169 - mean_absolute_error: 102.0169 - val_loss: 121.4976 - val_mean_absolute_error: 121.4976\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 94.49476\n",
      "Epoch 317/500\n",
      "549/549 [==============================] - 0s 100us/step - loss: 100.5177 - mean_absolute_error: 100.5177 - val_loss: 134.3411 - val_mean_absolute_error: 134.3411\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 94.49476\n",
      "Epoch 318/500\n",
      "549/549 [==============================] - 0s 100us/step - loss: 104.6204 - mean_absolute_error: 104.6204 - val_loss: 121.1894 - val_mean_absolute_error: 121.1894\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 94.49476\n",
      "Epoch 319/500\n",
      "549/549 [==============================] - 0s 98us/step - loss: 101.6675 - mean_absolute_error: 101.6675 - val_loss: 114.7990 - val_mean_absolute_error: 114.7990\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 94.49476\n",
      "Epoch 320/500\n",
      "549/549 [==============================] - 0s 96us/step - loss: 100.4445 - mean_absolute_error: 100.4445 - val_loss: 113.7410 - val_mean_absolute_error: 113.7410\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 94.49476\n",
      "Epoch 321/500\n",
      "549/549 [==============================] - 0s 100us/step - loss: 103.2478 - mean_absolute_error: 103.2478 - val_loss: 120.1767 - val_mean_absolute_error: 120.1767\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 94.49476\n",
      "Epoch 322/500\n",
      "549/549 [==============================] - 0s 105us/step - loss: 102.2749 - mean_absolute_error: 102.2749 - val_loss: 112.3399 - val_mean_absolute_error: 112.3399\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 94.49476\n",
      "Epoch 323/500\n",
      "549/549 [==============================] - 0s 117us/step - loss: 100.8358 - mean_absolute_error: 100.8358 - val_loss: 115.4708 - val_mean_absolute_error: 115.4708\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 94.49476\n",
      "Epoch 324/500\n",
      "549/549 [==============================] - 0s 111us/step - loss: 99.5802 - mean_absolute_error: 99.5802 - val_loss: 103.0199 - val_mean_absolute_error: 103.0199\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 94.49476\n",
      "Epoch 325/500\n",
      "549/549 [==============================] - 0s 104us/step - loss: 101.6957 - mean_absolute_error: 101.6957 - val_loss: 103.3761 - val_mean_absolute_error: 103.3761\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 94.49476\n",
      "Epoch 326/500\n",
      "549/549 [==============================] - 0s 114us/step - loss: 102.9719 - mean_absolute_error: 102.9719 - val_loss: 113.2980 - val_mean_absolute_error: 113.2980\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 94.49476\n",
      "Epoch 327/500\n",
      "549/549 [==============================] - 0s 113us/step - loss: 101.1630 - mean_absolute_error: 101.1630 - val_loss: 116.3342 - val_mean_absolute_error: 116.3342\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 94.49476\n",
      "Epoch 328/500\n",
      "549/549 [==============================] - 0s 111us/step - loss: 101.9197 - mean_absolute_error: 101.9197 - val_loss: 132.9442 - val_mean_absolute_error: 132.9442\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 94.49476\n",
      "Epoch 329/500\n",
      "549/549 [==============================] - 0s 107us/step - loss: 103.3108 - mean_absolute_error: 103.3108 - val_loss: 138.2865 - val_mean_absolute_error: 138.2865\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 94.49476\n",
      "Epoch 330/500\n",
      "549/549 [==============================] - 0s 109us/step - loss: 102.3034 - mean_absolute_error: 102.3034 - val_loss: 129.7029 - val_mean_absolute_error: 129.7029\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 94.49476\n",
      "Epoch 331/500\n",
      "549/549 [==============================] - 0s 122us/step - loss: 101.7135 - mean_absolute_error: 101.7135 - val_loss: 119.5688 - val_mean_absolute_error: 119.5688\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 94.49476\n",
      "Epoch 332/500\n",
      "549/549 [==============================] - 0s 107us/step - loss: 99.4185 - mean_absolute_error: 99.4185 - val_loss: 124.4645 - val_mean_absolute_error: 124.4645\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 94.49476\n",
      "Epoch 333/500\n",
      "549/549 [==============================] - 0s 107us/step - loss: 101.2291 - mean_absolute_error: 101.2291 - val_loss: 120.6314 - val_mean_absolute_error: 120.6314\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 94.49476\n",
      "Epoch 334/500\n",
      "549/549 [==============================] - 0s 104us/step - loss: 102.6519 - mean_absolute_error: 102.6519 - val_loss: 117.0472 - val_mean_absolute_error: 117.0472\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 94.49476\n",
      "Epoch 335/500\n",
      "549/549 [==============================] - 0s 127us/step - loss: 99.3349 - mean_absolute_error: 99.3349 - val_loss: 111.6246 - val_mean_absolute_error: 111.6246\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 94.49476\n",
      "Epoch 336/500\n",
      "549/549 [==============================] - 0s 105us/step - loss: 99.2712 - mean_absolute_error: 99.2712 - val_loss: 105.6026 - val_mean_absolute_error: 105.6026\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 94.49476\n",
      "Epoch 337/500\n",
      "549/549 [==============================] - 0s 116us/step - loss: 101.3918 - mean_absolute_error: 101.3918 - val_loss: 124.8277 - val_mean_absolute_error: 124.8277\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 94.49476\n",
      "Epoch 338/500\n",
      "549/549 [==============================] - 0s 111us/step - loss: 102.3734 - mean_absolute_error: 102.3734 - val_loss: 129.5792 - val_mean_absolute_error: 129.5792\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 94.49476\n",
      "Epoch 339/500\n",
      "549/549 [==============================] - 0s 121us/step - loss: 101.8273 - mean_absolute_error: 101.8273 - val_loss: 134.0542 - val_mean_absolute_error: 134.0542\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 94.49476\n",
      "Epoch 340/500\n",
      "549/549 [==============================] - 0s 113us/step - loss: 100.9915 - mean_absolute_error: 100.9915 - val_loss: 122.2485 - val_mean_absolute_error: 122.2485\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 94.49476\n",
      "Epoch 341/500\n",
      "549/549 [==============================] - 0s 109us/step - loss: 100.9815 - mean_absolute_error: 100.9815 - val_loss: 116.5827 - val_mean_absolute_error: 116.5827\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 94.49476\n",
      "Epoch 342/500\n",
      "549/549 [==============================] - 0s 122us/step - loss: 100.4754 - mean_absolute_error: 100.4754 - val_loss: 120.9947 - val_mean_absolute_error: 120.9947\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 94.49476\n",
      "Epoch 343/500\n",
      "549/549 [==============================] - 0s 105us/step - loss: 101.4862 - mean_absolute_error: 101.4862 - val_loss: 126.2243 - val_mean_absolute_error: 126.2243\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 94.49476\n",
      "Epoch 344/500\n",
      "549/549 [==============================] - 0s 124us/step - loss: 100.6405 - mean_absolute_error: 100.6405 - val_loss: 122.3847 - val_mean_absolute_error: 122.3847\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 94.49476\n",
      "Epoch 345/500\n",
      "549/549 [==============================] - 0s 132us/step - loss: 100.4548 - mean_absolute_error: 100.4548 - val_loss: 117.9545 - val_mean_absolute_error: 117.9545\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 94.49476\n",
      "Epoch 346/500\n",
      "549/549 [==============================] - 0s 123us/step - loss: 101.7290 - mean_absolute_error: 101.7290 - val_loss: 109.9356 - val_mean_absolute_error: 109.9356\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 94.49476\n",
      "Epoch 347/500\n",
      "549/549 [==============================] - 0s 120us/step - loss: 99.2866 - mean_absolute_error: 99.2866 - val_loss: 105.2159 - val_mean_absolute_error: 105.2159\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 94.49476\n",
      "Epoch 348/500\n",
      "549/549 [==============================] - 0s 124us/step - loss: 99.0159 - mean_absolute_error: 99.0159 - val_loss: 117.1736 - val_mean_absolute_error: 117.1736\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 94.49476\n",
      "Epoch 349/500\n",
      "549/549 [==============================] - 0s 118us/step - loss: 101.0638 - mean_absolute_error: 101.0638 - val_loss: 125.0480 - val_mean_absolute_error: 125.0480\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 94.49476\n",
      "Epoch 350/500\n",
      "549/549 [==============================] - 0s 109us/step - loss: 98.0231 - mean_absolute_error: 98.0231 - val_loss: 121.6642 - val_mean_absolute_error: 121.6642\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 94.49476\n",
      "Epoch 351/500\n",
      "549/549 [==============================] - 0s 107us/step - loss: 101.8759 - mean_absolute_error: 101.8759 - val_loss: 111.4104 - val_mean_absolute_error: 111.4104\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 94.49476\n",
      "Epoch 352/500\n",
      "549/549 [==============================] - 0s 129us/step - loss: 104.4399 - mean_absolute_error: 104.4399 - val_loss: 112.6822 - val_mean_absolute_error: 112.6822\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 94.49476\n",
      "Epoch 353/500\n",
      "549/549 [==============================] - 0s 113us/step - loss: 101.2367 - mean_absolute_error: 101.2367 - val_loss: 119.5923 - val_mean_absolute_error: 119.5923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00353: val_loss did not improve from 94.49476\n",
      "Epoch 354/500\n",
      "549/549 [==============================] - 0s 136us/step - loss: 97.0884 - mean_absolute_error: 97.0884 - val_loss: 123.4664 - val_mean_absolute_error: 123.4664\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 94.49476\n",
      "Epoch 355/500\n",
      "549/549 [==============================] - 0s 127us/step - loss: 99.1831 - mean_absolute_error: 99.1831 - val_loss: 113.4555 - val_mean_absolute_error: 113.4555\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 94.49476\n",
      "Epoch 356/500\n",
      "549/549 [==============================] - 0s 120us/step - loss: 100.8413 - mean_absolute_error: 100.8413 - val_loss: 112.8246 - val_mean_absolute_error: 112.8246\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 94.49476\n",
      "Epoch 357/500\n",
      "549/549 [==============================] - 0s 134us/step - loss: 99.2328 - mean_absolute_error: 99.2328 - val_loss: 115.8220 - val_mean_absolute_error: 115.8220\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 94.49476\n",
      "Epoch 358/500\n",
      "549/549 [==============================] - 0s 133us/step - loss: 97.1549 - mean_absolute_error: 97.1549 - val_loss: 110.5124 - val_mean_absolute_error: 110.5124\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 94.49476\n",
      "Epoch 359/500\n",
      "549/549 [==============================] - 0s 134us/step - loss: 98.9069 - mean_absolute_error: 98.9069 - val_loss: 113.7944 - val_mean_absolute_error: 113.7944\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 94.49476\n",
      "Epoch 360/500\n",
      "549/549 [==============================] - 0s 142us/step - loss: 100.5535 - mean_absolute_error: 100.5535 - val_loss: 104.7766 - val_mean_absolute_error: 104.7766\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 94.49476\n",
      "Epoch 361/500\n",
      "549/549 [==============================] - 0s 133us/step - loss: 99.7093 - mean_absolute_error: 99.7093 - val_loss: 105.0275 - val_mean_absolute_error: 105.0275\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 94.49476\n",
      "Epoch 362/500\n",
      "549/549 [==============================] - 0s 127us/step - loss: 98.3148 - mean_absolute_error: 98.3148 - val_loss: 116.6294 - val_mean_absolute_error: 116.6294\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 94.49476\n",
      "Epoch 363/500\n",
      "549/549 [==============================] - 0s 127us/step - loss: 98.0310 - mean_absolute_error: 98.0310 - val_loss: 114.4587 - val_mean_absolute_error: 114.4587\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 94.49476\n",
      "Epoch 364/500\n",
      "549/549 [==============================] - 0s 140us/step - loss: 96.7419 - mean_absolute_error: 96.7419 - val_loss: 113.3183 - val_mean_absolute_error: 113.3183\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 94.49476\n",
      "Epoch 365/500\n",
      "549/549 [==============================] - 0s 139us/step - loss: 100.8877 - mean_absolute_error: 100.8877 - val_loss: 109.4088 - val_mean_absolute_error: 109.4088\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 94.49476\n",
      "Epoch 366/500\n",
      "549/549 [==============================] - 0s 132us/step - loss: 97.2168 - mean_absolute_error: 97.2168 - val_loss: 122.5764 - val_mean_absolute_error: 122.5764\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 94.49476\n",
      "Epoch 367/500\n",
      "549/549 [==============================] - 0s 127us/step - loss: 96.6812 - mean_absolute_error: 96.6812 - val_loss: 121.7688 - val_mean_absolute_error: 121.7688\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 94.49476\n",
      "Epoch 368/500\n",
      "549/549 [==============================] - 0s 127us/step - loss: 97.1076 - mean_absolute_error: 97.1076 - val_loss: 112.4363 - val_mean_absolute_error: 112.4363\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 94.49476\n",
      "Epoch 369/500\n",
      "549/549 [==============================] - 0s 124us/step - loss: 98.1884 - mean_absolute_error: 98.1884 - val_loss: 116.4085 - val_mean_absolute_error: 116.4085\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 94.49476\n",
      "Epoch 370/500\n",
      "549/549 [==============================] - 0s 143us/step - loss: 97.9854 - mean_absolute_error: 97.9854 - val_loss: 112.8785 - val_mean_absolute_error: 112.8785\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 94.49476\n",
      "Epoch 371/500\n",
      "549/549 [==============================] - 0s 151us/step - loss: 97.3075 - mean_absolute_error: 97.3075 - val_loss: 108.8838 - val_mean_absolute_error: 108.8838\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 94.49476\n",
      "Epoch 372/500\n",
      "549/549 [==============================] - 0s 122us/step - loss: 98.0962 - mean_absolute_error: 98.0962 - val_loss: 123.8165 - val_mean_absolute_error: 123.8165\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 94.49476\n",
      "Epoch 373/500\n",
      "549/549 [==============================] - 0s 118us/step - loss: 96.4669 - mean_absolute_error: 96.4669 - val_loss: 124.9370 - val_mean_absolute_error: 124.9370\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 94.49476\n",
      "Epoch 374/500\n",
      "549/549 [==============================] - 0s 125us/step - loss: 98.1348 - mean_absolute_error: 98.1348 - val_loss: 119.9342 - val_mean_absolute_error: 119.9342\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 94.49476\n",
      "Epoch 375/500\n",
      "549/549 [==============================] - 0s 118us/step - loss: 98.4067 - mean_absolute_error: 98.4067 - val_loss: 123.6297 - val_mean_absolute_error: 123.6297\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 94.49476\n",
      "Epoch 376/500\n",
      "549/549 [==============================] - 0s 124us/step - loss: 101.5181 - mean_absolute_error: 101.5181 - val_loss: 126.5012 - val_mean_absolute_error: 126.5012\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 94.49476\n",
      "Epoch 377/500\n",
      "549/549 [==============================] - 0s 142us/step - loss: 96.1958 - mean_absolute_error: 96.1958 - val_loss: 115.9731 - val_mean_absolute_error: 115.9731\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 94.49476\n",
      "Epoch 378/500\n",
      "549/549 [==============================] - 0s 113us/step - loss: 95.5249 - mean_absolute_error: 95.5249 - val_loss: 115.2003 - val_mean_absolute_error: 115.2003\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 94.49476\n",
      "Epoch 379/500\n",
      "549/549 [==============================] - 0s 133us/step - loss: 95.1298 - mean_absolute_error: 95.1298 - val_loss: 120.9426 - val_mean_absolute_error: 120.9426\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 94.49476\n",
      "Epoch 380/500\n",
      "549/549 [==============================] - 0s 124us/step - loss: 95.7986 - mean_absolute_error: 95.7986 - val_loss: 118.6155 - val_mean_absolute_error: 118.6155\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 94.49476\n",
      "Epoch 381/500\n",
      "549/549 [==============================] - 0s 134us/step - loss: 98.8641 - mean_absolute_error: 98.8641 - val_loss: 117.8495 - val_mean_absolute_error: 117.8495\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 94.49476\n",
      "Epoch 382/500\n",
      "549/549 [==============================] - 0s 105us/step - loss: 98.3971 - mean_absolute_error: 98.3971 - val_loss: 108.1071 - val_mean_absolute_error: 108.1071\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 94.49476\n",
      "Epoch 383/500\n",
      "549/549 [==============================] - 0s 134us/step - loss: 94.3816 - mean_absolute_error: 94.3816 - val_loss: 129.3013 - val_mean_absolute_error: 129.3013\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 94.49476\n",
      "Epoch 384/500\n",
      "549/549 [==============================] - 0s 133us/step - loss: 97.6698 - mean_absolute_error: 97.6698 - val_loss: 108.5202 - val_mean_absolute_error: 108.5202\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 94.49476\n",
      "Epoch 385/500\n",
      "549/549 [==============================] - 0s 133us/step - loss: 96.9078 - mean_absolute_error: 96.9078 - val_loss: 108.7675 - val_mean_absolute_error: 108.7675\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 94.49476\n",
      "Epoch 386/500\n",
      "549/549 [==============================] - 0s 136us/step - loss: 96.6058 - mean_absolute_error: 96.6058 - val_loss: 118.9563 - val_mean_absolute_error: 118.9563\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 94.49476\n",
      "Epoch 387/500\n",
      "549/549 [==============================] - 0s 124us/step - loss: 99.3385 - mean_absolute_error: 99.3385 - val_loss: 121.8819 - val_mean_absolute_error: 121.8819\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 94.49476\n",
      "Epoch 388/500\n",
      "549/549 [==============================] - 0s 111us/step - loss: 100.7089 - mean_absolute_error: 100.7089 - val_loss: 109.8705 - val_mean_absolute_error: 109.8705\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 94.49476\n",
      "Epoch 389/500\n",
      "549/549 [==============================] - 0s 144us/step - loss: 96.2730 - mean_absolute_error: 96.2730 - val_loss: 121.1667 - val_mean_absolute_error: 121.1667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00389: val_loss did not improve from 94.49476\n",
      "Epoch 390/500\n",
      "549/549 [==============================] - 0s 120us/step - loss: 95.5284 - mean_absolute_error: 95.5284 - val_loss: 122.7529 - val_mean_absolute_error: 122.7529\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 94.49476\n",
      "Epoch 391/500\n",
      "549/549 [==============================] - 0s 120us/step - loss: 98.4301 - mean_absolute_error: 98.4301 - val_loss: 134.3264 - val_mean_absolute_error: 134.3264\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 94.49476\n",
      "Epoch 392/500\n",
      "549/549 [==============================] - 0s 140us/step - loss: 105.3379 - mean_absolute_error: 105.3379 - val_loss: 129.8738 - val_mean_absolute_error: 129.8738\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 94.49476\n",
      "Epoch 393/500\n",
      "549/549 [==============================] - 0s 129us/step - loss: 97.6558 - mean_absolute_error: 97.6558 - val_loss: 115.9960 - val_mean_absolute_error: 115.9960\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 94.49476\n",
      "Epoch 394/500\n",
      "549/549 [==============================] - 0s 127us/step - loss: 95.1604 - mean_absolute_error: 95.1604 - val_loss: 123.0917 - val_mean_absolute_error: 123.0917\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 94.49476\n",
      "Epoch 395/500\n",
      "549/549 [==============================] - 0s 133us/step - loss: 98.7087 - mean_absolute_error: 98.7087 - val_loss: 116.7120 - val_mean_absolute_error: 116.7120\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 94.49476\n",
      "Epoch 396/500\n",
      "549/549 [==============================] - 0s 124us/step - loss: 94.9335 - mean_absolute_error: 94.9335 - val_loss: 132.0708 - val_mean_absolute_error: 132.0708\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 94.49476\n",
      "Epoch 397/500\n",
      "549/549 [==============================] - 0s 120us/step - loss: 97.5389 - mean_absolute_error: 97.5389 - val_loss: 120.8922 - val_mean_absolute_error: 120.8922\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 94.49476\n",
      "Epoch 398/500\n",
      "549/549 [==============================] - 0s 118us/step - loss: 95.1337 - mean_absolute_error: 95.1337 - val_loss: 110.4338 - val_mean_absolute_error: 110.4338\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 94.49476\n",
      "Epoch 399/500\n",
      "549/549 [==============================] - 0s 118us/step - loss: 95.0159 - mean_absolute_error: 95.0159 - val_loss: 120.1002 - val_mean_absolute_error: 120.1002\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 94.49476\n",
      "Epoch 400/500\n",
      "549/549 [==============================] - 0s 125us/step - loss: 93.7930 - mean_absolute_error: 93.7930 - val_loss: 131.6913 - val_mean_absolute_error: 131.6913\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 94.49476\n",
      "Epoch 401/500\n",
      "549/549 [==============================] - 0s 127us/step - loss: 103.5872 - mean_absolute_error: 103.5872 - val_loss: 131.3130 - val_mean_absolute_error: 131.3130\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 94.49476\n",
      "Epoch 402/500\n",
      "549/549 [==============================] - 0s 120us/step - loss: 99.8076 - mean_absolute_error: 99.8076 - val_loss: 131.0726 - val_mean_absolute_error: 131.0726\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 94.49476\n",
      "Epoch 403/500\n",
      "549/549 [==============================] - 0s 131us/step - loss: 96.4101 - mean_absolute_error: 96.4101 - val_loss: 120.0290 - val_mean_absolute_error: 120.0290\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 94.49476\n",
      "Epoch 404/500\n",
      "549/549 [==============================] - 0s 131us/step - loss: 94.1172 - mean_absolute_error: 94.1172 - val_loss: 115.7308 - val_mean_absolute_error: 115.7308\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 94.49476\n",
      "Epoch 405/500\n",
      "549/549 [==============================] - 0s 136us/step - loss: 94.6730 - mean_absolute_error: 94.6730 - val_loss: 120.3262 - val_mean_absolute_error: 120.3262\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 94.49476\n",
      "Epoch 406/500\n",
      "549/549 [==============================] - 0s 124us/step - loss: 91.9994 - mean_absolute_error: 91.9994 - val_loss: 118.8007 - val_mean_absolute_error: 118.8007\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 94.49476\n",
      "Epoch 407/500\n",
      "549/549 [==============================] - 0s 125us/step - loss: 94.9471 - mean_absolute_error: 94.9471 - val_loss: 120.8211 - val_mean_absolute_error: 120.8211\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 94.49476\n",
      "Epoch 408/500\n",
      "549/549 [==============================] - 0s 129us/step - loss: 94.0768 - mean_absolute_error: 94.0768 - val_loss: 111.7092 - val_mean_absolute_error: 111.7092\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 94.49476\n",
      "Epoch 409/500\n",
      "549/549 [==============================] - 0s 125us/step - loss: 93.4359 - mean_absolute_error: 93.4359 - val_loss: 118.7223 - val_mean_absolute_error: 118.7223\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 94.49476\n",
      "Epoch 410/500\n",
      "549/549 [==============================] - 0s 129us/step - loss: 95.0936 - mean_absolute_error: 95.0936 - val_loss: 111.9952 - val_mean_absolute_error: 111.9952\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 94.49476\n",
      "Epoch 411/500\n",
      "549/549 [==============================] - 0s 120us/step - loss: 92.7588 - mean_absolute_error: 92.7588 - val_loss: 118.9390 - val_mean_absolute_error: 118.9390\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 94.49476\n",
      "Epoch 412/500\n",
      "549/549 [==============================] - 0s 125us/step - loss: 92.2606 - mean_absolute_error: 92.2606 - val_loss: 114.8550 - val_mean_absolute_error: 114.8550\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 94.49476\n",
      "Epoch 413/500\n",
      "549/549 [==============================] - 0s 125us/step - loss: 93.1090 - mean_absolute_error: 93.1090 - val_loss: 110.1707 - val_mean_absolute_error: 110.1707\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 94.49476\n",
      "Epoch 414/500\n",
      "549/549 [==============================] - 0s 134us/step - loss: 95.2471 - mean_absolute_error: 95.2471 - val_loss: 115.9358 - val_mean_absolute_error: 115.9358\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 94.49476\n",
      "Epoch 415/500\n",
      "549/549 [==============================] - 0s 147us/step - loss: 93.4357 - mean_absolute_error: 93.4357 - val_loss: 120.4419 - val_mean_absolute_error: 120.4419\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 94.49476\n",
      "Epoch 416/500\n",
      "549/549 [==============================] - 0s 133us/step - loss: 97.5324 - mean_absolute_error: 97.5324 - val_loss: 118.2335 - val_mean_absolute_error: 118.2335\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 94.49476\n",
      "Epoch 417/500\n",
      "549/549 [==============================] - 0s 125us/step - loss: 94.1807 - mean_absolute_error: 94.1807 - val_loss: 127.6590 - val_mean_absolute_error: 127.6590\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 94.49476\n",
      "Epoch 418/500\n",
      "549/549 [==============================] - 0s 138us/step - loss: 95.9248 - mean_absolute_error: 95.9248 - val_loss: 118.2643 - val_mean_absolute_error: 118.2643\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 94.49476\n",
      "Epoch 419/500\n",
      "549/549 [==============================] - 0s 125us/step - loss: 91.6519 - mean_absolute_error: 91.6519 - val_loss: 120.5751 - val_mean_absolute_error: 120.5751\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 94.49476\n",
      "Epoch 420/500\n",
      "549/549 [==============================] - 0s 138us/step - loss: 94.3500 - mean_absolute_error: 94.3500 - val_loss: 124.9348 - val_mean_absolute_error: 124.9348\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 94.49476\n",
      "Epoch 421/500\n",
      "549/549 [==============================] - 0s 116us/step - loss: 93.7786 - mean_absolute_error: 93.7786 - val_loss: 116.3906 - val_mean_absolute_error: 116.3906\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 94.49476\n",
      "Epoch 422/500\n",
      "549/549 [==============================] - 0s 129us/step - loss: 93.1479 - mean_absolute_error: 93.1479 - val_loss: 120.3565 - val_mean_absolute_error: 120.3565\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 94.49476\n",
      "Epoch 423/500\n",
      "549/549 [==============================] - 0s 133us/step - loss: 91.2788 - mean_absolute_error: 91.2788 - val_loss: 130.6060 - val_mean_absolute_error: 130.6060\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 94.49476\n",
      "Epoch 424/500\n",
      "549/549 [==============================] - 0s 131us/step - loss: 95.0134 - mean_absolute_error: 95.0134 - val_loss: 121.1480 - val_mean_absolute_error: 121.1480\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 94.49476\n",
      "Epoch 425/500\n",
      "549/549 [==============================] - 0s 133us/step - loss: 95.1238 - mean_absolute_error: 95.1238 - val_loss: 114.9098 - val_mean_absolute_error: 114.9098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00425: val_loss did not improve from 94.49476\n",
      "Epoch 426/500\n",
      "549/549 [==============================] - 0s 129us/step - loss: 92.1343 - mean_absolute_error: 92.1343 - val_loss: 117.1462 - val_mean_absolute_error: 117.1462\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 94.49476\n",
      "Epoch 427/500\n",
      "549/549 [==============================] - 0s 124us/step - loss: 91.9291 - mean_absolute_error: 91.9291 - val_loss: 123.0192 - val_mean_absolute_error: 123.0192\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 94.49476\n",
      "Epoch 428/500\n",
      "549/549 [==============================] - 0s 143us/step - loss: 91.6228 - mean_absolute_error: 91.6228 - val_loss: 113.5496 - val_mean_absolute_error: 113.5496\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 94.49476\n",
      "Epoch 429/500\n",
      "549/549 [==============================] - 0s 129us/step - loss: 93.1998 - mean_absolute_error: 93.1998 - val_loss: 118.3287 - val_mean_absolute_error: 118.3287\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 94.49476\n",
      "Epoch 430/500\n",
      "549/549 [==============================] - 0s 131us/step - loss: 93.3682 - mean_absolute_error: 93.3682 - val_loss: 113.2744 - val_mean_absolute_error: 113.2744\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 94.49476\n",
      "Epoch 431/500\n",
      "549/549 [==============================] - 0s 136us/step - loss: 91.6052 - mean_absolute_error: 91.6052 - val_loss: 113.9978 - val_mean_absolute_error: 113.9978\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 94.49476\n",
      "Epoch 432/500\n",
      "549/549 [==============================] - 0s 140us/step - loss: 93.9757 - mean_absolute_error: 93.9757 - val_loss: 128.5065 - val_mean_absolute_error: 128.5065\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 94.49476\n",
      "Epoch 433/500\n",
      "549/549 [==============================] - 0s 131us/step - loss: 97.1583 - mean_absolute_error: 97.1583 - val_loss: 119.8565 - val_mean_absolute_error: 119.8565\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 94.49476\n",
      "Epoch 434/500\n",
      "549/549 [==============================] - 0s 144us/step - loss: 93.1823 - mean_absolute_error: 93.1823 - val_loss: 121.7046 - val_mean_absolute_error: 121.7046\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 94.49476\n",
      "Epoch 435/500\n",
      "549/549 [==============================] - 0s 142us/step - loss: 91.3991 - mean_absolute_error: 91.3991 - val_loss: 108.0153 - val_mean_absolute_error: 108.0153\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 94.49476\n",
      "Epoch 436/500\n",
      "549/549 [==============================] - 0s 129us/step - loss: 91.9767 - mean_absolute_error: 91.9767 - val_loss: 113.4541 - val_mean_absolute_error: 113.4541\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 94.49476\n",
      "Epoch 437/500\n",
      "549/549 [==============================] - 0s 149us/step - loss: 90.9602 - mean_absolute_error: 90.9602 - val_loss: 111.5190 - val_mean_absolute_error: 111.5190\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 94.49476\n",
      "Epoch 438/500\n",
      "549/549 [==============================] - 0s 147us/step - loss: 92.7634 - mean_absolute_error: 92.7634 - val_loss: 118.9342 - val_mean_absolute_error: 118.9342\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 94.49476\n",
      "Epoch 439/500\n",
      "549/549 [==============================] - 0s 134us/step - loss: 93.4496 - mean_absolute_error: 93.4496 - val_loss: 122.0280 - val_mean_absolute_error: 122.0280\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 94.49476\n",
      "Epoch 440/500\n",
      "549/549 [==============================] - 0s 139us/step - loss: 94.1409 - mean_absolute_error: 94.1409 - val_loss: 111.4233 - val_mean_absolute_error: 111.4233\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 94.49476\n",
      "Epoch 441/500\n",
      "549/549 [==============================] - 0s 127us/step - loss: 90.8563 - mean_absolute_error: 90.8563 - val_loss: 123.1485 - val_mean_absolute_error: 123.1485\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 94.49476\n",
      "Epoch 442/500\n",
      "549/549 [==============================] - 0s 142us/step - loss: 90.0861 - mean_absolute_error: 90.0861 - val_loss: 114.1600 - val_mean_absolute_error: 114.1600\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 94.49476\n",
      "Epoch 443/500\n",
      "549/549 [==============================] - 0s 133us/step - loss: 92.7101 - mean_absolute_error: 92.7101 - val_loss: 111.0166 - val_mean_absolute_error: 111.0166\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 94.49476\n",
      "Epoch 444/500\n",
      "549/549 [==============================] - 0s 133us/step - loss: 90.7869 - mean_absolute_error: 90.7869 - val_loss: 117.9933 - val_mean_absolute_error: 117.9933\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 94.49476\n",
      "Epoch 445/500\n",
      "549/549 [==============================] - 0s 145us/step - loss: 90.1073 - mean_absolute_error: 90.1073 - val_loss: 113.7953 - val_mean_absolute_error: 113.7953\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 94.49476\n",
      "Epoch 446/500\n",
      "549/549 [==============================] - 0s 134us/step - loss: 90.8384 - mean_absolute_error: 90.8384 - val_loss: 114.4684 - val_mean_absolute_error: 114.4684\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 94.49476\n",
      "Epoch 447/500\n",
      "549/549 [==============================] - 0s 142us/step - loss: 90.2790 - mean_absolute_error: 90.2790 - val_loss: 116.3002 - val_mean_absolute_error: 116.3002\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 94.49476\n",
      "Epoch 448/500\n",
      "549/549 [==============================] - 0s 125us/step - loss: 89.2828 - mean_absolute_error: 89.2828 - val_loss: 116.8580 - val_mean_absolute_error: 116.8580\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 94.49476\n",
      "Epoch 449/500\n",
      "549/549 [==============================] - 0s 138us/step - loss: 89.8618 - mean_absolute_error: 89.8618 - val_loss: 120.8936 - val_mean_absolute_error: 120.8936\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 94.49476\n",
      "Epoch 450/500\n",
      "549/549 [==============================] - 0s 136us/step - loss: 91.5325 - mean_absolute_error: 91.5325 - val_loss: 108.9445 - val_mean_absolute_error: 108.9445\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 94.49476\n",
      "Epoch 451/500\n",
      "549/549 [==============================] - 0s 131us/step - loss: 89.9334 - mean_absolute_error: 89.9334 - val_loss: 121.3925 - val_mean_absolute_error: 121.3925\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 94.49476\n",
      "Epoch 452/500\n",
      "549/549 [==============================] - 0s 129us/step - loss: 91.9697 - mean_absolute_error: 91.9697 - val_loss: 113.5372 - val_mean_absolute_error: 113.5372\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 94.49476\n",
      "Epoch 453/500\n",
      "549/549 [==============================] - 0s 138us/step - loss: 91.9030 - mean_absolute_error: 91.9030 - val_loss: 116.6733 - val_mean_absolute_error: 116.6733\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 94.49476\n",
      "Epoch 454/500\n",
      "549/549 [==============================] - 0s 133us/step - loss: 94.3575 - mean_absolute_error: 94.3575 - val_loss: 126.5251 - val_mean_absolute_error: 126.5251\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 94.49476\n",
      "Epoch 455/500\n",
      "549/549 [==============================] - 0s 147us/step - loss: 91.9741 - mean_absolute_error: 91.9741 - val_loss: 112.2408 - val_mean_absolute_error: 112.2408\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 94.49476\n",
      "Epoch 456/500\n",
      "549/549 [==============================] - 0s 133us/step - loss: 88.0827 - mean_absolute_error: 88.0827 - val_loss: 117.0183 - val_mean_absolute_error: 117.0183\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 94.49476\n",
      "Epoch 457/500\n",
      "549/549 [==============================] - 0s 124us/step - loss: 88.3130 - mean_absolute_error: 88.3130 - val_loss: 118.2963 - val_mean_absolute_error: 118.2963\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 94.49476\n",
      "Epoch 458/500\n",
      "549/549 [==============================] - 0s 144us/step - loss: 87.5616 - mean_absolute_error: 87.5616 - val_loss: 107.7956 - val_mean_absolute_error: 107.7956\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 94.49476\n",
      "Epoch 459/500\n",
      "549/549 [==============================] - 0s 138us/step - loss: 89.0794 - mean_absolute_error: 89.0794 - val_loss: 124.8740 - val_mean_absolute_error: 124.8740\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 94.49476\n",
      "Epoch 460/500\n",
      "549/549 [==============================] - 0s 158us/step - loss: 88.2589 - mean_absolute_error: 88.2589 - val_loss: 120.8633 - val_mean_absolute_error: 120.8633\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 94.49476\n",
      "Epoch 461/500\n",
      "549/549 [==============================] - 0s 134us/step - loss: 89.0830 - mean_absolute_error: 89.0830 - val_loss: 115.2345 - val_mean_absolute_error: 115.2345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00461: val_loss did not improve from 94.49476\n",
      "Epoch 462/500\n",
      "549/549 [==============================] - 0s 142us/step - loss: 91.8048 - mean_absolute_error: 91.8048 - val_loss: 112.8999 - val_mean_absolute_error: 112.8999\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 94.49476\n",
      "Epoch 463/500\n",
      "549/549 [==============================] - ETA: 0s - loss: 91.6556 - mean_absolute_error: 91.6556  - 0s 141us/step - loss: 88.1014 - mean_absolute_error: 88.1014 - val_loss: 121.7247 - val_mean_absolute_error: 121.7247\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 94.49476\n",
      "Epoch 464/500\n",
      "549/549 [==============================] - 0s 138us/step - loss: 87.4521 - mean_absolute_error: 87.4521 - val_loss: 114.4309 - val_mean_absolute_error: 114.4309\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 94.49476\n",
      "Epoch 465/500\n",
      "549/549 [==============================] - 0s 143us/step - loss: 87.5584 - mean_absolute_error: 87.5584 - val_loss: 114.5339 - val_mean_absolute_error: 114.5339\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 94.49476\n",
      "Epoch 466/500\n",
      "549/549 [==============================] - 0s 146us/step - loss: 85.2846 - mean_absolute_error: 85.2846 - val_loss: 115.5394 - val_mean_absolute_error: 115.5394\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 94.49476\n",
      "Epoch 467/500\n",
      "549/549 [==============================] - 0s 138us/step - loss: 87.1236 - mean_absolute_error: 87.1236 - val_loss: 116.2795 - val_mean_absolute_error: 116.2795\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 94.49476\n",
      "Epoch 468/500\n",
      "549/549 [==============================] - 0s 133us/step - loss: 86.1941 - mean_absolute_error: 86.1941 - val_loss: 112.9860 - val_mean_absolute_error: 112.9860\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 94.49476\n",
      "Epoch 469/500\n",
      "549/549 [==============================] - 0s 154us/step - loss: 87.3214 - mean_absolute_error: 87.3214 - val_loss: 111.8880 - val_mean_absolute_error: 111.8880\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 94.49476\n",
      "Epoch 470/500\n",
      "549/549 [==============================] - 0s 147us/step - loss: 86.2186 - mean_absolute_error: 86.2186 - val_loss: 113.2425 - val_mean_absolute_error: 113.2425\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 94.49476\n",
      "Epoch 471/500\n",
      "549/549 [==============================] - 0s 145us/step - loss: 90.1700 - mean_absolute_error: 90.1700 - val_loss: 112.5294 - val_mean_absolute_error: 112.5294\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 94.49476\n",
      "Epoch 472/500\n",
      "549/549 [==============================] - 0s 151us/step - loss: 88.5323 - mean_absolute_error: 88.5323 - val_loss: 121.4949 - val_mean_absolute_error: 121.4949\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 94.49476\n",
      "Epoch 473/500\n",
      "549/549 [==============================] - 0s 147us/step - loss: 90.7433 - mean_absolute_error: 90.7433 - val_loss: 120.2244 - val_mean_absolute_error: 120.2244\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 94.49476\n",
      "Epoch 474/500\n",
      "549/549 [==============================] - 0s 127us/step - loss: 88.7019 - mean_absolute_error: 88.7019 - val_loss: 110.5701 - val_mean_absolute_error: 110.5701\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 94.49476\n",
      "Epoch 475/500\n",
      "549/549 [==============================] - 0s 131us/step - loss: 90.5131 - mean_absolute_error: 90.5131 - val_loss: 114.9784 - val_mean_absolute_error: 114.9784\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 94.49476\n",
      "Epoch 476/500\n",
      "549/549 [==============================] - 0s 122us/step - loss: 88.7811 - mean_absolute_error: 88.7811 - val_loss: 112.6921 - val_mean_absolute_error: 112.6921\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 94.49476\n",
      "Epoch 477/500\n",
      "549/549 [==============================] - 0s 133us/step - loss: 88.2607 - mean_absolute_error: 88.2607 - val_loss: 116.7362 - val_mean_absolute_error: 116.7362\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 94.49476\n",
      "Epoch 478/500\n",
      "549/549 [==============================] - 0s 133us/step - loss: 94.7677 - mean_absolute_error: 94.7677 - val_loss: 102.2316 - val_mean_absolute_error: 102.2316\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 94.49476\n",
      "Epoch 479/500\n",
      "549/549 [==============================] - 0s 134us/step - loss: 91.8954 - mean_absolute_error: 91.8954 - val_loss: 115.6067 - val_mean_absolute_error: 115.6067\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 94.49476\n",
      "Epoch 480/500\n",
      "549/549 [==============================] - 0s 137us/step - loss: 86.9147 - mean_absolute_error: 86.9147 - val_loss: 114.5137 - val_mean_absolute_error: 114.5137\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 94.49476\n",
      "Epoch 481/500\n",
      "549/549 [==============================] - 0s 133us/step - loss: 84.7798 - mean_absolute_error: 84.7798 - val_loss: 118.9857 - val_mean_absolute_error: 118.9857\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 94.49476\n",
      "Epoch 482/500\n",
      "549/549 [==============================] - 0s 140us/step - loss: 89.7253 - mean_absolute_error: 89.7253 - val_loss: 112.8845 - val_mean_absolute_error: 112.8845\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 94.49476\n",
      "Epoch 483/500\n",
      "549/549 [==============================] - 0s 144us/step - loss: 88.2229 - mean_absolute_error: 88.2229 - val_loss: 128.0189 - val_mean_absolute_error: 128.0189\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 94.49476\n",
      "Epoch 484/500\n",
      "549/549 [==============================] - 0s 139us/step - loss: 88.8093 - mean_absolute_error: 88.8093 - val_loss: 110.2666 - val_mean_absolute_error: 110.2666\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 94.49476\n",
      "Epoch 485/500\n",
      "549/549 [==============================] - 0s 131us/step - loss: 85.6088 - mean_absolute_error: 85.6088 - val_loss: 119.2252 - val_mean_absolute_error: 119.2252\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 94.49476\n",
      "Epoch 486/500\n",
      "549/549 [==============================] - 0s 142us/step - loss: 84.8149 - mean_absolute_error: 84.8149 - val_loss: 109.1721 - val_mean_absolute_error: 109.1721\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 94.49476\n",
      "Epoch 487/500\n",
      "549/549 [==============================] - 0s 133us/step - loss: 87.2282 - mean_absolute_error: 87.2282 - val_loss: 117.9551 - val_mean_absolute_error: 117.9551\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 94.49476\n",
      "Epoch 488/500\n",
      "549/549 [==============================] - 0s 133us/step - loss: 86.9065 - mean_absolute_error: 86.9065 - val_loss: 119.6475 - val_mean_absolute_error: 119.6475\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 94.49476\n",
      "Epoch 489/500\n",
      "549/549 [==============================] - 0s 133us/step - loss: 85.6241 - mean_absolute_error: 85.6241 - val_loss: 118.7944 - val_mean_absolute_error: 118.7944\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 94.49476\n",
      "Epoch 490/500\n",
      "549/549 [==============================] - 0s 140us/step - loss: 87.2868 - mean_absolute_error: 87.2868 - val_loss: 112.5025 - val_mean_absolute_error: 112.5025\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 94.49476\n",
      "Epoch 491/500\n",
      "549/549 [==============================] - 0s 136us/step - loss: 84.2502 - mean_absolute_error: 84.2502 - val_loss: 120.2787 - val_mean_absolute_error: 120.2787\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 94.49476\n",
      "Epoch 492/500\n",
      "549/549 [==============================] - 0s 144us/step - loss: 87.0443 - mean_absolute_error: 87.0443 - val_loss: 123.2517 - val_mean_absolute_error: 123.2517\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 94.49476\n",
      "Epoch 493/500\n",
      "549/549 [==============================] - 0s 160us/step - loss: 84.8021 - mean_absolute_error: 84.8021 - val_loss: 119.0646 - val_mean_absolute_error: 119.0646\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 94.49476\n",
      "Epoch 494/500\n",
      "549/549 [==============================] - 0s 133us/step - loss: 83.9036 - mean_absolute_error: 83.9036 - val_loss: 115.8502 - val_mean_absolute_error: 115.8502\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 94.49476\n",
      "Epoch 495/500\n",
      "549/549 [==============================] - 0s 149us/step - loss: 85.6170 - mean_absolute_error: 85.6170 - val_loss: 123.5326 - val_mean_absolute_error: 123.5326\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 94.49476\n",
      "Epoch 496/500\n",
      "549/549 [==============================] - 0s 131us/step - loss: 84.7505 - mean_absolute_error: 84.7505 - val_loss: 118.5475 - val_mean_absolute_error: 118.5475\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 94.49476\n",
      "Epoch 497/500\n",
      "549/549 [==============================] - 0s 183us/step - loss: 84.7427 - mean_absolute_error: 84.7427 - val_loss: 114.8542 - val_mean_absolute_error: 114.8542\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 94.49476\n",
      "Epoch 498/500\n",
      "549/549 [==============================] - 0s 182us/step - loss: 82.8148 - mean_absolute_error: 82.8148 - val_loss: 120.2278 - val_mean_absolute_error: 120.2278\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 94.49476\n",
      "Epoch 499/500\n",
      "549/549 [==============================] - 0s 151us/step - loss: 83.5108 - mean_absolute_error: 83.5108 - val_loss: 125.7256 - val_mean_absolute_error: 125.7256\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 94.49476\n",
      "Epoch 500/500\n",
      "549/549 [==============================] - 0s 200us/step - loss: 89.7529 - mean_absolute_error: 89.7529 - val_loss: 126.6001 - val_mean_absolute_error: 126.6001\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 94.49476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27900922710>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예제에서는 target을 대회에서 주는 target데이터를 기준으로 미리 train데이터와 맞춰졌있지만\n",
    "# gs/lv데이터는 아니다. 그래서 위에서 나누는 기준으로 삼은cut_line=732을 이용하여 데이터 사이즈를 맞춰준다.\n",
    "# 아니면 애시당초에(맨처음에) 훈련용 데이터와 타겟을 만들어 놓는것도 좋다.\n",
    "NN_model.fit(train, target[:cut_line], epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0719 12:30:54.557808  3200 deprecation_wrapper.py:119] From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0719 12:30:54.558804  3200 deprecation_wrapper.py:119] From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 500번을 학습하여 나온 결과들중, 가장 좋은(마지막에 저장된) Weights파일을 가져온다.\n",
    "# Load wights file of the best model :\n",
    "# 파일은 이 코드랑 같은 폴더에 위치해있어야 작동\n",
    "wights_file = 'GS-마스크date-Weights-033--112.25468-cat02-vf05.hdf5' # choose the best checkpoint \n",
    "NN_model.load_weights(wights_file) # load it\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "여기서 점수란 R-square값을 의미한다.\n",
      "Random forest을 이용한 마스크의 회귀분석 결과 :\n",
      "훈련세트점수 : 0.942\n",
      "검증세트점수 : 0.260\n",
      "XGBoost을 이용한 마스크의 회귀분석 결과 :\n",
      "훈련세트점수 : 0.925\n",
      "검증세트점수 : 0.278\n",
      "LinearRegression을 이용한 마스크의 회귀분석 결과 :\n",
      "훈련세트점수 : 0.407\n",
      "검증세트점수 : 0.196\n",
      "RidgeRegression을 이용한 마스크의 회귀분석 결과 :\n",
      "훈련세트점수 : 0.403\n",
      "검증세트점수 : 0.190\n",
      "LassoRegression을 이용한 마스크의 회귀분석 결과 :\n",
      "훈련세트점수 : 0.407\n",
      "검증세트점수 : 0.196\n",
      "OLS을 이용한 마스크의 회귀분석 결과 :\n",
      "훈련세트점수 : 0.406\n"
     ]
    }
   ],
   "source": [
    "#2016~2018전체를 난수로 0.67:0.33 = 2:1\n",
    "# train3 = combined.loc[:,'temp':]\n",
    "# target3 = combined.loc[:,'qty']\n",
    "# train_X, val_X, train_y, val_y = train_test_split(train3, target3, test_size = 0.33, random_state = 14)\n",
    "\n",
    "# 2016~2017 : 훈련 / 2018 검증 2:1\n",
    "# 1~732 / 732~1096\n",
    "trainXy = Xy[:cut_line]\n",
    "testXy = Xy[cut_line:]\n",
    "\n",
    "# 독립변수들\n",
    "train_X = trainXy.loc[:,'temp':]\n",
    "# 정답(판매량)\n",
    "train_y = trainXy.loc[:,'qty']\n",
    "\n",
    "val_X = testXy.loc[:,'temp':]\n",
    "val_y = testXy.loc[:,'qty']\n",
    "\n",
    "\n",
    "\n",
    "print('여기서 점수란 R-square값을 의미한다.')\n",
    "# RandomForest 회귀분석\n",
    "RFmodel = RandomForestRegressor()\n",
    "RFmodel.fit(train_X,train_y)\n",
    "# Get the mean absolute error on the validation data\n",
    "RFpredicted = RFmodel.predict(val_X)\n",
    "MAE = mean_absolute_error(val_y , RFpredicted)\n",
    "print('Random forest을 이용한 %s의 회귀분석 결과 :'%item)\n",
    "# print('Random forest validation MAE = ', MAE)\n",
    "print('훈련세트점수 : {:.3f}'.format(RFmodel.score(train_X, train_y)))\n",
    "print('검증세트점수 : {:.3f}'.format(RFmodel.score(val_X, val_y)))\n",
    "\n",
    "# XGBRegressor 회귀분석\n",
    "XGBModel = XGBRegressor(objective='reg:squarederror')\n",
    "XGBModel.fit(train_X,train_y , verbose=False)\n",
    "# Get the mean absolute error on the validation data :\n",
    "XGBpredictions = XGBModel.predict(val_X)\n",
    "MAE = mean_absolute_error(val_y , XGBpredictions)\n",
    "print('XGBoost을 이용한 %s의 회귀분석 결과 :'%item)\n",
    "# print('XGBoost validation MAE = ',MAE)\n",
    "print('훈련세트점수 : {:.3f}'.format(XGBModel.score(train_X, train_y)))\n",
    "print('검증세트점수 : {:.3f}'.format(XGBModel.score(val_X, val_y)))\n",
    "\n",
    "linReg = LinearRegression().fit(train_X, train_y)\n",
    "print('LinearRegression을 이용한 %s의 회귀분석 결과 :'%item)\n",
    "print('훈련세트점수 : {:.3f}'.format(linReg.score(train_X, train_y)))\n",
    "print('검증세트점수 : {:.3f}'.format(linReg.score(val_X, val_y)))\n",
    "\n",
    "ridge = Ridge(alpha=0.1, normalize=True, random_state=0, tol=0.001).fit(train_X, train_y)\n",
    "print('RidgeRegression을 이용한 %s의 회귀분석 결과 :'%item)\n",
    "print('훈련세트점수 : {:.3f}'.format(ridge.score(train_X, train_y)))\n",
    "print('검증세트점수 : {:.3f}'.format(ridge.score(val_X, val_y)))\n",
    "\n",
    "lasso = Lasso(alpha=0.1, max_iter=1000).fit(train_X, train_y)\n",
    "print('LassoRegression을 이용한 %s의 회귀분석 결과 :'%item)\n",
    "print('훈련세트점수 : {:.3f}'.format(lasso.score(train_X, train_y)) )\n",
    "print('검증세트점수 : {:.3f}'.format(lasso.score(val_X, val_y)) )\n",
    "\n",
    "customF = formulaGen(target='qty',ind_features=list_col)\n",
    "olsModel = sm.OLS.from_formula(customF, data=trainXy).fit()\n",
    "print('OLS을 이용한 %s의 회귀분석 결과 :'%item)\n",
    "print('훈련세트점수 : {:.3f}'.format(olsModel.rsquared) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "마스크 ols model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>qty</td>       <th>  R-squared:         </th> <td>   0.391</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.387</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   92.88</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 19 Jul 2019</td> <th>  Prob (F-statistic):</th> <td>1.57e-75</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:31:00</td>     <th>  Log-Likelihood:    </th> <td> -5525.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   730</td>      <th>  AIC:               </th> <td>1.106e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   724</td>      <th>  BIC:               </th> <td>1.109e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>     <td> -318.3027</td> <td>   68.076</td> <td>   -4.676</td> <td> 0.000</td> <td> -451.953</td> <td> -184.652</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>temp</th>          <td>   -4.5688</td> <td>    1.695</td> <td>   -2.696</td> <td> 0.007</td> <td>   -7.896</td> <td>   -1.242</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lgt_time</th>      <td>    4.8530</td> <td>    5.278</td> <td>    0.920</td> <td> 0.358</td> <td>   -5.509</td> <td>   15.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PM10</th>          <td>   16.0965</td> <td>    0.792</td> <td>   20.331</td> <td> 0.000</td> <td>   14.542</td> <td>   17.651</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>snow_or_not_o</th> <td>   51.8844</td> <td>  110.556</td> <td>    0.469</td> <td> 0.639</td> <td> -165.163</td> <td>  268.932</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rain_or_not_o</th> <td>   33.3373</td> <td>   48.480</td> <td>    0.688</td> <td> 0.492</td> <td>  -61.841</td> <td>  128.516</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1175.887</td> <th>  Durbin-Watson:     </th>  <td>   1.051</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>812025.918</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 9.400</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>165.306</td> <th>  Cond. No.          </th>  <td>    351.</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    qty   R-squared:                       0.391\n",
       "Model:                            OLS   Adj. R-squared:                  0.387\n",
       "Method:                 Least Squares   F-statistic:                     92.88\n",
       "Date:                Fri, 19 Jul 2019   Prob (F-statistic):           1.57e-75\n",
       "Time:                        12:31:00   Log-Likelihood:                -5525.5\n",
       "No. Observations:                 730   AIC:                         1.106e+04\n",
       "Df Residuals:                     724   BIC:                         1.109e+04\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=================================================================================\n",
       "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------\n",
       "Intercept      -318.3027     68.076     -4.676      0.000    -451.953    -184.652\n",
       "temp             -4.5688      1.695     -2.696      0.007      -7.896      -1.242\n",
       "lgt_time          4.8530      5.278      0.920      0.358      -5.509      15.215\n",
       "PM10             16.0965      0.792     20.331      0.000      14.542      17.651\n",
       "snow_or_not_o    51.8844    110.556      0.469      0.639    -165.163     268.932\n",
       "rain_or_not_o    33.3373     48.480      0.688      0.492     -61.841     128.516\n",
       "==============================================================================\n",
       "Omnibus:                     1175.887   Durbin-Watson:                   1.051\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           812025.918\n",
       "Skew:                           9.400   Prob(JB):                         0.00\n",
       "Kurtosis:                     165.306   Cond. No.                         351.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(item, 'ols model')\n",
    "# ind_features = 넣고 싶은 독립변수를 리스트로 넣어주면 알아서 formula생성해 준다.\n",
    "# customF = formulaGen(target='qty',ind_features=['temp'])\n",
    "# print(customF) qty ~ temp\n",
    "# customF = formulaGen(target='qty',ind_features=['temp','cloud'])\n",
    "# print(customF) # qty ~ temp + cloud\n",
    "list_col = ['temp', 'lgt_time', 'PM10', 'snow_or_not_o', 'rain_or_not_o']\n",
    "\n",
    "customF = formulaGen(target='qty',ind_features=list_col)\n",
    "olsModel = sm.OLS.from_formula(customF, data=trainXy).fit()\n",
    "olsModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = ['temp', 'cloud', 'wind', 'lgt_time', 'PM10', 'rain_or_not_o', 'snow_or_not_o']\n",
    "combined = Xy.loc[:,'temp':]\n",
    "target = Xy.loc[:,'qty']\n",
    "\n",
    "# 가장 좋다고 판명된 설정을 이용하여, 예측qty생산\n",
    "predictions = NN_model.predict(combined)\n",
    "# RandomForest 회귀분석 예측 qty생산\n",
    "RFpredicted = RFmodel.predict(combined)\n",
    "# XGBRegressor 회귀분석 예측 qty생산\n",
    "XGBpredictions = XGBModel.predict(combined)\n",
    "# linearRegression 회귀분석 예측 qty생산\n",
    "linPred = linReg.predict(combined)\n",
    "# Ridge 회귀분석 예측 qty생산\n",
    "ridPred = ridge.predict(combined)\n",
    "# Lasso 회귀분석 예측 qty생산\n",
    "lassoPred = lasso.predict(combined)\n",
    "# OLS 회귀분석 예측 qty생산\n",
    "olsPred = olsModel.predict(combined)\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "result_df['date'] = gs_day_w['date']\n",
    "result_df['qty'] = Xy.loc[:,'qty']\n",
    "\n",
    "# print(\"keras 신경망 predictions\",predictions.shape)\n",
    "result_df['keras_qty'] = predictions\n",
    "\n",
    "# print(\"randomforest 예상\",RFpredicted.shape)\n",
    "result_df['rf_qty'] = RFpredicted\n",
    "\n",
    "# print(\"XGBpredictions\",XGBpredictions.shape)\n",
    "result_df['xgb_qty'] = XGBpredictions\n",
    "\n",
    "# print(\"linearRegression 예상\",RFpredicted.shape)\n",
    "result_df['lin_qty'] = linPred\n",
    "\n",
    "# print(\"Ridge 예상\",RFpredicted.shape)\n",
    "result_df['ridge_qty'] = ridPred\n",
    "\n",
    "# print(\"Lasso 예상\",RFpredicted.shape)\n",
    "result_df['lasso_qty'] = lassoPred\n",
    "\n",
    "# print(\"OLS 예상\",RFpredicted.shape)\n",
    "result_df['ols_qty'] = olsPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['qty', 'temp', 'cloud', 'wind', 'lgt_time', 'PM10', 'rain_or_not_o',\n",
      "       'snow_or_not_o'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(Xy.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA54AAAG6CAYAAABgLZnuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nOzde1xVVf7/8TcHFES8JFCmJop4He0r5ljiRDdHwfJO2cUhMys1nTHDNG1MawZNx19qNWNNlpF2Q8umEDK/pphoGV5KTUolCfEGeAHxHIR9fn/49UxHEA6XA1t5PR8PHrXXWXvvz+ZT1uestdfysNvtdgEAAAAA4CaW2g4AAAAAAHB1o/AEAAAAALgVhScAAAAAwK0oPAEAAAAAbkXhCQAAAABwKwpPAECNysrKUlZWVm2HAZMrKChQWlpahc9buXKlrFarGyICAFQFhScAoFr9+uuv2r17t9PP4cOHHZ+/8cYbeuONN1y61qJFi1RUVOSuUFEFv/76q3755ZdSf06fPi1Jevzxx/Xee++VODcvL09Hjx51/Jw5c8bx2XXXXSdJ+vbbb/X00087nffBBx8oJCTE6adLly5at26do09MTIyOHj3qjkcGAFSBV20HAAC4euzZs0dhYWG64447HG12u11r165VZmam/P39Xb7Wr7/+qilTpmjixInl9n311Ve1cOHCy35+5swZrVq1SrfeeqvL90fZxo0bp+PHj+vgwYPq2bOnJGn79u1q3bq1Ro4cqUmTJqmwsFCFhYVO5/3000/q2bOnbrzxRkfbzp079e2336pLly46e/bsZe95//336/7773dqmzFjhjZv3qy+fftW49MBAKobI54AgGpz8uRJhYaGavXq1Y6fTz/9VC1btlReXl6FrvX666/LYrHozTffLLfvhAkTtH///sv+PPDAA9q3b19lH8tlKSkpatCggVq1aqWgoCD9z//8j9555x1JUmZmpiwWi6KioiRJ+/btk8Vi0fjx4x3nFxUVafz48erSpUuJa3/55Zfq0qWLWrVqpZiYGBmG4fbnKcvnn3+uefPmqXv37kpKSlJSUpJ69eqlv/3tb5o0adJlzzt+/Lh69Oihr7/+2vHTs2dPHT9+vFJxeHmV/A7997//vZo3b64vv/yyUtcEAFQ/RjwBAKazd+9evfnmm0pJSdHw4cN16623qnPnzpW+nmEYql+/fjVGWLrCwkL16dPHMfXzp59+0h//+Ec1atRIPXr0UEBAgH7++WdJFwrP6667zjEieObMGd13331ObRedOHFC48eP15dffqlWrVopOjpaS5YscSpa64LExERNnDjRafr12bNntWTJEqd+27ZtU5s2bWo4OgBAWRjxBACYyk8//aRBgwZpyZIl6tGjh5YuXaq7775bqampLl+juLhY586d06lTp3T8+HEdPXpUjRs3dmPUpevQoYMmTJigDz/8UJLUuHFj2Ww2SdL+/fudiulDhw7p3nvv1ezZs0tcZ/Xq1br33nvVpk0beXl5aebMmaW+O3mpAwcOqEOHDk5t//znP/Xkk09KulCQT548WUFBQbrhhht044036siRI5V+3tI8++yz6tSpk+bNm1fla3377bd64IEHnN4nPXHihIYPH+7Ub+3atVq9erWOHTtW5XsCAKoHI54AgBq3ZMkSrV69WnfddZcWLFgg6UKxuGzZMs2ePVtz5szRkCFDJEl33nmn3n33XT344IMaNmyYnnzySbVq1crpep9//rliYmLk4eEh6cJ7pV5eXqpXr54aNGigtLQ0l94VdQcvLy+dP3/ecdyyZUsdOXJEP//8szp27Oj4rFu3burWrZt++eWXEtdISUnRwIED9fnnn2v79u2aOXOmfvzxRxmGIYvl8t8ht2vXTtKF0dVOnTpJkj799FNNnjxZkvT+++/rxx9/VFpamnx8fJSbm1stBXphYaGKi4slSXPmzNGoUaNcPvfcuXPq3r278vPzHTH/1sUcSxf+mTl+/Lh++eUXpaena8CAAZKkHTt2qEmTJurYsaNjsSIAQO2i8AQA1LjRo0dr+vTpTtNfR40apdzcXCUkJKhbt25O/fv06aPU1FS99tpreuaZZ0qM9t1zzz265557Lnu/m2++WYGBgS7FVlhYqHbt2slutzu1169fXwcPHnTpGtKF4nfTpk1avHixli5d6mjv2LGj9u/frwMHDujuu+/Wnj17yr3WsWPH5O/vr/T0dGVkZEiSGjVqpJycnHKfa+DAgUpMTFSnTp109uxZ7dy507H4k2EYTs/ZrFkzl57t9OnTGj9+vI4dO6YffvhBw4cPV35+vr777jsdOHBAAQEBmjFjhkvXulSDBg309ddfa9OmTXrttdecPgsNDdWECRMUHx+v4uJiWSwWNW3aVK1atVKHDh105513SpKmTp3KVFsAMBkKTwBAjatfv778/Pyc2pYtWyZPT8/LnuPn56epU6dW6n5Hjx7V9ddf73Jsv/76a6XuI0mbN292TIlt3769li1bpttuu80xktmhQwcdOHBAp0+f1jXXXOPSNS++81mvXj3Vq1fP0f7bkdTLueeeezR37lw99dRT+vLLL9W3b19Hwf/ggw/qp59+Urdu3fTnP/9ZTzzxhEvvwjZq1EjTpk1TcXGxPD091aBBAzVq1EjXXHON0/kffPCBS893KT8/PzVo0KBE+6BBgzRo0KAyz+3QoUOp5wIAaheFJwCg2vj6+uqHH35wmlppt9uVlZVVotC8VFlFZ1UUFxcrPz9fTZs2dcv1L/XbxYVK07FjR61fv16+vr4uX9Pf3185OTl66KGH9NBDD0mScnNzXSpcb731Vj344IMqKCjQ559/7pjCLF34nb/44ot68sknNWvWLHXr1k1ffPFFuaOFFovFMSpdUFAgb29vl/Pn5+enXbt2OW2LsnPnzgrlx2az6e2339aaNWuUmZmp4uJitWzZUv3799fHH39c7j9rAICaR+EJAKg2PXr00Nq1a0tsnfLMM88oICCgVmJKT08v8U5oberQoYNmzJih0NBQl8+58cYblZqaqmHDhkmS0tLS1Lx5c5dG9ry8vHT77bfrq6++0vr160vd77R58+ZasmSJZs+erTfeeEOxsbEuxzZq1Cg98MADGjp0aInPbrrpJrVt29aprXv37tq6davTPyOzZ89Wx44dXb7nwIED1aZNG8XGxqpt27ayWCxKT0/XsmXLdOutt2rz5s0VKuwBAO5H4QkAqFY33XRThc+Jjo5WSkqKy/1btWqlDRs26NNPP9XTTz/t0jkhISGSpBUrVujmm2+ucIzVpU2bNtq3b59GjBjh8jn333+/brvtNj3++ONq2bKlXnjhBcfIpysGDhyo2NhY/e53v3MaDczMzFTDhg11zTXXKDc3V19//bUGDhxYoecxDMOxkNClLregU0WKzEsdO3ZMX3/9tRITE51GWbt06aJ58+bp5ptv1jfffON4jxUAYA4UngCAWhcXF1ep8wYPHqzBgwdXczSVV79+/cu+I1mvXj3HlNR//OMfuvPOO7V9+3Z5e3uX2u+32rVr5zinoKBA/fv317Rp01yOKzIyUo899phef/11p/b//d//1eTJk+Xp6Slvb29FRUVVeG9QDw8Pp3013S0wMFAdOnTQggULNH78eEchffbsWb3//vvKzs5W9+7dayweAIBrPOyXLtsHAIAb/f3vf5ekSq96CnP517/+pYULF1521LNevXras2dPmdu+/Nbrr7+uJ554Qps3b9Y//vEPffLJJyX6nDx5UnPmzFFSUpLOnTsnu90ub29v9e3bV9OmTXN5ISkAQM2h8AQA1KiL/9n57X6MQGnK26cUAHDloPAEAAAAALgVXyMCAAAAANyKwhMAAAAA4FZX/aq2WVlZLvcNCAhQdna2G6NBRZETcyIv5kRezIecmBN5MR9yYk7kxZzMnpcWLVqU2s6IJwAAAADArWpkxPPIkSOaN2+ennrqKfn6+urf//63bDabrr32Wo0bN04eHh5avny59u7dq+DgYI0ZM0aSXG4DAAAAAJiX20c8DcNQQkKCQkNDZRiG/Pz89NRTT2nWrFlq1qyZ0tLSlJGRIcMwFBsbq6ZNm2rfvn0utwEAAAAAzM3tI54Wi0VjxozRRx99JEny8fFxfObj4yNfX1/t27dPoaGhWrx4sQYMGKDdu3fL19fXpbZOnTpVKB673S6r1SrDMErsIXfs2DHZbLaqPzSqTVVzYrfbZbFY5OPjw56BAAAAQC2ptcWFzp49q+zsbLVu3VrfffedfH19ZRiGGjZsqPz8fBmG4VLbpdatW6d169ZJkubOnauAgACnz3NycuTj46N69eqVGpe3t3f1PyyqpKo5OX/+vCwWi/z9/aspInh5eZX4dwu1j7yYDzkxJ/JiPuTEnMiLOV2peamVwrOoqEjvv/++RowYIUny9fVVQUGBJk2apP3798vX19fltkv17dtXffv2dRxfuuLT2bNn1bBhQxUVFZU418vLq9R21J7qyImHh4fy8/Nlt9urKSqYfTW1uoq8mA85MSfyYj7kxJzIizmZPS+mWdW2qKhIb7/9tu655x41atRIkhQSEqIdO3ZIknbu3KmQkBCX2yqK6ZZ1E3kHAAAAak+NFZ4Wi0UWi0Uff/yxfvjhBy1ZskSzZs3S1q1bFRISoqKiIs2cOVMnTpxQ165dXW6De5w9e7a2QwAAAABwlfCwX+XzD7OyspyOCwoKSp2iK9X8VNslS5Zo7NixjuO1a9fKz89PYWFhpfYvLCzUwYMHndo8PDwUEhKiRYsWafLkyY72jIwMnTx5UpLUtGlTBQUFac6cOXr22Wc1ceJEvfLKK44YNm3aJOnClwMTJkzQzTffrEcffVRLly6t1uetjOrKSVl5R8WZfYpHXUVezIecmBN5MR9yYk7kxZzMnhfTTLW90lzcDmbq1KlKSEiQYRjVdu3k5GSn45MnT+rMmTOX7X/u3Dnt2rVLf/nLX7Rr1y7NmzdPSUlJKiws1O7du536jh8/Xrt27dKuXbv05JNPSrpQjEpyKuTGjh2rFStWaMWKFXriiSf0ww8/SLqwGux7772n77//vlqeFQAAAEDdVWur2l4JDMNQdHS0UlJSZLPZFB8fr7CwMMXFxcliqVrN/sUXXyg9PV2pqam66aabXDqnSZMmGjFihL744guNGDFCmZmZ6t+/vxo0aCDpQkHp6ekpDw8PXXvttYqOjpYkbdiw4bLXXLZsmWMU9fTp0xoyZIjjs169eqlZs2aVfEIAAAAAuIDCswyJiYmOolOSbDabtmzZoqSkJA0YMKDS1/3uu++0cuVKrV27Vk8//bSGDRumiIiIKscbGxur8PBw3X777aV+np+fr9TUVKe25ORkvfXWW6X2f/755zVkyBDde++9VY4NAAAAQN1F4VmG5ORkR9F5kdVqVXJycqULz/Pnz+vLL7/Uq6++Km9vb7322mv68MMPZRiGPD09yxxJPXTokN555x2lpaXpxRdfVFpamo4dO6bevXtLkmbOnFnmvc+dO6f9+/c7tfn7+2vy5MkqLi5WcXGxPDw81KtXL0nSihUrKvWMAAAAAPBbFJ5lCA8PV3x8vFPx6ePjo/Dw8Epfs169enr22Wf1008/aePGjY72pUuXytPTU3fddddlzw0KCtLMmTNLLTA/++wzp+OioiJZrVbH30tSYGCgRowY4TT19qWXXpLVapWPj49T0Zuenl6p5wMAAACAS1F4liEyMlJhYWGO6bY+Pj7q3bt3tUyLbdmypfr27evUlpCQoG3btqlfv35lnvvKK69oxIgRuvbaax1t9erVc+rTu3dvxcbGSlKZhbLFYpHVatVzzz2nnJwcGYahevXq6ZFHHqnoIwEAAABAqSg8y2CxWBQXF6ekpCQlJycrPDxcERERVV5YSJL27t2rl156yWmLj4KCgsu+n/lbJ06cKDEF+PXXX3c6HjdunMuxvPTSSxo9erRjX1SbzaaRI0eqZ8+ejoWLAAAAAFSMYRhKTEx01BKRkZHVUktciSg8y2GxWDRgwIAqLSZUmvT0dD366KOKjIys8LnXX3+9/vKXv5QoCqOjo9W/f//LnnfDDTeU2h4YGKjU1FQFBQXJ29tbe/fuld1uV/369SscGwAAAIALRee4Jybpd+2Hq32rB3Rgz2mNXT1JS15fWCeLTw+73W6v7SDcKSsry+m4oKDAaZTxt7y8vJz2uHSnvXv3as6cOSX2Ba2JVWQzMzPVqlUrx3FxcbGWL1+urVu3ymazqX379ho1apSuv/56t8bhiurKSVl5R8WZfePiuoq8mA85MSfyYj7kxJzIS9UkJCTowB4/tQ36H0fbwUM71b5rQZUGtcyelxYtWpTazohnLenSpYvefffdWrn3b4tOSfL09NTDDz+shx9+uFbiAQAAAK42ycnJat/qAae2hg2aKjm5alszXqnq3hgvAAAAALhZeHi4Cs6ddmorOHe6SjtkXMkoPAEAAACgmkVGRmr3z6t08NBOHTv+i9IP7dLun1dVyw4ZVyIKTwAAAACoZhaLRUteX6j2XQu0//AHCul6ts4uLCTxjieq6OzZs7JYLOVuu5Kbm6tmzZrVUFQAAABA7XPXDhlXorpZbteyI0eOaNeuXdq1a5d+/fVXSdKcOXMkSRMnTnT027dvn1JSUpSSkqKtW7fq9OkLc8Qff/zxy177559/dlyzIp555hmn4x9++EHffvut4/jTTz/VQw89pIceekgjR47Uhx9+KEn6/PPPtXHjRke/VatWaeTIkY5+8fHxkqSYmBiX4ti0aZM2b97sOP773/8uyfn3ctG6desUExOjmJgYbdu2zcUnBQAAAFDTGPGsBVlZWdqzZ48kacWKFfrss8+UkZEhSU5bh5w5c0YnTpyQJB06dEhbt27VpEmTytxe5M0331SvXr0uu2fn5Rw8eNDp+PDhwzp9+rR69eolSRo8eLAGDx4sSfruu+80f/58SdK2bdvUt29fx3lff/21Fi9erGbNmik3N1cvvviiy9vDzJ07V9ddd53Onj2rTZs2adq0aTp06JAklXjm6dOnOz6TpMWLF+uBBx7g2yQAAADAhCg8y2GcOCL7m/9Pys+T/BrJY8zTsgQ2r9I1b7rpJt10002SpJSUFNWvX7/UfheLPknavXu3vvjiizKvm56eruzsbCUkJOiOO+5weWrrjz/+qN27d+vnn39W+/bty+xrt9v15ptvavLkyWrSpImOHj1a4nNPT09JF7Zp+e02sf/+97/VvXt3/f73vy9xXcMwlJaWpmnTpkmShg8frilTppQoiC+KjY2VJGVnZ2vVqlU6cOCAevbs6dLzAgAAAKhZFJ7lsL/5/6SDaRcOjv/f8bPzqnTNt99+WwcOHNCxY8f0ww8/aOrUqaUWWGvXrlVqaqrsdruOHTum7t27Oz5LTU1VixYtdP3110u6MGI5Y8YMLVy4UFarVZMnT9bzzz+vtm3blv18drv+8Y9/6JNPPtGcOXO0cOFCNW7cuNS+hYWFeu6553Tbbbfp1VdflWEYOnr0qKZMmeLSc/fu3VvXXXddqZ/l5OQoMDDQcRwcHKynn35aL7zwQom+Bw4c0MqVK3X06FH5+/uruLhYHh4eWrJkie6991517tzZpXgAAAAA1AwKz/Lk511yfKbKl3zkkUdkt9s1fvx4rVq1Si1bttS4ceNK9Hvvvfe0aNEiSRdeTG7UqJHjs/3798vb21vXX3+9li5dqh07dujVV1+Vv7+/JOnll1/WggULFBwcrNGjR5caR2FhoaZNm6b77rtPnTt31owZM/Tkk0/q9ddfL9E3KSlJy5cv19ixY/WHP/xBDzxwYTPci+96XhQQEKBHH31U3t7estls6tGjh+Ozrl27XvZ34ufn53iHVbqwGNHJkydltVqd+uXk5OjQoUMKDQ11POtFBQUFys7OZiEjAAAAwGQoPMvj10g6fslxFVmtVj333HMaOnSoWrZsedl+FotFTZo0KXGuJI0YMcLRNnLkSD366KNO/a655hr97W9/KzMOT09PPfbYY44Rwnbt2mnZsmWOqbK/1aFDB73zzjslPvPx8XGaKvzcc8+Veq9u3bqVGUuDBg1ktVqVk5Mjm82m3Nxcbdy4Ubm5uU798vPzlZd34cuAzMzMUq+Vl5dH4QmgTjAMQ4mJiUpOTlZ4eLgiIyPr7DL9AABzo/Ash8eYp//vHc8zjnc8q2rhwoUaOnSo+vTpU2a/Fi1aaPLkyZIuFKE2m83xbuhveXt7VyoOT09Pde7cWd9++6327t2rUaNGOQpLi8XiVGQGBwfLbrdr0aJF2rp1q7y8vHT+/HmFh4dr/PjxTtfNy8vT888/r2PHjsliscgwDI0ZM6bceGbNmqUXXnhBhmFo4cKFCgoK0vfff+/UJygoSMXFxfrrX/9a4vzmzZtrwYIFlflVAMAVxzAMRUdHKyUlRTabTfHx8QoLC1NcXBzFJwDAdCg8y2EJbF7ldzovdXEBnfJcbsQyOTnZ8fcbNmzQv//97zKv8/jjj+u222677OdFRUU6f/68U1u/fv1Kva/VatX777/vaJszZ442btzodP1XX31VQ4YMUXh4uCTJZrPpT3/6k2655ZYy9/ts27atY2pxWYKDg7VixYoS7aNGjSr3XAC4WiQmJjqKTunCn7VbtmxRUlISK3wDAEyHwtMkKrr9yUW33367br/99irdOyAgQO+99542bNjg1N67d29NmDDBcXzttddq//79Onz4sK677jplZWXpwIEDGjp0qNN5LVq0UGpqqm688Ub5+vpqz549stvtlRqZrcjvhW/4AdQlycnJjqLzIqvVquTkZApPAIDpeNh/u9/FVSgrK8vpuKCgQL6+vqX29fLyKnOPzJqQmZmpVq1aldknIyNDrVu3rqGInG3btk0rV67U8ePH1bx5c917771OCwhd9PHHH+urr75SQUGB2rdvr1GjRql584pvQ3MxJ678XrZv315qLFLZeUfFBQQEKDs7u7bDwCXIi/m4MycJCQmaOHGiU/Hp4+OjV155hcKzHPy7Yj7kxJzIizmZPS8tWrQotZ3C8zfMUHjCWXXlhMKzepn9D7y6iryYjztzcuk7nj4+PurduzfveLqAf1fMh5yYE3kxJ7Pn5XKFJ1NtAQC4AlksFsXFxSkpKcmxqm1ERARFJwDAlCg8AQC4QlksFg0YMICptQAA0+NrUQAAAACAW1F4otZs2rRJmzdvdhzPmTNHkjRx4sQSfdetW6eYmBjFxMRo27ZtNRYjAAAAgKqj8KxhL7/8siIjIzV8+HDNnDlTlV3baevWrXrrrbdc7v/NN9+oT58+ioqKUlRUlDIyMip1X1eMHTu23D5z587V/v37tWPHDs2dO1eSHDFdupjQ9OnT9fbbb+vIkSM6cuSIFi9erDVr1lR/4AAAAADcgnc8a5hhGJo/f766du2qefPmadu2berVq1elrlNcXOxy/+LiYo0aNUqPPfZYhe9VUYWFhWV+bhiG0tLSNG3aNEnS8OHDNWXKFB08eLDU/rGxsZKk7OxsrVq1SgcOHFDPnj2rN2gAAAAAbkPhWY4jeYV6OSVLebZiNfL21OQ+LdTcr36Vr3txb8pGjRqpqKhIkyZN0vHjxxUYGKg5c+YoMTFRqampyszMlIeHh5YuXari4mJNnDhROTk56ty5s9q1a6f8/Hw99dRTOnnypFq0aKH58+drxowZys3NVefOnbV27Vq98MILpcZQ2rmrV6/WokWL9PDDD2vNmjVq2bKlFi1apGnTpikjI0OBgYF6+eWXVVxcrMcee8yxhP/y5cu1du1avfHGG9q3b5+ioqL0+OOPq1+/fiXum5OTo8DAQMdxcHCwnn76ac2ePbtE3wMHDuiDDz7Q0aNH5e/vr+LiYnl4eGjJkiW699571blz5yrnAgAAAIB7UXiW4+WULKVlWy8c5J3Xy5uz9FL/NlW65pQpU5Sbm6uJEyc6Cqfu3bvryy+/1M8//6yEhARZLBYFBgZq3rx5Wrp0qZKTk5Wenq6hQ4dq4MCBjmJyxYoVGjhwoAYNGqRly5Zp9erVKioq0vPPP6+JEydq8eLF+uSTT3T77bdr2bJl+uKLL+Th4aF333231HNHjBihb775Rj4+Pvr0008lScuXL9cf/vAHDR06VCtXrtTq1at1yy23qGPHjpo5c6bjufr166d+/fpp9OjRZU4D9vPz0+nTpx3Hubm5OnnypKxWq1O/nJwc/fLLLwoNDZW/v7/TZwUFBcrOzlZubq6aNWtWpXwAAAAAcC8Kz3Lk2Zyns56xuT699XLmz5+vc+fO6bPPPpMkJSYmKiMjQ8uXL9dXX32lQ4cOqXHjxuratask6YYbbtCpU6eUnp6u4cOHS5JuvvlmZWRkKD09XcOGDZMk9enTRytXrpQkNW3aVAEBAfLz83MUdJdOtb3cuYZhOC3Nv2fPHv3www9asWKFzp8/r379+ikoKEjR0dH68MMPFRAQoLvuusvl52/QoIGsVqtycnJks9mUm5urjRs3Kjc316lffn6+zpw5I8MwlJmZWeq18vLyKDwBAAAAk6PwLEcjb08p77zzcTX4/e9/ryVLlujw4cM6dOiQ7rjjDtWrV0+ffPKJevToIUny8PBw/NVut6tjx47aunWr7rnnHn377bdq3ry52rVrp61bt2rgwIHasmWL2rdvr2PHjrkUQ2nnXlS//n+nE7dv3149e/Z0FL0XtWnTRm3atNHMmTMVFBSkkJAQSdL58+dVnlmzZumFF16QYRhauHChgoKCtGvXLqc+QUFB8vDw0LPPPlvi/ObNm2vBggUuPScAAACA2sWqtuWY3KeFOgX4qEWjeuoY4KPJfVpU6XoWi0WenheK1z//+c969dVXNWTIEC1evFiDBg1S165d5enpKU9PT1ksF9Jz8fj+++/Xf/7zHw0dOlR5eXny9PTUQw89pISEBEVFRWnnzp0aPHiwvLy8ZLFYHH+9eP7F+15U2rlbtmzRhg0b9PDDD+vw4cOSpAcffFAbNmxQVFSUHnzwQR0/flypqanq37+/IiIilJGRoZYtWzqu6+Pjo+HDh2v9+vWX/T20bdtWixYt0iuvvKKgoKDL9gsODtaKFYsahb8AACAASURBVCtK/OTk5FQ6BwAAAABqloe9svt5XCGysrKcjgsKCuTr61tqXy8vrxJbeaDmxMbGavr06Ro3bpz+9a9/Sbp8Tsp7j/RSZeUdFRcQEKDs7OzaDgOXIC/mQ07MibyYDzkxJ/JiTmbPS4sWpQ/UMeIJ05g+fbokacaMGeX2nTBhgrvDAQAAAFBNKDxhOq1atSq3z8X3YAEAAACYH4UnAAAAAMCtKDwBAAAAAG5F4QkAAAAAcCsKTwAAAACAW1F4otpt2rRJmzdvdhzPmTNHkjRx4sQSfdetW6eYmBjFxMRo27ZtNRYjAAAAgJpD4VnDPv/8cy1ZskSSFB8fr3fffVeS9MEHHygyMlJDhgxRdna2Vq5cqbvuukvDhg3TtGnTVF3brY4dO9bp2G6367nnnquWa0vS3LlztX//fu3YsUNz586VJGVkZEhSif04p0+frrfffltHjhzRkSNHtHjxYq1Zs6baYgEAAABgDl61HUBdc/fdd+v+++/Xfffdp5UrV2r58uVKS0vT2rVrlZCQIA8PD0lScXGxpkyZooiICL344ovas2ePunbtWuX7FxYWOh17eHjob3/7W5WvK0mGYSgtLU3Tpk2TJA0fPlxTpkzRwYMHS+0fGxsrScrOztaqVat04MAB9ezZs1piAQAAAGAeFJ7lOJtXrB3fFKjQZqi+t0Wht/iqoZ9npa/n4eGhMWPG6L777tO4ceNUr149rVmzRhMnTpTFUnIA2maz6dChQ/L391d+fr6eeuopnTx5Ui1atND8+fN1/vz5Em3FxcV67LHHZLPZ5OPjo+XLl2vt2rV64403tG/fPkVFRenxxx/XbbfdppEjR+rw4cNKSUmRJH344YdKTU1VZmamPDw8tHTpUhUVFekvf/mLTp48qS5duqh79+6KiooqEWtOTo4CAwMdx8HBwXr66ac1e/bsEn0PHDiglStX6ujRo/L391dxcbE8PDy0ZMkS3XvvvercuXOlf8cAAAAAzIXCsxw7vinQyZxiSdLZ/GLt2FqgP/RtVKVr3nLLLUpPT1d4eLgk6ejRo2rVqpXmzJmjzZs3Kzo6WpL00ksvaerUqXrooYd0/fXX6/XXX9fAgQM1aNAgLVu2TKtXr9apU6dKtN1yyy3q2LGjZs6c6bhnv3791K9fP40ePVpvvfWWoz0+Pl6jR492ii8wMFDz5s3T0qVLlZycrEOHDun+++/XH//4R02aNEnFxcWlPpefn59Onz7tOM7NzdXJkydltVqd+uXk5OjQoUMKDQ2Vv7+/02cFBQXKzs5Wbm6umjVrVonfLgAAAACz4R3PchTajDKPK+Of//ynxo4dq3/+85+SpObNm+vw4cN69tlnNX36dEfxNnXqVKWmpqqoqEgbNmxQenq6evfuLUnq06ePDh48WGpbUFCQoqOj9eGHH+p///d/KxzfxSm9N9xwg06dOqX9+/erR48ekqTQ0NDLntegQQNZrVbl5OQoKytLubm52rhxo3Jzc5365efnKy8vT+fOnVNmZqbTT25urnJzc5WXl1fhuAEAAACYU42MeB45ckTz5s3TU089pdatW2v58uXau3evgoODNWbMGEmqUps71fe26Gx+sdNxVRw/fly7d+/Wu+++q8cee0xHjhzR3Xffrfnz5+tf//pXif5eXl4KDAxUXl6e2rVrp61bt2rgwIHasmWL2rdvr5MnT5Zok6Q2bdqoTZs2mjlzpoKCghQSEiJJOn/+fLkxXnzP1MPDQ3a7XS1bttT27dv1xz/+Udu3b1dYWNhlz501a5ZeeOEFGYahhQsXKigoSLt27XLqExQUpOLiYv31r38tcX7z5s21YMGCcmMEAAAAcOVw+4inYRhKSEhQaGioDMNQRkaGDMNQbGysmjZtqn379lWpzd1Cb/HVNf6eaujnoWv8PRV6i2+Vrrd48WJNnjxZ0oXtRV577TV16NBBffv21dChQxUbG6sOHTrI09NT8+fPV1RUlH7++WdFRkbqoYceUkJCgqKiorRz504NHjy41LbU1FT1799fERERysjIUMuWLR339/Hx0fDhw7V+/XodOXJEUVFR+vbbbxUVFaUff/xRnp6ejndNPT095enpqZEjR+qtt97SiBEjZLVa1bhx48s+X9u2bbVo0SK98sorCgoKumy/4OBgrVixosRPTk5OlX6/AAAAAMzHw15d+3SU46OPPlKvXr30008/6frrr9dXX32lAQMGaPfu3fL19a1025AhQ8q8b1ZWltNxQUGBfH1LLx69vLxKbPmBCyvsenp6ym6364knntDzzz/vVMyWJzY2VtOnT9e4ceNKHdX9rUvfQa2unJSVd1RcQECAsrOzazsMXIK8mA85MSfyYj7kxJzIizmZPS8tWrQotb3GFxfKz8+Xr6+vDMNQw4YNlZ+fL8MwKt0G99uzZ49joaIhQ4ZUqOiULuzXKUkzZswot++ECRMqHiAAAAAAU6vxEc99+/apZcuW6tatm/bv36/vv/9evr6+lW4bNmyY033WrVundevWSZLmzp1bYt/KY8eOydvbuyYeGSZis9l03XXX1XYYVw1mB5gTeTEfcmJO5MV8yIk5kRdzMnte6tevX2p7jY94hoSEKCUlRd26ddPOnTvVoUMH+fr6VrrtUn379lXfvn0dx5cOQ9tsNnl6lr4Pp9mTWBdVV05sNpuppyRcacw+xaOuIi/mQ07MibyYDzkxJ/JiTmbPy+Wm2tbYdioWi0UWi0UhISEqKirSzJkzdeLECXXt2rVKbQAAAAAAc6uxqba1hcWFrmwsLmROZv+mra4iL+ZDTsyJvJgPOTEn8mJOZs9LrY94wr02bdqkzZs3O47nzJkj6cKWLZdat26dYmJiFBMTo23bttVYjAAAAADqJgrPGlZUVKRJkyZp2LBhmjRpkmM074knnnDqZ7PZNHbsWEVFRWnUqFFlXnPu3Lnav3+/duzYoblz50qSMjIyHPf7renTp+vtt9/WkSNHdOTIES1evFhr1qyppqcDAAAAgJJqfHGhum7t2rXq1KmTFi5cqNdff11ffvmlIiMjdf78ead+GzZsUM+ePTVmzJgyr2cYhtLS0jRt2jRJ0vDhwzVlyhQdPHiw1P6xsbGSLiy6tGrVKh04cEA9e/ashicDAAAA8FtH8gr1ckqW8mzFauTtqcl9Wqi5X+mrvl7tGPEsx6lTp/TRRx/pnXfe0UcffaTTp09X6Xrbt29Xv379JEkRERHavn17qf06d+6sn376qURBeqmcnBwFBgY6joODg/X0008rODi4RN8DBw7opZde0lNPPaUlS5bo6NGj8vDw0JIlS/Tjjz9W4akAAAAAXOrllCylZVuVlXdeadlWvbw5q/yTrlKMeJZj7dq1Onr0qCTp9OnT+uKLL3TfffdV+npnzpxR06ZNJUlNmjS5bCHbunVrPfLII5o+fboefPBBhYaGltrPz8/P6Rq5ubk6efKkrFarU7+cnBwdOnRIoaGh8vf3d/qsoKBA2dnZys3NVbNmzSr9bAAAAAD+K89W7HR85pLjuoTCsxznzp0r87iiGjdurFOnTqlZs2Y6ffq0mjRpctm+nTt31rx58xQTE6PAwEC1atWqRJ8GDRrIarUqJydHNptNubm52rhxo3Jzc5365efnKy8vT5KUmZlZ6v3y8vIoPAEAAIBq0sjbU8o773xcR1F4lqNBgwZOI4oNGjSo0vV69OihtWvXauzYsUpKSlKPHj1K7VdYWKj69evLw8NDTZs21dGjR0stPCVp1qxZeuGFF2QYhhYuXKigoCDt2rXLqU9QUJCKi4v117/+tcT5zZs314IFC6r0XAAAAACcTe7TQi9vztKZ37zjWVdReJajf//++uKLL3Tu3Dk1aNBA/fv3r9L1+vXrp5iYGA0bNkytW7fWo48+Kkn67rvvFBUVJenCAkHt27fX7NmzVa9ePXXt2rXMBYDatm2rRYsWlXvv4OBgrVixokR7eavmAgAAAKi45n719VL/NrUdhilQeJajSZMmVXqn81JeXl5auHBhifbvv/++RNtnn31W6fvccMMNLve1WFhjCgAAAID7UHFcpaZPny5JmjFjRrl9J0yY4O5wAAAAANRhFJ5Xucu9F/pbl3vPFAAAAACqA4UnAAAAAMCt6lzhabfbazsE1ALyDgAAANSeOld4WiwWFRUV1XYYqEFFRUUsoAQAAADUojq3qq2Pj4+sVqtsNps8PDycPvP29pbNZqulyFCaqubEbrfLYrHIx8enGqMCAAAAUBF1rvD08PBQgwYNSv0sICBA2dnZNRwRykJOAAAAgCsf8w8BAAAAAG5F4QkAAAAAcCsKTwAAAACAW1F4AgAAAADcisITAAAAAOBWFJ4AAAAAALei8AQAAAAAuBWFJwAAAADArSg8AQAAAABuReEJAAAAAHArCk8AAAAAgFtReAIAAAAA3IrCEwAAAADgVhSeAAAAAAC3ovAEAAAAALgVhScAAAAAwK0oPAEAAAAAbkXhCQAAAABwKwpPAAAAAIBbUXgCAAAAANyKwhMAAAAA4FYUngAAAAAAt6LwBAAAAAC4FYUnAAAAAMCtKDwBAAAAAG5F4QkAAAAAcCsKTwAAAACAW1F4AgAAAADcisITAAAAAOBWFJ4AAAAAALei8AQAAKZnGIYSEhI0depUJSQkyDCM2g4JAFABXrUdAAAAQFkMw1B0dLRSUlJks9kUHx+vsLAwxcXFyWLhO3QAuBLwpzUAADC1xMRER9EpSTabTVu2bFFSUlItRwYAcBWFJwAAMLXk5GRH0XmR1WpVcnJyLUUEAKioGp9qe+rUKb322ms6f/68fH19NWHCBH388cfau3evgoODNWbMGEnS8uXLXWoDAABXt/DwcMXHxzsVnz4+PgoPD6/FqAAAFVHjI57r16/X0KFDNWvWLN18882OBQJiY2PVtGlT7du3TxkZGS61AQCAq19kZKTCwsLk7e0t6ULR2bt3b0VERNRyZAAAV9X4iGenTp20e/duBQcHa8+ePWrbtq06deqkxYsXa8CAAdq9e7d8fX0VGhpablunTp1qOnwAAFDDLBaL4uLilJSUpOTkZIWHhysiIoKFhQDgClLjhWeHDh2Umpqqjz/+WK1atdK5c+fk6+srwzDUsGFD5efnyzAMl9pKs27dOq1bt06SNHfuXAUEBLgcm5eXV4X6w/3IiTmRF3MiL+ZDTqpXdHS0oqOjq3wd8mI+5MScyIs5Xal58bDb7faavOG7776r/v3769prr9XBgweVmpqqTp06qVu3btq/f7++//57+fr6qmXLluW2DRs2rNz7ZWVluRxbQECAsrOzq/J4qGbkxJzIizmRF/MhJ+ZEXsyHnJgTeTEns+elRYsWpbbX+ByV7Oxs1a9fX9KFdzT27t2rHTt2SJJ27typkJAQhYSEuNQGAAAAADC/Gi88hw8frjfeeEOvvfaali9frrFjx6qoqEgzZ87UiRMn1LVrV4WEhLjUBgAAAAAwvxqfalvTmGp7ZSMn5kRezIm8mA85MSfyYj7kxJzIizmZPS+mmWoLAAAAAKhbKDwBAAAAAG5F4QkAAAAAcCsKTwAAAACAW1F4AgAAAADcisITAAAAAOBWFJ4AAAAAALei8AQAAAAAuBWFJwAAAADArSg8AQAAAABuReEJAAAAAHArCk8AAAAAgFtReAIAAAAA3IrCEwAAAADgVhSeAAAAAAC3ovAEAAAAALgVhScAAAAAwK0oPAEAAAAAbkXhCQAAAABwKwpPAAAAAIBbUXgCAAAAANyKwhMAAAAA4FYUngAAAAAAt6LwBAAAAAC4FYUnAAAAAMCtKDwBAAAAAG5F4QkAAAAAcCsKTwAAAACAW1F4AgAAAADcisITAAAAAOBWFJ4AAAAAALei8AQAAAAAuBWFJwAAAADArSg8AQAAAABuReEJAAAAAHArCk8AAAAAgFtReAIAAAAA3IrCEwAAAADgVhSeAAC4gWEY+vjjjzV16lQlJCTIMIzaDgkAgFrjVdsBAABwtTEMQ9HR0dqyZYusVqvi4+MVFhamuLg4WSx85wsAqHv4rx8AANUsMTFRKSkpslqtkiSbzaYtW7YoKSmpliMDAKB2UHgCAFDNkpOTZbPZnNqsVquSk5NrKSIAAGoXhScAANUsPDxc3t7eTm0+Pj4KDw+vpYgAAKhdFJ4AAFSzyMhIhYWFycfHR9KForN3796KiIio5cgAAKgdLC4EAEA1s1gsiouL0+bNm5WQkKDw8HBFRESwsBAAoM6i8AQAwA0sFouGDh2qW2+9tbZDAQCg1vHVKwAAAADArSg8AQCoIMMwlJCQoKlTpyohIUGGYdR2SAAAmBpTbQEAqADDMBQdHa2UlBTZbDbFx8crLCxMcXFxvMMJAMBluPRfyJ9//lmFhYU6evSoDh486O6YAAAwrcTEREfRKUk2m01btmxRUlJSLUcGAIB5uTTiuWvXLvn7++vQoUM6evSogoODq3TT7777TqtXr5aXl5cef/xxrV+/Xnv37lVwcLDGjBkjSVq+fLlLbQAA1KTk5GRH0XmR1WpVcnKyBgwYUEtRAQBgbmUWnnv37tWGDRv0yy+/KDMzU6dPn9a5c+d0+PBhBQcHKzw8XG+88YYCAgI0cuRIl26Ym5urb775RrNnz5anp6cyMjJkGIZiY2O1cuVK7du3T76+vi61derUqVp+CQAAuCo8PFzx8fFOxaePj4/Cw8NrMSoAAMytzMKzdevWJTa7NgxDdrtdTZo00QcffKCePXsqPT1dX3/9tf7whz+Ue8Ovv/5azZo106xZs9SlSxf5+/srNDRUixcv1oABA7R79275+vq61EbhCQCoaZGRkQoLC3NMt/Xx8VHv3r1L/PcSAAD8V5mFp5+fn/z8/JzaNmzYoOLiYt11111KT0/X6NGj1bFjR73//vsuFZ7Hjx+XxWLRiy++qI8++kinT59Wu3btZBiGGjZsqPz8fBmG4RjhLKutNOvWrdO6deskSXPnzlVAQICrvwt5eXlVqD/cj5yYE3kxJ/JSc9asWaNPP/1U69ev15133qnBgweXurAQOTEn8mI+5MScyIs5Xal5KfcdzzVr1ujo0aMKCAjQoEGDFBgYqO3btzv18ff318mTJ126oY+Pj0JDQyVJPXv2VGpqqgoKCjRp0iTt379fvr6+8vX1damtNH379lXfvn0dx9nZ2S7FJUkBAQEV6g/3IyfmRF7MibzUrFtvvVW33nqrpAuvkZSGnJgTeTEfcmJO5MWczJ6XFi1alNpe7qq2X331lW655RZt3LhRkhQYGOj4D+zFb3fPnz+v+vXruxRI+/bt9eOPP0qS4687duyQJO3cuVMhISEKCQlxqQ0AAAAAYH7lFp7e3t7q0qWLfHx8JEm+vr6yWq2SJE9PT1mtVh04cEA33HCDSzfs1auXjh8/rpkzZyorK0tRUVEqKirSzJkzdeLECXXt2lUhISEutQEAgNpjGIYSEhI0depUJSQkyDCM2g4JAGBSLm2n8ls+Pj46f/68JCkiIkLPP/+8DMNQTEyMS+d7eHho/PjxTm2jR48u0c/VNgAAUPMMw1B0dLRjkaX4+HiFhYUpLi6u1PddAQB1W7mFp91uv9DRy0v/+c9/ZBiG4xvNXr16qUOHDvL29laDBg3cGykAADCNxMRER9EpSTabTVu2bFFSUhL7mQIASij3K8no6GhJ0qOPPqomTZrommuucRp5bNq0KUUnAAB1THJystNeppJktVqVnJxcSxEBAMys3BHPjh07Srqwp2fr1q3dHhAAADC/8PBwxcfHOxWfPj4+Cg8Pr8WoAABmxUsYAACgwiIjIxUWFiZvb29JF4rO3r17KyIiopYjAwCYUZkjnsXFxdq0aZNLq9R5enrqtttuq7bAAACAeVksFsXFxSkpKUnJyckKDw9XREQECwsBAEpV7lTbS4vOb775RiEhIfL393dqv7gIEQAAqBssFosGDBhgusWEDMNQYmKioyCOjIykIAaAWlZm4enp6anw8HAVFxdLulBcZmZmqnv37iXe96xXr577ogQAAHAB27wAgDmVO+K5ZMkS5eTkOLWlp6eX6BceHq477rij+iIDAACoILZ5AQBzKrfwnDBhQk3EAQAAUGVlbfNC4QkAtafcwtNms2nmzJmO6bYX2e12de7cWWPGjHFbcAAAABXBNi8AYE7lFp5nz55VmzZtNG7cuBKf/f3vf3dLUAAAAJVxcZuXi9Nt2eYFAMyh3MJT0mVfxuclfQAAYCZs8wIA5uRS4Wm1WpWfn+/UZhiGioqK3BIUAABAZZl1mxcAqMvKLTwbN24sm82mBQsWlPisc+fObgkKAAAAAHD1KLfw9PLy0jPPPCNJ+v7779WpUyfVr1/f7YEBAAAAAK4OFXrhYcuWLTp79mypn1261ycAAAAAAFI5I54FBQVatmyZ4zgtLU3vvPOOY8TT09NTTzzxhCRp4cKFevHFF90XKQAAAADgilRm4enj4+O0/PjFvzcMQxaLxWmFOMMw3BQiAAAAAOBKVmbhabFY1LhxY3l6euqaa66RJJ05c0arVq3SI4884tTXw8PDfVECAAAAQB10JK9QL6dkKc9WrEbenvrbPQ11Ja64U+7iQrt371Zubq5+97vfSZLsdrtOnDjh9sAAAAAAoK57OSVLadnWCwd55zU76Sf9/a6WtRtUJbi0j2daWppOnTrlOD537pzbAgIAAAAAXJBnK3Y6PmU9X0uRVI1Lheedd96pm2++2XE8f/58twUEAAAAALigkbenlPffYrOpT71ajKbyXCo8f/nlF/n4+DiO8/PzJUlJSUlKSUmR3W53vAMKAABwpTEMQ4mJiUpOTlZ4eLgiIyOdFlEEgNoyuU8Lvbw5S2f+7x3P5yM6SOdL3+LSzFwqPH/99VcVF/93iDcvL0+S1LdvX4WHh0uSfH193RAeAACAexmGoejoaKWkpMhmsyk+Pl5hYWGKi4uj+ARQ65r71ddL/ds4jgOaNFB29lVYeFosFoWHh6tXr16Otl9//fXCyV5e8vJyqXYFAAAwpcTEREfRKUk2m01btmxRUlKSBgwYUMvRAcDVodyv8S4tOiXpL3/5i9sCAgAAqEnJycmOovMiq9Wq5OTkWooIAP7LMAwlJCRo6tSpSkhIkGEYtR1SpVRq/shv3/cEAAC4koWHh8vb29upzcfHx/E6EQDUlouvAkycOFHLly/XxIkTNWTIkCuy+OTFBQAAUKdFRkYqLCzMUXz6+Piod+/eioiIqOXIANR1pb0KsHHjRiUlJdVyZBXHC5oAAKBOs1gsiouLU1JSkmNV24iICBYWAlDrkpOT5dcwQH9+/CX5NbxG+WdP6q0Vzyg5OfmKewedwhMAANR5FotFAwYMuOL+Rw7A1S08PFwt/QeobdD/SJKuUxs9+qf5at+1oJYjqzi+ygMAAAAAE4qMjJS/f3OntsCAFlfkqwAUngAAAABgQhaLRa2DWji1BQffcEW+CnDlRQwAAAAAdUSPW3x1jb+nGvp56Bp/T93W77raDqlSeMcTAAAAAEyqoZ+n/tC3keO4UeP6smXXYkCVxIgnAAAAAMCtKDwBAAAAAG5F4QkAAAAAcCsKTwAAAACAW7G4EAAAAIBKOZJXqJdTspRnK1Yjb09N7tNCzf3q13ZYMCFGPAEAAABUysspWUrLtior77zSsq16eXNWbYcEk6LwBAAAAFApebZip+MzlxwDF1F4AgDqFMMwlJCQoKlTpyohIUGGYdR2SABwxWrk7VnmMXAR73gCAOoMwzAUHR2tlJQU2Ww2xcfHKywsTHFxcbJY+C4WACpqcp8Wenlzls785h1PoDQUngCAOiMxMdFRdEqSzWbTli1blJSUpAEDBtRydABw5WnuV18v9W9T22HgCsDXuwAAU6vOqbHJycmOovMiq9Wq5OTkqoaJy2BqMwBAYsQTAGBi1T01Njw8XPHx8U7Fp4+Pj8LDw6szbPwfpjYDAC7iT30AgGmVNTW2MiIjIxUWFiZvb29JF4rO3r17KyIiotpixn9Vd/4AAFcuCk8AdRZTAM2vuqfGWiwWxcXF6dVXX9Wf/vQnvfLKK4y+uRFTmwEAFzHVFkCdxBTAK4M7psZaLBYNGDCAxYRqAFObAQAX8X9XAOokpgBeGZgae2UjfwCAi2ptxPODDz5QZmamYmJitHz5cu3du1fBwcEaM2aMJLncBgCVUdYUQEbCzOPi1NikpCQlJycrPDxcERERjEpfIcgfAOCiWvmTPzMzU15eXjIMQxkZGTIMQ7GxsWratKn27dvnchsAVFZ4eLhjFOYipgCa08WpsXPnztWAAQMoWq4w5A8AINVS4fmf//xHgwYNkiTt27dPoaGhWrx4sbp37659+/a53AYAlcUUQAAAgJpT41NtU1JS1LNnT9WvX1+SlJ+fL19fXxmGoYYNGyo/P1+GYbjUVpp169Zp3bp1kqS5c+cqICDA5di8vLwq1B/uR07M6WrJy5o1a/Tpp59q/fr1uvPOOzV48OArejTmasnL1YScmBN5MR9yYk7kxZyu1Lx42O12e03ecNmyZTp79qwkaffu3SosLNSkSZPUrVs37d+/X99//718fX3VsmXLctuGDRtW7v2ysrJcji0gIEDZ2dmVfjZUP3JiTuTFnMiL+ZATcyIv5kNOzIm8mJPZ89Kixf9n784DoyrPxY9/zzmzL5msECKrbCouUDc2o1ZKCEprrUutitarttwWrbZqrV3cq/ZntdXqvVpvFUEtUK9XDQHEKpFFrYJVUEFQXAiBJGSdzH7m98cwkzmTyUpIJuH5/MU5s5x3zpkM88zzvM9blHZ/n2c8r7jiisS/77//fs477zw2bNjAcccdx/vvv8+ECRNwOBxd2ieEEEIIIfqXruuUl5cnGkiVlpYO6OoRIcSh0a+fCmazmXHjxhEOh/ntb39LdXU1xx57bJf3CSGEEEKI/hNf96lPlQAAIABJREFUE3nhwoUsXryYhQsXMn/+fHRd7++hCSEyTL8tpwJw/fXXA3DllVe2ua2r+4QQQgghRP8oLy/nk493ce01T+Jy5tDsrWPJ8t+wcuVKWZpKCGEgdRBCCCGEEKJHKioquPSCOxk7ZgpDh4xm7JgpXHL+nVRUVPT30EQf0XWdsrIybr75ZsrKyiTbLdrVrxlPIYQQQggxcBUXF1P1ea5hn9uVy4TjZU3kw0G81HrDhg0EAgGWLVvG9OnTWbRokczzFW3IO0IIIYQQQvRIaWkpSkoaQzEhayIfJsrLyxNBJ0AgEGDjxo2sXLmyn0cmMpFkPIUQQgghRI+oqsrnI7KJNAaxKyr+qM4XI7Ml23WYqKiowOXM59pr7kvM8f2fJTdRUVEhc3xFGxJ4CiGEEEKIHquJhPlA35/YLgqb+3E0oi8VFxdzRN5cxow6AYChjOY/LvsD449t6eeRiUwkP0cJIYQQQogec1u1DrfF4FVaWkpeXqFhX37eMCm1FmlJ4CmEEEIIIXrshhlFHJVvo8htZmK+jRtmFPX3kEQfUVWVkaOM13vkqCIptRZpSamtEEIIIYTosUKXhftKRvf3MEQ/+cZUB5vfaiEY0LFYVaZMdfT3kESGksBTCCGEEEII0SNOl8bMWe7+HoYYACQPLoQYEGSBaiH6n/wdCiGE6CnJeAohMp4sUC1E/5O/QyGEEAdD/qcQQmQ8WaBa9CZvU4R1a5r4Z1kD69Y04W2O9PeQBgT5OxRCCHEwJPAUQmS8ioqKxJfdOL/fT0VFRT+NSAxkm99uoa42grc5Sl1thM1vZf56c5lQ4ip/h0IIIQ6GBJ5CiIxXXFyM1Wo17LPZbBQXF/fTiMRAFgzoHW5nmniJ68KFC1m8eDELFy5k/vz5fR58yt+hEEKIgyGBpxAi45WWljJ9+vTEl16bzca0adNkgWrRIxar2uF2psmUElf5OxRCCHEwpLmQECLjqarKokWLWLlyJRUVFRQXFzNnzhxpaCJ6ZMoAW3OuoxLXuXPn9tk45O/w0NB1nfLy8sQ5LS0tlXMqhBiUJPAUQgwIqqoyd+7cPv2iLQangbbmXHFxMcuWLTMEn/1V4ip/h71LOgULIQ4n8qkmhBBCZDApcR28ysvL+eTjXVx7zZPc/ssyrr3mSbZ9sks6BQshBiXJeAohhDis7GkK8uCGSpoCEdxWjRtmFFHosvT3sNolJa6DV0VFBZdecCdjx0wBYCijueT8O6moWCVZZSHEoCOBpxBCiMPKgxsq2Vbjj200hXhwfSX3lYzu1zF1RkpcB6fi4mKqPs817HO7cplwvHQKFkIMPvJzqRBCiMNKUyBi2G5M2Rb9KxPWLO0rpaWlqJrx/adqESmjFkIMSpLxFEIIcVhxWzVoChm3RUY43JrtqKrKBZccx+ur9uBrCWN3mLjgkuMG5WsVQggJPIUQYpCT5RqMbphRxIPrK2lMmuMpMkNHa5YO1jJjV5aZeReM7O9hCCHEISeBpxBCDGKHWwapKwpdloyf03m4ypQ1S/vSQGt2JYQQPXV4fusQQoiDNFDmoXWUQRIi0xQXFyeWjYnrrzVL+0q82VVlU4htNX4eXF/Z30MSQohDQgJPIYTopngWceHChSxevJiFCxcyf/78QxZ8HkyQ21EGSYhMcziuWSrNroQQhwsptRVCiG7qy3loB1sqW1xczLJlywzB52DPIImB63Bcs1SaXQkhOlNfX8/q1avx+XzY7Xa+//3v9/eQemTwfpILIcQh0pdZxIMtlT0cM0hiYIuvWXrvvfcyd+7cQR10QqzZ1VH5NorcZibm26TZlRCDTH19PUuXLuXpp59m6dKlNDQ0dPs5Vq9eTVVVFQ0NDVRVVfGPf/zjEIz00JOMpxBCdFNfZhEPttnK4ZhBEplPOi23kmZXQgxu8aARoKGhgVWrVnHhhRd26zl8Pp9h2+v19tr4+tLh+SkvhBAHoS+ziL3RbGWvN8wb2jE0TL+aN7Rj2NcS7u1hCtFlfT1HWggh+lNq0Ji63RV2u92w7XA4DmpM/UUCTyGE6KZ4FvGRRx7hsssu4+GHHz5ky5P0RpArXTNFJpFOy0KIw0lq0Ji63RUlJSUUFhbi8XgoLCzk/PPP763h9SkptRVCiC5IVxo4d+7cQ762YG+UykrXzP6zu97Hb1btkjUak1RUVJBTdDTXXPUX7GYbvpCfx59YMKjX6hRCHL5KSkpYtWpVojFQSUlJt5/D4/EYynNzc3OpqanpzWH2CQk8hRCiEwfbWfZgxZut9PRLuXTN7D93rNrOthp/bKMpxIPrKw/7+XzFxcUcecwCimzZAOSYbFxzzWOMKPion0cmhBC9LzVoPJxJqa0QQnRioJcGStfM/lPvDxm2JdscKx93mo2lZk6zXTotCyHEICcZTyGE6MTBdpbtb9I1s/9k28x8jT+xLdnmWAZfsWiQFIMrFu2w7WorhBCHC/mUF0KITvRGZ9mDpes6ZWVl3HzzzZSVlUkH0AHid3MmSLY5jRnFThq1MM1EaNTCzCh2dvoYb1OEdWua+GdZA+vWNOFtluyxEEIMJJLxFEKITsQ7y8bLbQ/l8inp9PccU9FzRR67ZJvTGD3Ezujzu9fZcfPbLdTVxoJNb3OEzW+1MHOW+1AMTwghxCEggacQQnSiNzrLHoyO5pgOhFJfIXpDMKB3uC2E6B/19fWsXr3a0LXV4/H097BEBpKfyoUQogvinWXvvfde5s6d26eZxo7mmApxuLBY1Q63hRD9Y/Xq1VRVVdHQ0EBVVRWrVq3q7yGJDCWf2kIIkeEyYY6pOHgyT7dr2pvLOWWqg5w8DadLISdPY8pURz+PVAgB4PP5OtwWIk5KbYUQoo/ouk55eXmiXLe0tLRLmdP+nmMqeq6xIci6NU0EAjpffrmTv/z1HvZU7ZJ5uh1oby6n06XJnE4hMpDdbqehocGwLUQ6EngKIUQfOJgGQf09x1T0XMWrexNBVH7ukVx6wV384eFLZZ5uB2QupxADS0lJCatWrTLM8RQiHQk8hRCiDxxsg6D4HFMJUgaWgN+45IfLmZP490BaC7YvWayqYamUvprL6W2KsPntFoIBHYtVZcpUB06XrLsqRGc8Hg8XXnhhfw9DDADyc7kQQvQBaRB0eLLajIFLs7cu8W+Zp5tef83ljJf4epuj1NXGSnyFEEL0Hsl4CiFEF/R0fmZccXExy5YtMwSfEngMfqfPHsprK3Yn5nguXvZrAJmn24HemMu5pynIgxsqaQpEcFs1bphRRKHL0uFjWnyRDreFEEIcHAk8hRCiEwczPzNOGgQdntxZlkQQpeuTGTrq1kM+T/dgfyQZDB7cUMm2Gn9soynEg+srua9kdIePqQ6EyUr6WlTtD/PMsmqUiII/GmEz+7j17PEUZdkO4ciFEGLwksBTCCE6cbDzM0EaBIm+mafbGz+SDAZNAWO2sjHQefbyXVMTx4Sc2BUVf1RHi0K+YgEFPIqJyfpQfvL0m/zjJ2cdVudSCCF6iwSeQgjRiY7mZ3YniJAGQeJQ640fSfpbT8pkU+WbTJyourEdCCK/MHW+rqDZrvCKd39i+3w1D5Sk51RMmLAOqHMphBCZRH6yE0KIThQXF2O1Wg37ZH6myESDoYlVvEy2sinEtho/D66vBGIB6U2rdrHgpZ3ctGoXVc3Bdp/jTC2HoaoFj2JiqGrhTFNOu/eNu2FGEUfl2yhymxmi+WgJGpsLmRSVs9wjB9S5FEKITCKBpxBCdCI+PzMefMr8TJGpBsOPJO2VybYXkKYVjhq3Q9H090tS6LJwX8loHvv2WHI3P8/jj/+YsG4ci7mleUCdSyGEyCR9XmpbU1PDE088QSAQYMiQISxYsIAlS5bw0UcfceSRR3LVVVcBsHjx4i7tE0KIQ03mZ4qBoj+bWPVGiSyA26pBU8i4TffmbR7sWqDFxcUsXbqUL774gLFjpiT2R8Je+cFJCCF6SLvttttu68sDqqrKtGnTmDVrFjt37sRkMrF7925uuOEGPvvsM1RVpaWlhc8//7zTffn5+Z0er6mpqctjczgctLTIul2ZRK5JZjocr4uiKIwfP55Zs2Yxfvx4FEXp/EF97HC8Lpmur6+Joiice+65TJgwgezsbBYsWMCNN97YJz+S3PnGV2yr8dMU1KltCbOj1se3xmV3+3mOK3Swo9aHRVMoyrJww4wiXBaNil2N1LaEE/cryrK0+/x5Q0zU74+gaeByx9YCtVhaz0Fn12XcuHG89957vPbGC4w44hhC4SBNzXv5/uWTsdnN3X5Ng119fT0vvfQS7777Ltu3b2f48OHYbN3r/iufX5lJrktmyvTr4nanXxJLiUajndefHCIvvvgiVquV4cOH8/rrrzN37ly2bNmCw+Fg2LBhne4799xzOz1GZWUHpTgp8vPzqampOZiXJHqZXJPMJNclMw3G6zLQlwZJvia9lRHMVAte2kllUqayyG3msW+P7bXXXdUc5MH1lTT2wvnryt+KrutS5dBFS5cupaqqKrFdWFjIhRde2K3nGIyfX4OBXJfMlOnXpaioKO3+futq6/V6qampITc3F4fDga7rOJ1Ompub0XW9S/vSWbNmDWvWrAHg3nvv7VJWNM5kMnXr/uLQk2uSmeS6ZKaOrsvueh93rNpOvT9Ets3M7+ZMoMhj7+MRdo+u65x77rmsXbsWv9/P8uXLOf3003nxxRcHTACQfE1uX/UBE/Y7Yp1WvTr/vb6Ghy85vp9H2HtynbsNgWeu00Z+fj63vvZvw5qaD79dzX9fdEK3nz8/H54cnf7LTHd19TNs/vz5zJ8/v1eOOZgFg8E22939P0L+X8lMcl0y00C9Lv0SeIbDYZ577jkuuugi1q9fT0tLCz/72c/YsWMHDocjkT7ubF86s2bNYtasWYnt7vwakOm/HhyO5JpkJrkumamj6/KbVbsSX/6/xs9vXtnKfSWj+3B03VdWVsYbb7yR6NLq9/tZu3YtixcvHjDLWSRfk9ENZnLVWJmmRwFzgzKo/o4WnlrAg+tDiYzkwlMLqKmpYb/Xb7jffq+/31+3fIb1LovF0ma7u+dXrklmkuuSGfTqPUT/+kdobgKXm7wb76belLkVM+1lPPv8J+NwOMzf/vY3zjnnHNxuN+PGjWPz5s0AvP/++4wbN67L+4QQQnRNdxqzZIrBsDRInK7raCHjOddCEXRd76cR9b7krrD3l4xOlMHmm0zMU3O5QMtnnppLvkmWEB9sSkpKKCwsxOPxUFhYSElJSX8PSYhBJfrXP8Jn22BfJXy2jcaHbuvvIfVIn3/6v/DCC3z44Yfs3r0bgDlz5hAOh/ntb3/LsGHDOO+881BVlYqKik73CSGE6Jr2OoVmsuLiYpYtW2YIPgfa0iBx5eXl7K/KwTP8mMS+ur2fsXLlpgGTve2pM7UcvGoswPYocKSpe01n+lN9fT2rV6/G5/Nht9spKSnB4/H097Ayjsfj6facTiEOF6nZSuWqn6MWFHbvSZqNzVIjjfVkXovDzvV54HnhhRe2+XCaOnVqm/tdeeWVXdonhBAiveTGPFNmfBPyJxoavGS6/lwapLdVVFSwZt2HXH31ozjMNnwhP0/9406+ddoJgz7w7Mmampli9erViaY5DQ0NrFq1qtcDLAluhRjcEtlKgH0Htm+5v3tP4nLDvtZNLSubgVgvI/UuQggxCOm6zvz58xNB27Jly5g+fTqLFi0aMI15BtP6qcXFxXyafzomqwNVUdFUByPPXUjxpP4e2aF3sGtq9iefz9fhdm/oi+BWCNGPUrKVNDd2+ymUq35+IGvaCC43WT+7jfpeGl5fksBTCCEGofLy8kTQCRAIBNi4cSMrV67s9QzboVzyRFVV5s6dO+CzgqWlpeypr2KoGpv36FHg7KITmDNnWD+P7NCbMtXB5rdaCAZ0LFaVKVPTNwfMRHa7nYaGBsN2b+uL4FYI0Y9SspW40q9x2RG1oNCQJTXl58MAbPokgacQQgxCHTXm6c0grrPM6kBfh7O3qKqK22yFpCpTt9l2WJwLp0tj5qzuf9E6GD0pX019zLRp0wiHw2habD50bm7uIWma0xfBrRCi/6RmK5Wrft7fQ+o3EngKIcQAFw/u3nnnHU455RRKS0v7rDFPR5nVOXPmDPhy395U4DHjrdcN2+LQ6En5aupjXnnlFUKh1oZcmqYdkrmXJSUlrFq1yhAkCzGQ9ErznEEsNVt5OJPAUwghBrDUjOMzzzzD9OnTeeqppxKNeVzOfK667A/k5RXithThbY7gdPVOV9vUzKolp5BRF/+K/6nK43+X/pt/fbSjT8p9B4JTZzgHbMlpX2ovW9mdLGZPyldT7xMOh7v9HD0hHWHFQNcrzXPEYeHw+8lZCCEGkfYyjqtXr2bRokU88sgj3Pyz/2HMqBPIcg2lvjbC5rdaeu34xcXFWK3WxPaYi3+Fe/QkQlYP+yJ2jvjeLwz3H6jrcPaGeMnpN8/2MHOWu9eC/8EmnnlsaGigqqqKVatWdbg/ndRy1a6Ur6bex5Sy3qiUwArRjl5oniMODxJ4CiHEANbRXM54Y55hhSMMt39V4+fH/7eTm1btoqo5eFDHjy95Eg8+LS5jBsrszDZsD9R1OEXP1dfXs3TpUp5++mmWLl1qmM+YTnvZyu5kMUtKSigsLMTj8VBYWNil8tXUx8ybN6/bzyHEYSm1WU4PmueIw4OU2gohDhuDsdFNV+Zymq0qJC1n4Y1G2dMcYk9ziAfXV3JfyegeHz91yZMvXTa8SbeHvK1BxkBah3NPU5AHN1Qa1j0tdFn6e1g90pvrRH755ZeUlZURDocxmUyceeaZfPDBB/h8PkyaGa83gh4JYzbbOPucORQOy0k733L27Nntjqm9ZjvdacLTk/LVdI+RElghOifNc0RXKdFodOCs5NwDlZWVXb5vfn4+NQOwNfFgJtckMw3E65I6F9JqtQ6KRjepryse3CW/rpdfWsNnn5hw2rPRs/NZq7bQfGDp6QKHxl+/O77XxnP9r+/g3/ZjMDuzCXkb+Py5uwnW7WX8+PHcdNNNA2YdzptW7WJbjT+xfVS+rdsB+ud7fWxc14ISBl2LMqPYyeghfV+uuXTp0kTgB1BYWNjjgOqxxx4zNNzpiNNewH9cfTFPP/20IWD0eDzY7fZ2xxQPTlOD0vb298RA/Awb7OSaZCa5Lpkp069LUVFR2v2S8RRCZKzezFB2Z13LgZQZTc44/utf/+Lkk09uE9ytW/8qixcvBmDyHS9jsrU2tWkK6m2esz1dOS+zpp3I/y1c2CYDe9NNNw2ohkJNgYhhuzFluys2vOklK3Lgv9kIrK/wMvr8vg88u1OimprRnDdvHsOHD0/cntpwpyOhUCxwT5ep7GhM7WUrpQmPEEIMbBJ4CjHADKSg6GB0tj5kd3V1XcvePm5fiM/lnD9/ftpfQJPLcSPeBkPgmWXVuvSe6up5ic/5TM3ADoTy2mRuqwZNIeN2N6kRpcPtrqirq2Pp0qU0Nzfjcrm48MILycnJ6dZzdKdEtaysLJHRDIVCvPzyyyxYsCBxu8lk6nLG02S2AemXC1m1apWsXSmEEIcZ7bbbbrutvwdxKDU1NXV+pwMcDgctLb3X7VEcPLkmRvEv/48//jibN29m9erVvPvuu5x77rkoSve/1PaUw+GgubmZFStW8OSTTxIMBhk3blyvjmHFihU8/vjjiWAxEolQVVXFhAkTGD+++6WhwWCQ1atXE4m0Zq5sNhsLFiwwPF9Pjqvr+iE9Fx0dJxqNJvZFIhFGjBjR5tjjxo3j3XffZc+ePbgnnYY1e0jiNt++L3jlz7/r9D3V1fOiKArnnnsuEyZMIDs7mwULFnDjjTdmbNAOsfmcd77xFS98VEvFrkaOK3QwbaSbHbU+LJpCUZaFG2YU4bJ0L/j84JMWrNHW1+3XdApGmNocq73n1XWdBx98EE3T0DSNSCTC+++/z5dffsnw4cOx2WzU19fz0ksv8e6777J9+3aGDx+O3+837Js+fTp1dXVomkZOTg4lJSXYbLa0x3zrrbcM29FolFNPPTWxPWzYMHbs2EE0GsVsNvOtb30Lr9eLpmnU1TVgUp0oqhkiFrZs/Telpd/CbrczadIkJk+ezKRJk7DZbAwfPpy9e/d2aUyHgvzfknnkmmQmuS6ZKdOvi9udvsGUZDyFGEC6Uy56KPVFVrCrGcqu6momrrvH7asMabrjTJs2DYCNGzcSCARYvny5YX5ncoOcokvv4uJJ/+D5537PqIt+mZiD+eWy+wjX70tksdp7T3XnvMQzsKn7+7thT3vHf3BDZet8zqbWhksH03QJYEaxk/VvtqAmzfFMd6wfj3Wy+tU1RHQ/mmqjpORbjD4yn/Ly8jalraqqJpYSufDCC1lZvop91XuB2NzI8vJVqCqGZj5vvvkmmta1oDk1o5m6pMjw4cMNGVCAiRMnUlZWxu23396mxHrlyllp/26kbHZw0av3HGgu05RoLqMWFPb3sIQQGSZzf4IWQrTR0Zf/vvTiiy+2GwD3ltT1IeHgluKIz4V85JFHuOyyy/jTn/7ERRddxC233EJZWRm6rvfouB39GJBK13XKysq4+eabDcfsSPwxP/jBD1i3bp3hOOvXrzfs8/v9hmPHg5zKphDbav3syDsJf+0etj16HVv+cDnbHr0WX/XuNqWT6d5TvXE9DOOp8fPg+q43f+sN6Y7f3Bji6Do7F2j5zFNzcaGyc7//oJeZARg9xM51P5rIxRflccn5+WRbAuR99gZn1L7KGbWvctL+DbQ0NbH61TUEQtWEI00EQtWsWvUqEPt793q9aZ87Pieyvt44V7K+roWmZuOv4LW1tV1e/3LevHmYzWYURcFsNjNv3rwuvdZM+WwS/SP61z/CZ9tgXyV8ti22LYQQKSTjKcQA0pWlM/rCa6+91qvZyHQOxVzBeCZuzpw57WYpU4+rqipjx45l9uzZaZ/zUM4dTX1MqnRz7ZKPndogJ2p2YLFYCAZbgyqTyYSiKIbnMpvN7Nq1i7KyssR8z964Hr3RsKcn4suJFNQ04oqa2eo6Dr/JQYM/wtJnP6IgZxQAHgXOIJtX9P2dLjMTf06v14vf78dsNhMKhbBarbhcrkTH1f3797N06VJ8Ph9erxd7UgbTE22GyrcIp1z/cCQWOBYXF3P/fX/iiO9OQNF8QGsT+vicSE01/higqVb8AWOz+tTfNzpqLlRUVMTIkSMT831TOxN6myJsfruFYEDHYlWZMtWB06VlzGdTb5NMXhc1p0xram7sn3EIITKaBJ5CDCCZ0rjlrLPO4plnnjmkXzJT14csLi7utaU4OitZfuqpp5g7dy4ff/wxuq7z6aefcsUVV6QNErv6hbsnZdKpj0mlKAqqqraZtxo/ttuiAa0B5f6qr8g+4kjyzv4Jmj0L3ddI3o41aP6GRLmuqqqEw2HefPNN3nnnHUNw3NXrkVzSatUUFEXBH9bZ7zOWjfakYU9PxNeRtAJWYFLzh7yXfSoRXyOKzUnl/nIiET+aZsPqngZKx0FxfX09S5YsMZz35IY8zc3NibUqn3/+eUOgn0pv2IvuyTXu0wPouk5paSl7vxpOQc4oQuEmqhvXEQw1MbQwm5KSEgCOHHE6O75Ymxj/kSNOp7IuREvjhsS+kB5Cj9Qnnl81GYPV1uN2/uPI5rdbqKuNvW5vc4TNb7Uwc5Y7Yz6bDka6IDORyQPYdyCzd8v9h/y4Ay64dblhX8q2EEKkkOZCSTJ9ou7h6FBdk75qBtPbMqVxy5QpU6ioqGDPnj1EIpHEl8wbb7yxV8+joiiMHz+eWbNmMX78+F577ieffJLNmzcb9oXDYbKzs5k1axbl5eUsWbIkMb+uowZDyY17OjoXnR2zq+NMdcQRR+D3+4lEIrgLR3HsggfY7RpPxReNjPftYPOOr9FDAXzVX/HZs3dTdN4vcI06BpMzC7OngLEnn86jv/ghEyZMoLGxka+//jpRApz6urt6Pe584yu21fhpCurU+yPU+SM0BXXCOthMCkWWEMc1bGKkbxefHmiIcygby7z77ruG4F3RNOzDJ2B6dxlOawvhcC16NEhEbyEUquEzWxGjtQa2vfESGzdu5O2332bLli3s2LGD4cOHs3r16k7/b9E0jS+//NLQuTWd2ppqLFl5mGgNYv2YadxfzYQJE9j+mRkzKppqxW0fT5brGIbmTeSIkU4sFpWhwxxEfGPIcR/FsCETOXlGDtt2Rci3TcTjPBq3fTxmSyHh8H4UNMwmD5p9GpOPy24zlq40kNrxsZ9QUhytaTBmgi1jPpvi9Oo96H++g+jKF4i+sxaOOh7F6Urcnu7/Fv3Pd8SCTG8T1NXCp1vh6y+MKWOzBfWsc3p3rKnH/fxT1NO+1avHOOSOOh4+/xTMFhhahHLVzw3nuyvkO1hmkuuSmTL9ukhzITHo9HRZkYG4XEay9hq39PUYDlU2si90lqXsbiOdrpyLnpQipntMqsrKSo4++mimTJlCw0mXsidoZk9ziD3NIb7wmvn4LwsN9zc5sgzbTYFI4j21du3aNs1skl93V5sDpZbUJot46zk29D6BQIBmPzQ3NiQa5fSGeAls8tIdqcuJHJHtYFTDO9QU5RMI7UdLip8j4Vqm1a/HXe8nlJTR9Hq9eL1eXilfSTjg73Qc6daqTBWNwvr17zL7B0dji7ReY59mSZzzADoOWjPDKgp1tRHe3uDlm7OzcLo0Zs4y/gc/o9jJ+govakRB16JoOCnKLU3c3kz661NRUUFO0dFcc9VfsJtt+EJ+Hn9igeF9H1KMZbwhtXW7vc+m/sjo9ShTmVouWvkVhFPK2Q9FJm8QlKmqBYW9ngkWQgw+EniKjBZvrJIaXB5M8JgpnWEHukwIgHtC13V0XScvL4/q6mpCoVCbssDOgsR0P3p0di56UooYf8z69evbLdfUdZ2dO3dy7bXXsqwxQe1cAAAgAElEQVTZmIG0uHIwmUyGYDLkbcBWMDyxnVzuWlxczIuvVnDE936B2ekh5G2g8oX/l3jd7XV/TZVvMnGi6samqPijOq/r9TQTyxrt3/M1VeEqw1qUnQVo9fX1lJeXs3//fgBycnI4++yz8Xg8be4bL6uFWEfXZ599lqamJhRFIRAIYLVaE8t4AIagE0ABHBFvO6EZVNU1MyLP3SaTqaoqmqYZ5nimrlUZuxYWTKqGptkoyJrJ5Redyive3ZzkVDFHg4QUC++1uPnxgXP+qauFkQ12ChQzWlKGuaa+/bU0Rw+xM/r81nUxlyyvIfkF6Vo0zaNi13/8pJ8w1Br7cSLHZGPBj/6bovwtifv8vbGaaVEP9gPX9v2GRhxPf0VQsWGJ+plyZgGukcagMl0QqF91fbvBaHuBarcC2J4Ec6nloqlMZpSrft7583SXlKkKIQ4TUmqbJNPT1oNNZ+Wuuq7zgx/8gMcee6zN+oLl5eU9XuOxJyWPXR3z4WKg/q3Ef7B44oknqK+vR1VVCgsLuffee7npppsSP1p0VD4bjUZ7tJZqT0oR449pbm7mvffea/d+4XCYTZs2ERp2DGZPQWL/qDwH1e+sMHwONu3YhGPERExEKXRq3PLNMbitpsTr3hAs4mRHPWPVWkbaI2jHz+HmH5SiKAovfFRLU7C17NCiKZwzsXV+Ynw9zKOanBQoFmyKikvRGKZo/Lt6B759X/H5c3czfvQIsrJaM685OTlMmjSp3TUpn3vuOZqamohGo0SjUVpaWti7dy+TJk1qcy5Sy2p1XcdkMmEymTCbzZjN5oPKzAdMTi78dikbNmxIzK+tqanh008/5Y477uAb3/iGYa3K/fv3oygKOTk5fPe73yXcPAmH+Sjc9vFoqpWhQwvYsv6vfOw4ii+1Qr7wQvanr3Hz9deiKArHDHOwomY/dr+GS2n9kaBZiXDCJGfaNUhT1wV1aXXs3B0iRBSfHmD6ZJ2c/LbBzbhx4/jyEx016X1s08yc851jURQFXddZ/n4NY1Q7JkUhTJQjsKKacwmZnPhNHqp37GPM5ALD80ZXvhArI40zW2DLpnbLS9srPe1OSWr0nbWx+8QNLUI9rbVBWNrPsJRyUexOaGydG8vo8Wgl56U93kHphTLVwWCg/r8y2Ml1yUyZfl2k1FZklK5kLMvLy1m7dm3azOTBrPHY0+6LmVai29NS4/5wKMba3nN2dKzUbHc4HGb//v2oqmoYT7x8dsWKFSxZsgSAiy66KO1zdCdj3pMssaqqNDc3E42mz1JBrAvtvn37qFlyF2Mu/hVmZza6r5ETs1wU//a33PXAk1z9wz+RZ4l9md1TtYP/+q+f8pG3Ft8rre9hVVWZbKnFEYxn6XxMtrSeG7dVg6bWTFs8Wxovb/2qtgmXbiLqOR3MrXMILS3VHL97NS6Xi9LLL8bv9xMMBtE0DYvFQjgcpqGhoU22Mr7sR7ruvemypLqu8/XXX7f7H15777l4R9pknqxsmpqa0aOxbHFAseJTbfiOOJENGzbw9PKXWzPDjgYq//3/2rwHPB4PV199NTU1NYl9VmsTLc2t6UerTWXJ4w8f+FxbEyvVvvnhxFiH+Gq5Z9Nf+F3Bd4g6JyQyjV94Yq+/K1noUS88xKh4xhHgi6PSlkWqqoqC8X2mEDX8/ZypncgQNVZe7VEgHDW2zK02GxslAekzeh1lJNu7rRtZzERjoObGRHa0M6nlonp1VbefoyekTFUIcbiQwFP0i658ea+oqMDvN86ligeX7QWPM2fOTFuam6wnJY+6rnPXXXexdu3aROOV5DHPmTOnT4PATAuCO9LVsXYnOG3vOZ966imuuOKKxP5nn32Wo48+mhUrVmAymdr9weKZZ55JOy/z+eef5+233yYQCPD222/z97//naKioh7/6NHVOZKpr9XpdCaC6lQ2m43c3FwqKyuhbi/bHr0ucdv7l13GPffcw96GyQw1twZjI4om8h+XP8j9D17U5u/OHDUGYOZoa4nvDTOKeHB9JY1J46+vr+e5554jFAphBrKBxv0v0YiKWfOQ6zqZ2uYKhg9vLe9NXg80njFctWpVm2CyoxLc+HIiycrLy1m6dCnz58/HbDa3+9g4TdMoKChg6tSplJWVJbr65ufnMzT7DFrsjsR99yshtnq8sXNw5yKO+N4vcI+OZVxtBcMpOu/nXXoPTJ7gZfPr1YnS1MkTClBVd7s/SMTLVP9z9xM8dMzFNFpcuNUI108fB3RxiZpuBGxObyV+1wjDNuQBsc/k04+eSMTefiMof1QncuuPDaWw6YLA6F8faL+8tL3S026UpPZGMCcBoRBC9C4JPEW/6ErGsri4mOXLlxuCz3hmcs6cOW2Cx6lTp/Lcc88lloVoL8DpbmOceJCTHHQmj3nt2rU8++yzfRoEHqp5qp0Ff/Hb33nnHU455ZR2g8Pk53E6nZ2OtbuBdLrXv3btWq666irDfl3X2bp1K3PnzmXlypXtNuvZuHEj8+fPNxzv+VdepWrSeYw/6QpC3gY+f+5uNm7cyBVXXIHVau3RUjKp2ak/rq/k9PDWDs93/Lwkv/esVivjxo1jypQpTJ55Fv/0FpC3t5Zgc2ycwbq9iTGpqorT7EgdCjlDxwCx9/D69etpbm7G5/NhiwYN8xtt0SBPP/00drudadOmcWLDO4nGPfZICWUrVqTNSIJOKFLH3oY1QNuAOVX8OZPnRMaDy9T5lPn5+YnlRJJVVFRQU1PD3r17DYFuskAgtlTJmDFjKCkpwe12s+BHP2PS+O/hsnto8TXwxuv/4KJzzyZ53cwRTguXl8RKSIuLi/nsMzclam5iHuumoSMIFuTw4xe2keW0tvujgn3JA0xPzj5WHoV+873t/90dCBqHBur4/eZHWx+3N5a1bC8LbdCNgG3qhw/z/tFXETS7sYSamfzxE8Djidft/Go/jfb8xP1bwi20aJZEJnZX0yewr9LQ0CddAKd3kJFsL1vZkyymEEKIzCGBp+gXHZW7xoOWtWvXMnHiRD7++GOCwWCbzORFF12UWEfvkksuIRqNct1113UpGOtOyWM8yGkv2+Ryufq8WdHBlBq3p7PgL/X2RYsWMX78eCZPnszpp5/ebuOndJm61LF2N5BO9/p1XefVV18FwJJTeKDkNNYgZ+fSe1m5cmUi2/3mm28aG+6EQm2O939VNpwjRwGxjNaYi3/Ftkevo7m5mfGTTyF84gWYnB4ivkYKdr7WpfUKU7NT27/YzfJ7F3ZYbp66jqeqqvzwhz/k1ltvRVVVblq1i32Nfqz5w/Fk53LalVejNO4F4ITjT2bdmibstAazoXAje+vXEoo0cPPNvyQcDmEymRIlrsnH0XWdSDhEQ0MDDQ0NvPLKK4kgs6GhgaeffrrT19yVoBNiQebKlSsZMmQINpsNv9/P+++/zwMPPMCrr75q6FKbrqkQtH6uvPzyy8ybNw+Hw0EoFGLEiBHouo7X62X8+PF8+9vfTpzjsrIyJo3/HmNGnZB4nihR/M37gdYGSGa19f0y56TJ1Nbp5CSVnJ7oGMVL0XrwRdnj8ydKXpMbpM07+kimJQedAM0N3HD5JVzir+Yqk0r9U+9xw/OL+ePTS2JjbK/hzYGs5c+OMvHQa3tpxIKbID87alzb+373MnjkLgiFwGyG785Pe/50XeejYJCXIvtpUgO4I14swSAn6zqqqlJaWsonP70G9YRrEoFp7ifP839zFtIUiODa+wXXb0l6T3SQWe0om9jebZKBFEKIgU0CT9EnUjNpJSUlactdZ8+ebQhabDYb48ePZ8qUKZx++umJL/fJ97FarWiadlAlkB1JF+TEjR07lqampnaPG41GqaioYObMmQCsW7euV0pxezpPNVnqNdF1vU3wt27dOlasWME555zTJggKBoNs3bqVrVu3smzZMmbMmMGiRYva3C9dwA6wadMm9ANfaLsbSHe2zMiYi39lKIMcdeHNiedatGgRl1xyCRUVFR0ez+LKwZ803c3szMZms7F582b0M3+Ma+QxidtGTJnSpeuZmp1qrt3babl58mv0eDycc845OJ1Oli9fTklJCSFflHkHMm/1DavQTSHIjc2z+8cLf0dTHESizUmjsAKtz2kypf9vIN11S11q5WAoaESx4vGYsdvtmM1m3njjjTbv6Y0bN3Z5qZXkMvolS5YkPlfuvPPOdq9PRUUF44dfbNg3fNhRDH/vYRrHfgefJZuwxU3z3mZeXublzJJh2P/nISyjjNk2u2JintKaAd3a1EBk7242XnUeE1u85AdDHL1vC5iMGcm6UIRL/NV8IzspK12/L/E+SGT5vtxpXNrjQNZy6LN/4vfJwezeo9p0jK3SrTx0zJU0mZ24Q16uf+kFio46rs25KC8v5y8jv4PZMxqAPRRwx6jv8JMDY1FVlYm/u4O6P/6OSP1+tOxccn55OzOGFgEQ+f1/QaCuzRiFEEIIkMBT9IGO5uOtXr3aUO6aGrT4/X527tzJz372s8SX8bKysrTZsYMpgexIcXExzz77LCbPEEMW7fPn7iYQCOByudIed9OmTTz//POEQqFEg5poNNorpbilpaWccmYp1WPPwuTIItzSSMHONV3KukH6a5KXl9cmkAuFQtx+++3MnTu3wwA8GAyyYcOGdhs/QZos5N9/z913382tt97a7UC6pKSEcePGsXXr1rS3m53GjJjFlU3xlNhzqarKpZdempi72d7xhuVm0VjbWuat+xoZO3Ysn376KRPmGtfC3FPX1GGZcvy20Mb3GHrsPFR7FnV7v+aDJXcZniddufmLr1Yw7vzrONnZRBY+TAcajlZVVbF48WKOxE1Q12mJNtM2u6inBJ2QHHR2V1SPxtYb6TYVFRtRgqiqFU21k+c5kdJ5R+LKis3FvPnmm9v98WHOyVO6tIxGahn9KVNOYqO3gP986i3cwWYW7HuL3aO+SzDnCKwOM1OmOph94gmEqgMkFwtbLDbqx5/HzHduZ8NJv6HeMQRMdtBh6ZIPuLy5kUA0gjPpMVZFxa3E/kv1KGALOqh74HeMU8LgtDLGaUVP0yDqrxE33zMZPweyTRr/OPA+iGf52m10k2b+ZuryJY+ddD0TsyYlguLHLE7uTHOVKioqiAz7FsmzY8NWNxUVaxLvSW1oEfn3PZHm0VIKK4QQomMSeIpDLl0Z5Ztvvslll13GpZdeyj333JP4gt6VzFd79/F6vd1uGtSZ+JqPHo+HISlZtGOu/ysfPXgVf/vb33C5XInXZrPZGDt2LB999FGiE6k5e6gh6Hp7+f2JTGJnx28vmAlMORdXTmsTkEBKJ8+Our6ma5S0d+/etGOorq7ucH5k4viBQLuNnxRFaZuFvOiXPP5f17Nt2zaeeuopTv1mLJDW7FlEfI1o7yzljTfeIBqNtgnirrjiCj799NN2z1vqWpV2LWp4H3SlwdQNM4v44/pK9uxvJNRUx8XHuthUN5mtW7e2ef5QUx3z589PrLeZ3NRIVVXmz5/Pli1bKC0tZUTl64kOstOvvpxoNEooFMLn8+H3+xk5ciRPPPFEorHOgisuJko1Gm2DllipeT29l4dsXyw7bQPSryfaEas5j6LcUsM+lf3su+lSdJsZ59BhzD7xG2x8+UX+cMwR5JpN7A+FufHjSoqLi9OuA9leyaXPGyXLchqzT5tBZUMzlVYvzejsseay0XU0WWY3+KDFF2HzWy2csXMzLZVrWDvtXqJa65zMoBbLQAbNxmUtohETdWGdT5u2McpzbGJuozuqY9Fam+4MCfsJN9QZHtsmZrfaOO6bs2hZ9KFhd4sebfOjS7tlpl3oGDveNYHcpLJgk/3IRLVBsuLiYt77wFgeq/sau/zjnZTCCiGE6IgEnuKQSxcohsNhKioqePvttw3Zv65kvjq6z5w5c7rcNKgzuq7zg6sXUj32LIZf8zCWnKGG2002R2Len9fr5Yc//CFer5fi4mKefvppw/IXqUHXhJ8+yl1//HWifC35mPFgcebMmTz//PNpmyWVl5fji1ixJo3HpyuJ8rzOur6ma5QUny+bKhQKUVFRwT333MP06dPTPhZiHULj5zw5iNR9jahbVmM5wjj3zOzMRtd1Nm7cyOrVqxlx/o20JGUYvdELWfKXhSxfvpxp06bx/e9/n3Xr1iWaFQWD7QdAnz93d2JZEYcW5S+Xn5Y4z/HOsq4Lbuesc5pwb32ZWdNOZPbs2W0C9ftTlqVwqyGWL19O7St/4rQLLsaqQVBXiAS8LD4QdMavY7yp0cKFC9myZQuXXHKJoZsrxAJyRVGwWq2J21JfV2zU7S+l0lcadAu5ZhfRyH7D/masRBUNR9SHhoJFyybHdRL1LZuJEiDbZSbfOxyfHga19b8cizfAqIgPvD74rJHTgd+fMYPlx16YKAm9Z8hSTjhmMmsrh1EzahgmxUw4GiI/UM2pzRGcrraNdDatq6e+Mbbfozk5Qzfzih4bs6oam/0EAzo0N+Hw1+Bp+oL67NY1gBV/LACzhJpJXikt0FLHkxE3//nxEh6aeAGNB8Zaah6KN2tM4n72aAsmTw7UtUaFiqIQ1qOomopqscJPf0PphEl89o/HSb7GDqejyz+YdaVjrBXjZ6AWVdLOny4tLWXx/y6kOgqaI/a3m7/zNebc/HCXxiKEEEJ0RIl2tDjcIFBZWdnl++bn5xvWWxO9o6ysjIULF7abKbPZbDz88MNpA6Z4Jiq5LLUr9+kNL7/8Mn/6KIpz+IR276OHQ3i/3s7XKx5nyuW/wuLKIdhcx4dP3IrP50tkOS3ZBahmY9Dh/WIrN0y2J778hcNh5s6dy8cff5zIRkSjUUMAGz9Xa9eu5V9ZpySCWYCmXVs5ufEd7rvvvrTn3Gazcfnll/Pkk092a65e6vW5++67efzxx9sEn5MmTWLlypWoqsqNKz9ne23SsTUFf8T4UdO0ayvbHr0WgMsuu4yG6VdTmTT/MRIKEKqvJuRt4Iu//55IYw2hUKjdZUUsFkuboM1qtfLII48YvmDftHIX25IC3CGaj5xNz/H++++zY8eOxLzheJDf2NjI6tWr8fl8mM1mdu3ahc1mM8yN1HWd+vp6gsEgqqqSm5trGGfqOqGZxo8FExFCipmIasatK6jRABHC+BUzftXKVtdx2EIhTgruQNEDoFpZ73UyzONmp/MIAC7Q8vEorefFEajmjHdupQo7r0+9C0/Ski4NoSbW+XbF5hx+9Bwuk879x/6I0c4JiZLQL7zbmG3Opt42os2Yc7IizCzNa7N/1eLPCZpbmwI1RMMsi8Q+1y+IOg1jyPbuYvqmeyDgp8WWz/vH/pig2U3Y14D7w8VEplyOz+IhbHFjCTVhDTRg2vAAdRfN57Stm3g/q4Sg2YUl1MzExgq2DSkhqFixRP1MObMAu1UneOuPsSSlOvfoCkVP/K/h/RC59Uewb0/rnYYMQ7v7v7txBY2Sy3K/qK1j1YTrGH5E65zkz77awp6aV7j33nvbPlbXe+3Hu0wm/99nHrkmmUmuS2bK9OtSVFSUdr9kPMUhl1rWmMrv93P//fcnSiqT52idffbZzJgxA8CwPue9Dz/O7au2U9cSIMdh5fa5RyW+6HdnPc3KBj+/K/+YupYgOQ4Lt889mkKXhfLycm677TaKFv5Ph69NNZlxj57E+Ct/j9/miDWjcQ6l8NyfAYohMEylOTyJuVO6rjN37lzDnMV0wVXyOqb/++s7GX7+LzA7sxNzTk1F+bz88ss88MAD7a5X2Z2g02QyMXXq1ET2RVVVbr31Vj7//PNEIxhVVRNlpRAL2D/6woUpZ1jrsUNhUFuzU1Fd5+uyxxPP6XQ60S0aJM2008xWtILhZOfkctoV/4HSuJeWlhZefvllFEXhnHPOweFw4A+G2eI+jpAjj9CB5UTciokrL7mP3NyhRLwufvubuzl16mRKSkrY/uVucLYGLF9W1bLiwBxciDXw+c53vkN+fj4PPfRQm6AxXladLB5sptvf16LEZnq2KA4s0SBWdNrrLKtgoiDrTGy2QiJEWRnZzz7C2MJ+nl33W35yyo3scRQk7m8N1PBSUxb5hWP4pjmfs7OhILCf58Ih6kxm/FEdT1KQZfHVQ8DPQ1OupFLxcYZuxa6oBPQw/1R8NDsK2EMBDx1zMT/+4G+MdxxpKAk1O8YSDHvTjj1QU098fclk1YqZ5Bm+AT3MsJZq3CEv03euYveEiwlasrAEm5i85TEI+NEBh7+G6e+2zrldM/lXBJMyoNbmr5j+7l3sMSmcOmcO65VTE5nVFmD7yCM5LU0g7Jh4LOHtWxLbw8ZNaPu+cGUZA0+XcQ5xdyWXvH5cVsaTv72RSy+4C5czh2ZvHUuW/4bb7/hV+sd2o+O3EEII0R2S8UyS6b8eDATtBX6xAO8T9jU0463dx85n7yJYZ5xTmJxlin8xy8/PZ9++fW262J5ww3+jJ81vHKr5yO4ga9XeWpPfe/Q1w/NE9n5G9St/xj3rGjRHFta8IpQ0j41GdRRFbXfbX/01gGEeYFTXDc/V8uVHXH+Cjblz51JWVsa1t9zGqIt+aWhelHqOzGYzjz76KLNnz2bGjBl8/fXXhtvjQVI4HG7TzOfLZfcRqN3TbpfZuOTHRVoayd+5hmefeMRwDnNzc1m8eLEhKxIPnj/66COOWvioIVMcCfjQrHbDcZp2bWHbo9fFjmmxMPWsuYy44Eb21DZSH4wmMsQn1r9NdqR1HcdAIMD+/fsZNqw1sK3XPLyXfWrieb+t5TF2zJTE7Ts/38yfH/8PjjvuOKacUYrdpGKOBmJz7qJRIqFgbA1Lmw1N09rt9DoQWEz5FOWWEolGcLTsw+ccyp7aFYQixvmGJiWLEUPONewLRnUWRfZR4Kvlv9++j19O+U+2H+hwCjDBuxuTYuEk9yQsSe/3Gj1ABAW7omJDRQXMehi3dzff+OBhbjz+h4YA1qNHKTblJTKbHzbv5DvZX1AdnYUj2nruW5QwBb59eG1tfznN9u7itCsnt9l//dPvcYxlBHZFJRjVseoRPIqGQhRncyUnfvgwDn/K57yqgW4sNV857Q/oztbyemvLPs7a8As4ciL+n97LGyubSP5TUhTIztWYMtVhKAHODgep/cOvDaWwqY2R0jUOStc8qSf6qkJkoJH/7zOPXJPMJNclM2X6dZGMpzjkOloH8qG3qtgXsYHLhtOVz9gf/Jqdz97dpkts8nISuq7zwgsv8Nhjj7Fhw4ZEpi4QCNASUbAlHfvLvbWUJWWt4vdLtxZkPDh+5plnaJl2leF5QqoF51lX4xx1DG5UzlCzE1+O39EbOVlx4VI1WiJB/kmsaQlgCDoBIoEWrPnGBeyd9fs4wz0au9mGL+znnd2bmHPzHUBsHuyoi35pmAc69rLb0MMhw/nJt2vMnj2byy+/nN27d6e9BvHAMnVe6bhLfs2HDy3o9DomPw6gOhpl9uzZXHfddZx99tmJ4DY5K5KasU39OctfW4mtYDhaUrmx2Zmd+LfdbmdUvpuRO1dwUn4+L9Q48eWNjd0v2rZ8Nj8/37DPE2ngm7WracLOu84jcGmxUstQuJHqxvWojkYWLFiA2WxGVYPGKZMKaBYLFotx/l+/i59EJZ5CVEg319OseYhGFSCCptkoyJqJoiiYFBNBVxEaMDT7DPbWv0HoQABvUrMoyD6DaDSKorSmKLUD7W9ygrHmNNd/9BwPHXNxYh7j9Tte4NUTf2cIOgGyFTOmlH1RzUJj1hg2Hb8Qd6SapHwep2s5DEnKbCr24ZzxvVN549VmvPWt0VyBx8LrVitjWoLkKmYUQCeKq3k3k+teBtoGnua8LNY21HOGmk2eYsJkshA9cOaaPGP41/E/5fR3bkt5kBlS1lits7oMmdNqiwOOnIhy1c/Z/HYLqb/fRKNQVxtrWDRzVms5r6mwCK2ThjuHsilPaqffwVw+K4QQIrNJ4Cl6TbrutRs3bmTFihXsaRwJtH6xHzZmHNYr78A0NBZc2AqGJxr1rNm4iTfUY9j+xW7UhmZOP/EnzDrtVsJON2/QRFM0QjTlW1+wuTUjlpd7BFdecl+irOztjRuZO3duoqy2sqYeq99MSel9OCwOAkR5Xa+nGZ2QtyGxFMcZajZDk74cz45mY9VifzIuk5kzIhqvROsTx02ek6hoZky21nX5onqEK448gWDzgS/2Jhvnnn4Nt9xyC8XFxcycOZN/f2bMCNqHjkpk/eLnZ+Snr/DKK6+wdu1aOitWyMsqoERtXVfwX0eM5ROzGcWV1ybgT86spi5F4hg+Ae+ZC/jZrbfz3HPPcfHFF/Puu+9yyimnJDLaZWVlbN26NZEtdQwbY3gOzWzFV7kT16jYPDNbuIVvmL7ijF/+jqBqhqCPHC1MOBymqqqK00wm3qxsYpKlDocSIlVqRjIeOmXh40x1By3RHXy+17hOZ2pjn74QxdjJNIKCDytO/J2uSpLr9/Jv9zc4MXsKTs1KKNxEdeM6IhF/IsA0m2IBTmoAmcpscuPOK2VZpIZz1Vzy1fRBdoQoExp2cf3Hf6fFls/24xZyuquIKArulj2Yso7Ao9nbPE7r4NU0OY9gwZQC3nxfwxJViehBcsMtRJI6wI7wuFFVlVNnONn8VgvBgI7FqjJlqoNl/6xmFKAdeH0qCjYzOK+4Ou3xbjhjFK+VN5Clm9PeXu1szZRjMsPII+G78+HvT0LVV7H9RSN4z9rCMSFzomvtVmeIS275AwDBQEOaZ45p8aVv0tWfpHxWCCFEJpDAU/Sa9pY5uf322/GcdwvOka3NLYblZEF0BI1JsZNz+EQm/uef2e7KQqv1gyuPuVnjE8EfwDd1e6JDZSTgQzOZiOg6imZmxPFn8sOL7mKILSuRfRnKaPZUZxEMBvn50+s5q+A4bPkjsKMasjZn4OH/AlUomplIINbD0paSwbGoxg6a9pS4L1D1GZ/85Vp0Xee4mxcbM6aqToSOzFAAACAASURBVHWdH4+59Ut7nd/E+8O+xXsfNJK/Yw32Kd81zMRLDSvNzmxee+01Xn/9daLRKIVjJnPNVX+JZVBDfh5/YgFVuz5I3H+We6QhcJ4ezeO9/HyyvnsL7tGTYuMbmo3l5pfZv3s7z7z8Bzxz/4PcnELOSgpYXzfVo46exFHf/zmjmz9k+/btZGVlsWHDBpYvX865557LL37xCyw5hRxz/RO4TDCpeRPmaJCQYmGr6zjMviqm2/ah1baWBsdDFWfET0QzBi6RcJip5ipUJX1WpqMgq0fLTB4CUeCU3Tv4Kjsfn8mMLRzif4eV4jO725QPa5Ew7mALTVYnUWJB5weu49nuGc1ERcFJLHhMXZIEOg864/zR2LsrWzEGZNFoFJ1Y0LmlYSsLPvgbQyM+Xp16LyFXa6lMo3sU746/Bi3lmoT1MJrStrts4vYorH1jH/m5Rx54sVaiis94nwB4D3SpTc4WAritGrYW4zFD+SNRC4w/kMQVuiwMc1jwNqf/YcasWPjaUcjwQo+xpPV3Dxnvt3IXr9S2dvCdaGsNlC1WFW9z+gCzOtAXi9t0T3fnvgshhBCHggSeotekW+bEZDJRXV1NzZK7Estb6L5GvpHlYlPuBBqTuovGG/U4w2G+eSDwcaYsA2A/8KXXjcrptmHYVVMsOBph54xL/x+FJhupbJ5hzJw5k6t++g9DEJtsqGLhe9ZhrBlhJxoO8i01lyyMX6ZTv9w7FRV7bRVed04sSFRNHH3idD799ztoTk+bjKmupGRpTabEHNAa4MoREdb4AtSGYvkjVTX+eZrcuZg8BYns5DVX/YUie6xcNcdk4+qrH+O+//e9RDbTbjK+1khdDXv27CHfE5trlxifxULOmClc86P/YoXWwllqrmHcZ5DNK/p+TnY2k+tqnXc2dOhQ8ocOZfsXu/nPhdfGag2962JjT9zLx6n169AcURSl/Y8bNU0J6UD/YmzRcshWcznls3cT+54b/j0AtrqOY1Lzh1iiQcY07WH2V1vICrX+LUSj8K+ps5in5ibmKppQUNMEmG2CzjYlurHg8N8tu7CZLKAZ87ARovwjtJcztRymWAr58qSbKfj3I/ichW3+g4ioxqBVjQTwRnU8HcyJtTR+SSRifNy+hlry8wqJ/xcUidCmRDXuhhlFvLayEZLiPIu14/dGamCYHJybVY3XJv6IoaOqKM0bkvbxuq7z1fI/4D3yLDRHbF3Zr15/DX32w6hqLBO7+a0WAgGdL+pbQNWwHPih5tXa7WS9XJcoS+9vHU2ByITxCSGEOHxI4Cl6TWr3WpvNRm5ubqzBU93e1kYyOYW8eO0fcQ8JQ1THrZgMcylVc5SCdgJEOyoXaPmGjKVHgfOUfPztdO70o7N79+42GcxkiqKQr1g4g2wUC+2WIiYzaRYuKDgeHzrBqI4yYhimi07Fd16Ad6LN5KUEWsnZu4Ae5nW9tUw3xz0EX8Mo5lmcYFaoj4ZYo9ejQOu5cei4fv4Um5+4kZHf+QkOm7G7qsvq5Ns/uZddWbFmScGUwLWxoZoRNgs2Tx7WcAsB7ya+0gOJsk2HxQGRFkwRL7v3ryIYaSDeDfUsNHSlbYZHAzQOZHjaSbqZlA5upMOHZigVBQUFK2azk4KsmTRGI+ytexV7NPaji1nzMDT7DD7JLWXCP6+kyprDQ8dcnJil6Tc5eC/7VIbpKhPzclk3GjQ9xImbHqDq6/fJzS/iWMfoxDxIAL2LfeCy6z/l6H//hXeOX0iT5wj8UZ1Pm7Zxy5anGRqo4/nTHyXL3PreiQKzTHnkqRZw2GiigPeO+2n6V66H0ZPeV87GLwhljzXcJxqNktX4ORGTHUuomdx/PcT2CVcxtGBU4j4NjdU4nQ7sttZOwMFA+r/fQpeFc+fktCnB7YghMPxqFxbXEEO1gR7RWLhwYbsBWHl5OW//s5xA+YuJfZ/bbIn54vHMbFlZGc9+GTBUczTV7eWn993A3//+dxYtWtThOPtCe1Mg0q3jKYQQQhxKEniKXpOuiUUkEuG6667D5cxPzLskKwfNYsfiV7GZVEMmx6NAOKW7ZDgaxUskEWxaaBtAWhQVU9QYvkSjUUJEeSscK2ms2/s5OSOONdyemi2yK2q3gqDEeJIelG21UBp1tmm2ksxHNNGYyI3Kd/OP/v/t3Xl8VPW9//HXOWcmM9kXQhIWcUGlCi5AW9lBsSxRb11r1Ur9WX629Fdv1Vu19talD+va9toW722r1V5aF9rrdbmaolwBSQWXuiFKRVEQSEwCCdmTmTlzzu+PZCazJYCazADv518wOefMd+Zzzpz5zPf7/XzJihmuWGpk8TVjOAZ9PVqFBlycfRhnXfkngkCW3UFt23pCoQ5cgpimj2NMP632cJo8ftY4zcyhkDzXJb9lJ3997Mf8aM503ml+lTy3LdqBZDtt7Gx8AoC5QLuRh+O2J7Q4nOJdP7QYeCgrPI0cf3K10WLXJWv42eThiTunXNfl44Kx3HDiN+n2+JMKVpVYnuh5EjY9vDbpBxxbu4jfTPgmXzLj56QOdF6GnDCFXQ1khdox1/8bJ636G+NG/xP5Bb0JZv5Ifnn8Rdzx5n/wWvdOZniOjf5w4zVMionvkezIP4xmJ0hpzDzMkBtm+kSXDdUfEMoqoNGbzUOhbi5OOjNcquwG2oxc8sMdnB4Ksmz5j/jm12+PW87jpusfiVvlZaBezNw8i2mn5bJixQqe+99qWgIDDxeNHbLrOCdQ9d87welLPNs79gyYgPU3baC6ujpu2+rqaj6s+t/oaI7InGnbtqPHXrRoUb+vayjs62sREREZbEo8DyD7M09nKOb0pFoDc2SBP6na6fLly/nSCYvjlrcYSOIcMhPIxWRvKaFpGDiui4OLxzAxDIMsDKZ4CnkOeGDZ1Vx+yd0cfviJeEwr5by47L0+y77x7uUoIbuNr7S9Dk4Aj+nHKJwJnvhhhqZhRKuyxhaUKezdrrZ5FYHQruj2YccGp4NTOjbxj8IpTHUtmhqfY5fTSj0usy/7Fu+4Bnlu24BtS046Dz1eqxDTzEpZyCcV0zDIJ7mYjWEYvPmlH1PphikwPFgYcT8kJBaIClk+coeN4LiC4/AknJ+J52vs3MyN7R9wzbv3srmmju+98WHPa0goEtXqzQXAE2ymCyfuB5zEszVseCjCJOiG6cal27F56rEb8RecQ3hUmF9u3NFTKKqkDAfiUs+w60aXYPmE4bxz/EWU1v2GX9/3rbjlPE6dP4K3Xu3ep15Mx3G45Ior2TV2LlbvvOiHnriSh+9bmvJz7ZO2IPesr6UtECbfZ/Hd2eWse/pDujpt2jv28ODD1wH9J2Cppg34/X5mzZqVcrvIaI5YkWOnO/Hc19ciIiIy2JR4ZqiOtjBvvtI3tOzkL/v5znf/T9I8nZ/c9Vte/Fs7lmsSdGze/2A50ycfw/Lly3nppZcGnNMT+XLW2h0m3NVK3jtPc/rUyf0mqXFf5rIsNm/9GArKwZdNV9jkmScbKPN56bC8vOZpw5ttcM30kZxx3uVkB07c59ee+AXbNIyU6WCqHkvTMDASRiQWmBb3nHsJLVOvIduw+i1YAz09mANVi93XQi572ybY9jKO3djzb6e9t8fRjA7RjCQ5u1rXRZPLSM+kSQ5eby7BUOoEMmjvZmzjMzTEPGYC+c7Bm1CaRjZeTx7DC2ZQ37w6umxILJ93OCNLFhIMtbK7LT6ZB/qtGPtZZRkWJf0U30k8Twzglem/oHCAYj2x+/YMdTY42iok79/+yG8WL6bO/gCAUEdL3Dqy+XZP0ayrNz3Kqinjwdv3+mzXwRvznD3LsfT8f8fWN/nZ0m8AUH3sMFzXxTPiK9Ftm90QpUbfkOAmuzsuE3WyC5g0aRJXX3110nIeM05PXXk20YoVK9g1dm7ckNZd0O9w0XvW1bI5Mn+8LcR/vFnP7IqPufHKK/cpAUs1bWDq1KksWLAg5XYvvvgioVB89eVMSe729bWIiIgMNsPd25oMB7ja2tp93jaTFmN98fk29jTGDDk1W/nJnRfzjQtujRuudsU/P0KFryC6WU1XM3feMgfbtuMSKL/fz69+9SsMw4h++Xs9fALjO/OwMAjjsrJhE5MKR5PrzcbIspg+K5cjyvqGp91Y9TGHt2ZHhwmutvfQYfY8x1kxBWmg54tso2vzYW4nR7d5KfPkDuK7tReug9NPYZZPK1VPZGKiMtA2O3Y9ge303/NomXmYhpdQeM/n1uaDg4HPWzpgYhhZdsQOd+I4AQyyonMxP69kMtN0hlp5Y+O/s3DhQn7729/yt7/9jazi8rghoLPbN/DN1g8INTXS4C1hw4TvkJVdRHvHHp545hecc+a/cMQRJyWNOKhv2MbNd56B3+9n6dKluK7LPW/3zWvMw2Q2BfiBrlA3K3Zvxh3RN5+zc/smrj7Jn5Qg9jdiIpXrr7+et0Z8JS6R7t61k4l1z3PnnXfGbes4Duf+/iXc3GHRx4yORh67/BQuu+yypASsvyI7juP0u/Zl4o9wJwfe456f3khDQwO2bccdu6ysLO33lYFey6Eqk+730kMxyUyKS2bK9LiMHDky5eNKPGNkUhBXPtVMoK/IJV3de6hrauDIkeOij22t3UxR2eEUx1RydV2X7Ts3cd+yq2hsin/tI0eOpLGxkUAggM/n4+7bXsUXUygksTev1bK56NwSVqxYwdq1aznmxO9T4e1LIJucEHmGhYWBSepevl1OAL9hkp9iCYd96TnMVLVNK+KGuUZ60wbapqcUT+zllrqYikSYJL5Hqd5ngTq7i1uu+yJ+v5+xY8eyZcuWpHl948ePJxAIsGXLln6Pc9Nt66KVkiM+3PomS+9fHE2kgJ5hr70VXz35JXFr1nbsfB/XDuHJLcTpaqX0w1VJQ2Idx+G8/1iFU3xY9DGzaQf//f/mpkyIqqqq4pJd6D+hTbVtV80HHD7mMKzsfMJdbeS/2zO649MmYNc9u62vRxUYV+rnzq+MSZncZdJ9RfooLplHMclMiktmyvS49Jd4aqhtBrJtm9bWLnxZfb2NhuGnuOLIuO2KK44kGOyCmMTTMAwOP2w8P/7BE7S27o7OZ2ppbSDXP4prb34Cr8dHyA4kFb9JWi4kbPHoHz6gsTGPZ1esYcIJ8fOYig3PXpPHYsPL7kAb+f74+WYHWtIZ23tpGh6CCT2RgdAuttU/gmn68Fg5+D2HJSSdELcehAAwLG8Gje3r6UswDbxWUXS4caT3MnFY7P76rD90fNr9P68fWBzXYVewA7/lJdv0EjQgC4NuHLp6Rx9Az7zCLVu2UFZWxo4dO+KOsWnTJk444YQBn+e++5fwnSt+R2lWHuBSW7eFVS/+hqVLl8YlaQ/ft5Rnn32Wu+66Ff85N8YlnpYvh5K3HuDwww/vScCuT56HuWLFCrrCPmLLJ3U5Rr9DZxcuXMhDT1zJLhesnIJoQrvg+qVJ26Yq+GN6vDSE/dAeAvyUzLiUyvlHDPhe9MdxHN7fXgMxParvf1wDjImb3y4iIiLxlHimyUDFf26//XbGHrMk7ktZ0OshkND7k2N4yA91gj95IfVsfx7Z/jzKOYLLL7mbe35zGd9bfC++3iU4PB7vgHMZoWfuWEFeOQV55XzvB/9FR1MNxRXHRv++L1+oLQwqLN+g9nDu77Ej20eSyVRDMkN2O/Utq+lLFiMLYQxwXGzCjk3Y6UiRdB68DLJx6drPvSzKC+eS7SunIPtwMFPPafR68lP2cLqui42Lh/4L8Di9xXea3SAFhoesmHVZXdft2bOf88aOXhtuz/InOPhcM+Vamo7rEsbFG/NDTmSouQcYFjP/MeiG8fTOWB7onI1cm5HiQSsC9Xy1ZAvr1q3jo/pmuk69EtPqez223df7FhnRkOqYGzduxOPxYNt2yuet2/Y2t//snOhasE53K9/69mIqK+fFbWeaJpWVlb3Dblvj/hbuauXSSy8dMAGrrq4mWPBlfKV9Q2eD7S1UV/895X6maUaT3erq5/tNaCF1wZ8TrlsWt01r4NP/CLRixQraGwPkxiSeHY31Wp5ERERkL5R4poHjOCz59lWMP+Y8jhl9ER++28J3nryK3/7ul5imyapVqxh5zLcpiNnHb1hAT9XWyBffLMMknJd6AfRYRxxxIsefeCpeX3zVyP6++NpuT4Ib2yOa68uDYZ5Pl0B6/dH0wOn9wr+/ieJA7XVJrsrZGWphV+uLGE4Qy/RRVjgzbn5fMNRCbVMVLnbMcWwCoU5qG1fi0JHiWQ5dPu9winIm0tT+SkzhHpPywtPJ8VdEeyaDoTZcepKgyPIj2b7ylLFzXJdngg3MMnMoNAeeexnuTe4iPXxrnObocjSecAjb8pLXu1RJtmH2rKtKz1IhPieEZcUP9Q44QQoCLQRzUl8/TU6A/3H2cLY1jFIzC29vtZxGJ0ghnrhro40wLlAUU1GnA4ennaaeNtHTpm7X4entf8cuHcEofwFnWMPiktig6/QeoW8d18hrdC0Xy7K48847qaqq4rq7r+eIRT/B9GTh2CE+eOCG6HH8fj+nn346O3bsSBpu67oupmkyb948Nm7cSH19PY4T/4PWURf9iLwjxkf//0aWn6/3E5f58+fz49vn4VR+N9q7uOev/868a1f2s0ePWbNm8eSNtzLqvB9E96t9/OfMuvXGfveJJLt7S+5SFdPJtty4n+3yfXsv3tSf/pZQqW6Zp8RTRERkAEo802DFihWM/8LXOXJ03xwkw/Ly7LPPctzkU7n0/z5EtmERdB26cfAPsH6luw+Lf1iGxfkX3YkNZA2wnd27FEnQDZNjxJ8aeViYMYvO76ukJSAAG5esAdptuw4mRjShHKgokNvbZty+5NPGpbF1XbRqbNjpSYqGF0yP9nCGncTEsk9y0nnoMPDg9x5BV6hvHmBh9mRKCnoSkWzfP9GJgw8zbrmP/nom92Zh1nAgudfadXt6Gh16EoZUCWeidhyecZoY4Zgs9A7vO288yeetz8wikD086Sx0XZdWu51T3vk93vHfoihhbnKh4aXRDVEe04vZ0d2OYXkoyup7nnwsyvDQgM0zThPhUIDOmi1sffQ2xl3xC76cMzruvHZcl3bXpqS3QFepkcUcinjGaeptGKxdu5bKykoWLlzII488wrpbzyMUCsW9b5GiNjfccAObN29m7dq1SYllMBikvLycBx54gEWLFsUlaFOmTME88hhaY35n+WhnPVVV76asdv3cc8/RXLuVhpjeRZ/Px8qVKwdMwhYuXMijjz7K+geu+9wrraZaT/jkGTP51Ut1tPYur3LN9NRzT/ZFqh7VTKlgKyIiksmUeKZBdXU14yf8c9xjhWWH89BD91LZ+WXKY5Y56CkaO0DiRU/CaDFwL6Lf9NDu2hT1rjCZelu3J8FNsdxIYvIXO0x3f3ovTehJKnv3dxOO7bgue5wgq9xW2nH4p5BNZ2s1LmEMLMoKT+vtYWuloXktwd7eN69VSEH28TS1vxLXixkRCDXS0FxNMNy0z209mA3Lm4HHyqGhZXXSe9tjWsr9DMMgyzVS/ASy//pbJifyPAZE/56FSYEBcyhirdPMHLMIv2EScGxWNb+LPWxUdN+veIfttYLx3s7ZmrHnUZxVkOIvLmucZuZQiC8cZk/dRzyw7GqO/MYtXH74tOjzmobBAquY++rejPaIBffUAxBsa8JflrBWrWFQlLAOaHbMddhVv43c3PjK0JFryDRNxo0bx+TJkznjjDOYPn16NPm67bbbuO++++KSz0iSlCpBmzdvHhf8dg3EFP1pqtvJlbddm3JJpurqaoLBYFy7AoFAyrUx415viuf+PCutpuodvetTzulMpOVJREREPh0lnmkwa9Ysapvih27merMJTfoGbtAGqy8siQWAYrlu/LyygWRj4jUHDre1D72nEZ92vmbifmHXJZyw7EhRzkSmdbxO2AnQEbP2pItNfctKPG35hJ2uuAQzFN5DY/u6AZ7ZIRhu/lRtPhgYeLDM7KRlXY7wX7zfx0o854ayQvFww8s5VmnfjyOWhynhPJ76+F08OYWEOlrwjj1tv44ZO5TbMAwKvfl0WjkpPxz3uDbtODxWv5F3fvZNLMti7ty5rFt+K+71K+K2tTB452ffTDrG1kdvo+tH0ym04p8hMVnu6G6nu3lnNHEdNqFnfvWKFSt46aWXovM0w+EwH330EVdffTXnnHNOtMqdaZr867/+K5s3b+43SUpM0Kqqqtj0YPwQ2K2P3kYwEOCll15KmscY6f3bl7UxE+3r0NlMM9hJs4iIyMFKiWcaLFy4kOt/9wIlxSdHv0BnGSZfHz2FVieUcp9UX+73Vpwk9u+JRVj293iDobP7Expa1sQlkLbT1tsLl7r4SWSbT+dgW77EArxAX2EZg1y8lo+wG9jrGpYH4pI2lmEk/UBSVDqa9354bt8Dv3gn5b6R4buJCV6DG2KY4Y37MExcyzJSLGiN0/PjRaijBa/Xy4wZM7j//vuprKzEdsNYMUPUbddh/Pjx0aVNTNPEcRyCe+p5ZsfrLDp8asrRBQBBO8C//+KCuCWRGht7itlUV1cnzd3s7u6murqaRYsWxT2+v0lSdXU17fU74oaRJj5HbKJ4qPb+HahJs4iISDop8UwD0zS5Yut/82rRCXFfPLMME8sNE3TDZBmfvvgFJCeR6U4yY5cjMQwPBka/w14HSjoPbn3rfFpGHpbpwQ5349Dd+7gLmHitwuhyI59FqqJMKbdzXUK4WBhYn+N5M1DRqNgiWnvT7cb/oNDkhBhmJc9mbnBDrHGaOc0sosTwECnis8Zp5nyrlNh3I4zLbicULQoUmVvq2CE6d75P56rfc++991JZWYlpmpx88sk8UfMW546ehIVBGJcnat5i0qRJXHXVVVRXVzNjxgxs2+b73/8+b//pRh667A4qR56E37DIdg18MT2gO3ZsSlqHd9iwnsRzf3sZ9ydJSnXsgZ5DvX8iIiKyr5R4ponVuhvbCYIZP6/LZxjkdzUSSKi2eaD1TMUK2a3UND6Nq3Us++XzDk/ZKzlYgmGbVmxKrb41YPvrEQT43cfVnD9iMiN88e0LuQ6tTohhVvLyHXtTF2zDAsp8yXMpG90gYdeg1PDGFTGKtt916MKh23Wo+uTtuL89VfsmlaMmkmOY+DGjhYlWddfR1NbIU8U2Rkxi5IQCNBlBymLei6ZwkGdIHprdufN95jib+Nfnq+KSq9mzZ3PNjT/mk4QqrZfcemNS0ldZWcnkyZN59Z7LebX3sWElI7n8krsZPnwkphnmwYevi3tewzD4xje+AQxuL2Pk2OvWrYubuznQc6j3T0RERPaFdcstt9yS7kbsj4ceeojly5ezdetWJk2atNft29r2fVhmTk4OnZ2dn6V5++yNTpsXm7oYmV0R14vka9mOEezAySnd72Om6kHa21Ikn0WqoZrBUAv1LWtobn+HjsA2/N6KnkqyMXM1DxU+73BGFM8nO2sMnYGP6alB7GFY3jQctwsDC6+nkBHF8ynKHY9l9iVvbu8alP0VgoqsY5nYA7lPRZ+cMP+z8iaeW/8MR084lTDQ7No8secDXmt4nxMLRmHG9MS32508cvu5ZOUEGXbYF/EZFg4ujW6IP29/hfdaa5lQODquLeHe9nUSxgA6CNPq2nS6YYLhEHsC7dx//xJ2736XsjGTcS0LA4MOwjS7Nk/XvMXruz/g41wfw00/Tm/95nbXpinQzuN1b/N6ZwMbd31A3TO/ItTZRjgcxuPxUP+P9ewcPYZ37TY20M2bzdvZUL+Z9/7wQ+pXP0Te0ZPxFfX9sBPevZ1tgTYqsosJ49Li2jzX9AFZThu2HSIcdgm27Ca4ewfDP1rNL392R1KP3tFHH82rL65l4/88yCfVj9GxcQ2nTDyRa6+9NikOlmWxbds2Nm7cGH2sq6uN9a8+TsVoh+//y4WsW1/Nzp07cRwHr9fLzJkzuf7666PzUM8++2yOPfZYioqKWLJkCddeey2maX7mz7DIsceNG0dhYSHTpk3juOOOi3sO2T9DeV+Rfae4ZB7FJDMpLpkp0+OSn5+6I8VwY7+pZrjt27fzwgsvsGjRIh577DEmTJjAF77whQH3qa2tHfDvsUpLS6OFOQab4zhccsWVhL9wBgtHnEQ2JgXtnzDt9DJe3LQZ/yclBPJGEjazkpck6Se5CLphXumuZbp/NAY9QykL33+UpmMvxBszdDfV0hUhXDrdMHmGFTeHLuw6ZHfU4bEsAqEAe7JLMK0sOg0IBxoItv6ttyfTwGPlY4dbOZjXvOzEx6a8CRzdtYUcJ4jP7RmECn1zc7OsQnILZ5LjKSDQuxxO4hzbiNg4OL09jmFc/hqoo8G1yff4mWv1DA2NxCWEy7PhJjqcMHM9xXHDRiNrP+ZhMtsowGrejRvspLz0MLC8EA7y8J+uZtH3v8npp5/OFVdcwerVqwmH+3qjJxz7RS6+fClZlo9QOMDDD17JO++/xqxZs3hj89a4wjMf//kOLvz616kvn8Ep3lFke30EIDo0tfOTj8gqLo9bb7Lj43fwer1Mnz6dZcuWcccdd/Cf//UUh1/4w/iCNnvqySouT1ov0WlrZNy4cUyaNClaiXXlypXR4ayPPPII69evJxTqi4nP52Pq1KkAvPaPDxl13g/Iyisi23I5f7TNH/7RRe6YvuWNOrdv4uqT/CxYsGCfh5E6jrPP21ZVVXHllVcmDZddunQplZWV+3WsWEP5GSb7RjHJTIpL5lFMMpPikpkyPS4jR6ZetuyASjxXrlzJiBEjWLNmDZWVlbzzzjucffbZA+6TqYknDPxFNXzHtfDRZj4uGMvLE68h25MDLuS21zD5rX/D09nAZt9RNMz4Ea6VhREOkv+327ju+b+ys7tniJzX62XKlCk0OyNYfM6P8RgmYdfhsfWPcOqU87FCjXS0rsXpHQIbIIs9dTt46onHaWtro6iox49q5wAADjdJREFUiNtvv50zzzwT0zRpbGxk2bJlGIYRbeeBOAQ4esr3LtkR0Q3EDhgNA11mLiYOISOLd/NOoLFuJ67rkl1xBAYGrhOmwJvDqZ7i6HzAVaEmmlob8GTnEu5oJRzswjdsJKbX1/u0Pc+ah8mpRgE+w6LbDVNVu4EN/3kDwT31mKbJzJkz2d0Zxv7SBWTlD8OTV4RhmrhAV93HGAbkjj62r72hAG7YJtzRSrCtiW3Lb+dLxx/Nnj17ogVuIkMmY5fFiD0Pp02bxk9/+lNqamri3jOfz8evf/1rli9fnjTE849//CMACxYs4IPa3UmJYmQZkchxzj//fObMmRM93x3HSVpPcuzYsZx88sm89dZb0bZ7vV6GDx/OzTffHJ1bmUrs68nNzaW9vZ3Zs2dHh4kmXnMAl1xxJbuOmouVU4DT1Urph6t4+L6lg9bDl+o1J8bl08j0G9GhSDHJTIpL5lFMMpPikpkyPS4HReL5+OOPc9JJJ/H0009z4YUXsmrVqui8p4jnn3+e559/HoA777wzaY25gXg8nugSBelm19XS8stb2PneJmpb2vneGx9SE+jpwYn0WI4ZM4Z58+ZRVFREMBjEtm0mT56M3+/nhRde4LTTTuOrX/0qtm1z0UUXsWHDBs4++2xGjRpFIBCgpaUVrzd5mq/rupimyQF0agwo8jpCoRBHHXUU+fn5/PGPf+TFN9+NS5K2/9ddBBo/iVvzMLbHzelqxfvG42z6+4txvWmJvXLB6mW8tW41K1asYPXq1cyZMwfbtrnmmmtoDVv9JmYej4eCggImTZrE5ZdfzjnnnIPjOEybNo2NGzdG2xVJUpoCEJx4LlZOQdKxvF4vp556Kk899RQATz31FKtXr46eEwMlN7ZtM336dDZu3Eg4HMbn8zFnzhyefPLJ6LHWrFnDqaeeGnesVPvNmjWL8ePH89xzz7FgwQJ++tOf4vEkn3OO46RsY3+Pf96G6nkG+zkz6TNMeigmmUlxyTyKSWZSXDJTpsclKyu5yCMcYInns88+y6hRozjhhBPYsmULb7/9Nueee+6A+2Ryj+e+iO29mTFjBgAvvvhitLfmscceo66uLrp9RUUFX/va11Ie6y9/+UvctgervLw82tvbe4rluC67d+9m2rRpScMVbdvmjjvuYNWqVcydO5frr7+e559/nrVr15KXl0dHR0fSez5v3jwuu+yyaE9VpBfurLPOoqOjI9qzliqBSOzlit23s7Oz3yGV/fWMJ/bstbW1kZ+fT0dHx2euLrq3oZ79XSufdoiofD4y8TPsUKeYZCbFJfMoJplJcclMmR6Xg6LHc8uWLaxfvz46x/PYY4/lxBNPHHCfAz3x3Jtly5bR0tKS9HikCIlhGBQXFzNp0iRWrlyZhhZ+doZhRHstLcviq1/9KqNHjwagpaWF5557jq6uLrKzs5k/fz6FhYWD2p7PklwdLInZgXitHAoUl8yjmGQmxSXzKCaZSXHJTJkel4Mi8QR48MEH2bZtGyNGjODb3/72Xr+0H6iJ5/bt26mqqsK2bTweD2eddRZ5eXn89a9/Zc+ePQAUFhbS3NwcNzQ0U3k8HubOncvf//53mpri1++MnWsIPYlmaWkplZWVjB07NmNiIn0y6VqRPopL5lFMMpPiknkUk8ykuGSmTI9Lf4nnAbeO5+WXX57uJgyq5uZmVq5cGTckNhQK8fjjjydtm5jAZYKZM2fy+uuvx5V4njlzJhMnTgRg3Lhx6WqaiIiIiIikyQGXeB4sIglm4hDRxKQz01RUVDB//nyqqqpoamrCcRxM06SkpIQzzjiDwsLCaJIpIiIiIiICSjzTJjbBbGlpYdmyZQwfPpzu7u60titStAaIzg81TZNQKBSXIF988cVpbaeIiIiIiBw4lHimSVdXV9Jju3btwuv1fq7PY5pmtOJpxBe/+EV27tw5pAV5RERERETk0KXEM02ys7NTVqP1+XwAcetE7o/i4mIsy0rqoRQREREREUkXJZ5pMn/+fB555JGkBDMvL4/zzjsv5d8sy6KkpISZM2eyfv16Ojo66O7uxufzkZeXpyRTREREREQykhLPNInMk6yqqoouj1JSUhI3h3Kg9Sm/9rWvpavpIiIiIiIi+0WJZxoNVKSnsLBQyaWIiIiIiBwUzHQ3QERERERERA5uSjxFRERERERkUCnxFBERERERkUGlxFNEREREREQGlRJPERERERERGVRKPEVERERERGRQKfEUERERERGRQaXEU0RERERERAaVEk8REREREREZVEo8RUREREREZFAp8RQREREREZFBpcRTREREREREBpUSTxERERERERlUSjxFRERERERkUCnxFBERERERkUGlxFNEREREREQGleG6rpvuRoiIiIiIiMjBSz2eMX74wx+muwmSQDHJTIpLZlJcMo9ikpkUl8yjmGQmxSUzHahxUeIpIiIiIiIig0qJp4iIiIiIiAwq65Zbbrkl3Y3IJEcddVS6myAJFJPMpLhkJsUl8ygmmUlxyTyKSWZSXDLTgRgXFRcSERERERGRQaWhtiIiIiIiIjKoPOluQKZ46KGH2LRpE0cddRSLFy9Od3MOWbt37+b+++8nEAhQVlbGBRdcwE033UR5eTkA3/3udykrK0tzKw89u3fv5sYbb4yLw8qVK3XNpNHLL7/Ms88+C0BbWxtTp05l1apVulbS6JNPPuHuu+/m6quvZsyYMSnvK7rXDK3YmOTk5MTdX5YsWUJjY2PSZ5uum8GXGJdUMdC1MvRi41JbWxt3jznttNM45ZRTdL0MocTvxEuWLOHhhx8+oO8rmuMJbN++na1bt3LNNdfw0UcfYZompaWl6W7WIck0TaZOncrpp5/Ohx9+SGFhIQBXXXUVc+bMITc3N80tPDS1t7fT1dUVjUNjY6OumTQbPXo0c+bMYc6cOdTX1zNr1iwCgYCulTRxHIe//OUvjB49mjFjxtDa2pp0jXR2duq6GUKJMSkpKYm7v3i9Xvx+f9xnm66bwZcYF4/HkxQDfS8beolxmTBhQtw9Ztq0aZimqetlCCV+J/Z4PNTU1BzQ9xUNtQXee+89Jk6cyK9//WtOPvlk3nvvvXQ36ZDl9/vx+/3RfzuOw4YNG/jJT37C8uXL09y6Q5dhGHFx0DWTOUKhEHV1dWRlZelaSSPTNFm8eHH08yvVNaLrZmglxiTx/pKTk5P02SaDLzEuqWKga2XoJcYlInKPGTVqlK6XIZb4mbVt27YD/r6ixJOe3pycnBwcxyE3N5f29vZ0N+mQ19HRwe7duznuuOO46667uPnmmzFNk9deey3dTTsklZaWxsWhpaVF10yGePnll/nyl7+cFCNdK+mV6r6ie01miNxfxowZo+smA6SKga6VzBG5x0DqWMngi3xmdXV1HfD3FSWeQE5ODp2dnVx11VV0dHSQk5OT7iYd0mzb5tFHH+XCCy/EMAyysrIAmDhxIrW1tWlu3aEpMQ6ArpkMsW7dOqZMmaJrJcOkuq/oXpN+sfcXSP5s03Uz9FLFQNdK5ojcY0DXSzrEfmYdDPcVJZ7A0UcfzZtvvgnAW2+9xdFHH53mFh26bNvmD3/4A2eeeSb5+fk4jhP920svvaTYpEliHMaPH69rJgPU1NRQUlISHZYeoWsl/VLdV3SvSa/E+wskf7YpJkMvVQx0rWSG2HsM6HoZaomfWQfDfUVVben5glBdXc1NN93EiBEjOPfcc9PdpEPW448/zsaNG6mpqQFg/PjxbNiwAcMwmDhxIscff3yaW3ho2r59O7///e/j4vDyyy/rmkmzVatWMXfuXCB1jGTomaaJaZop7yumaepekwaRmCTeXxYsWEBFRYWumzSJxKW/zy5dK+kRiQvE32NA95mhluozy7btA/q+Yriu66a7ESIiIiIiInLw0lBbERERERERGVRKPEVERERERGRQKfEUERERERGRQaXEU0RERERERAaVEk8REREREREZVEo8RUREREREZFBpHU8REZEh8N5773HPPfdQXl4O9KwhfeSRR7J06VLuvfdePB4PS5Ys4Xvf+x4zZ84E4Mknn+SVV17hjjvuiB7njTfe4M9//jOGYbB48eKMXzBcREQElHiKiIgMiXA4zNSpU7nssssAeOCBBwgEAhQXF1NfX4/H46GoqIhwOAzAww8/DIBt29FjBINBnnjiCW699Va6urr4+c9/zq233jrkr0VERGR/aaitiIhIGkyePJk//elPVFRUsGvXLhoaGhgxYkT07zNnzuSSSy6J22fz5s1MmDCBrKwsCgsLKSsro66ubqibLiIist+UeIqIiAyx7u5uXnjhBS644ALKy8tpaGigoaGBioqK6DZjxoxJ2m/nzp2Ul5fT2dlJd3c35eXl1NTUDGXTRUREPhUNtRURERkiL7/8Mlu3bsXj8TB79mwcx6GiooKamhosy4pLPFPp7OykuLiYNWvWkJubi9/vp6OjY4haLyIi8ukp8RQRERkiU6ZMic7xBHjhhRfw+XwEg0Esy8Ln8w24v9frpbu7G9M0MU2T7u5usrKyBrnVIiIin52G2oqIiKSZZVmY5t5vyWVlZdTX17Nw4UJmzZpFQ0MDw4cPH4IWioiIfDZKPEVERNLM5/PtU8/l+PHjee211wgGg7S2trJ161aOOOKIwW+giIjIZ6ShtiIiIkMgVa+maZpYlsX06dMxDIOampqkbWIT0vz8fM4880xuuOEGAC699FIsyxr8xouIiHxGhuu6brobISIiIiIiIgcvDbUVERERERGRQaXEU0RERERERAaVEk8REREREREZVEo8RUREREREZFAp8RQREREREZFBpcRTREREREREBpUSTxERERERERlU/x8IocyZu9B5EAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1116x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 독립변수 Xy의 col번호: 0=qty\n",
    "# ['qty', 'temp', 'cloud', 'wind', 'lgt_time', 'PM10', 'rain_or_not_비o', 'snow_or_not_눈o']\n",
    "n=5\n",
    "# alpha 값 0~1\n",
    "alp = 1\n",
    "# scatter plot 점 크기\n",
    "dot_size = 20\n",
    "\n",
    "matplotlib.rcParams['font.family'] = 'NanumGothicCoding'\n",
    "plt.figure(figsize=(15.5,7))\n",
    "plt.style.use('ggplot')\n",
    "plt.title('%s - %s vs 판매량' % (item, Xy.columns[n]) )\n",
    "plt.scatter(Xy.iloc[:,n],result_df.qty, label = '실 판매량', s=30, c='k', alpha = 1)\n",
    "plt.scatter(Xy.iloc[:,n],result_df.keras_qty, label = '케라스 신경망 예측', alpha=alp, s=dot_size)\n",
    "plt.scatter(Xy.iloc[:,n],result_df.rf_qty, label = 'RandomForest 예측', alpha=alp, s=dot_size)\n",
    "plt.scatter(Xy.iloc[:,n],result_df.xgb_qty, label = 'XGBoosting 예측', alpha=alp, s=dot_size)\n",
    "# plt.scatter(Xy.iloc[:,n],result_df.lin_qty, label = '선형 예측', alpha=alp, s=dot_size)\n",
    "# plt.scatter(Xy.iloc[:,n],result_df.ridge_qty, label = 'Ridge 예측', alpha=alp, s=dot_size)\n",
    "plt.scatter(Xy.iloc[:,n],result_df.ols_qty, label = 'OLS 예측', alpha=alp, s=dot_size)\n",
    "\n",
    "# X axis\n",
    "plt.xlabel('{}'.format(Xy.columns[n]))\n",
    "\n",
    "# y axis\n",
    "plt.ylabel('판매량')\n",
    "\n",
    "# 범례\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실험 구간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# 2016~2017 : 훈련 / 2018 검증 2:1\n",
    "trainXy = gs_week_w.loc[:cut_line]\n",
    "testXy = gs_week_w.loc[cut_line:]\n",
    "train_X =pd.DataFrame(trainXy.loc[:,'temp'])\n",
    "train_y = trainXy.loc[:,'qty']\n",
    "val_X = pd.DataFrame(testXy.loc[:,'temp'])\n",
    "val_y = testXy.loc[:,'qty']\n",
    "\n",
    "\n",
    "\n",
    "print('여기서 점수란 R-square값을 의미한다.')\n",
    "# RandomForest 회귀분석\n",
    "RFmodel = RandomForestRegressor()\n",
    "RFmodel.fit(train_X,train_y)\n",
    "# Get the mean absolute error on the validation data\n",
    "RFpredicted = RFmodel.predict(val_X)\n",
    "MAE = mean_absolute_error(val_y , RFpredicted)\n",
    "print('Random forest을 이용한 %s의 회귀분석 결과 :'%item)\n",
    "# print('Random forest validation MAE = ', MAE)\n",
    "print('훈련세트점수 : {:.3f}'.format(RFmodel.score(train_X, train_y)))\n",
    "print('검증세트점수 : {:.3f}'.format(RFmodel.score(val_X, val_y)))\n",
    "\n",
    "# XGBRegressor 회귀분석\n",
    "XGBModel = XGBRegressor(objective='reg:squarederror')\n",
    "XGBModel.fit(train_X,train_y , verbose=False)\n",
    "# Get the mean absolute error on the validation data :\n",
    "XGBpredictions = XGBModel.predict(val_X)\n",
    "MAE = mean_absolute_error(val_y , XGBpredictions)\n",
    "print('XGBoost을 이용한 %s의 회귀분석 결과 :'%item)\n",
    "# print('XGBoost validation MAE = ',MAE)\n",
    "print('훈련세트점수 : {:.3f}'.format(XGBModel.score(train_X, train_y)))\n",
    "print('검증세트점수 : {:.3f}'.format(XGBModel.score(val_X, val_y)))\n",
    "\n",
    "linReg = LinearRegression().fit(train_X, train_y)\n",
    "print('LinearRegression을 이용한 %s의 회귀분석 결과 :'%item)\n",
    "print('훈련세트점수 : {:.3f}'.format(linReg.score(train_X, train_y)))\n",
    "print('검증세트점수 : {:.3f}'.format(linReg.score(val_X, val_y)))\n",
    "\n",
    "ridge = Ridge(alpha=0.1, normalize=True, random_state=0, tol=0.001).fit(train_X, train_y)\n",
    "print('RidgeRegression을 이용한 %s의 회귀분석 결과 :'%item)\n",
    "print('훈련세트점수 : {:.3f}'.format(ridge.score(train_X, train_y)))\n",
    "print('검증세트점수 : {:.3f}'.format(ridge.score(val_X, val_y)))\n",
    "\n",
    "lasso = Lasso(alpha=0.1, max_iter=1000).fit(train_X, train_y)\n",
    "print('LassoRegression을 이용한 %s의 회귀분석 결과 :'%item)\n",
    "print('훈련세트점수 : {:.3f}'.format(lasso.score(train_X, train_y)) )\n",
    "print('검증세트점수 : {:.3f}'.format(lasso.score(val_X, val_y)) )\n",
    "\n",
    "customF = formulaGen(target='qty',ind_features=['temp'])\n",
    "olsModel = sm.OLS.from_formula(customF, data=trainXy).fit()\n",
    "print('OLS을 이용한 %s의 회귀분석 결과 :'%item)\n",
    "print('훈련세트점수 : {:.3f}'.format(olsModel.rsquared) )\n",
    "\n",
    "combined = pd.DataFrame(gs_week_w.loc[:,'temp'])\n",
    "target = gs_week_w.loc[:,'qty']\n",
    "\n",
    "# 가장 좋다고 판명된 설정을 이용하여, 예측qty생산\n",
    "# predictions = NN_model.predict(combined)\n",
    "# RandomForest 회귀분석 예측 qty생산\n",
    "RFpredicted = RFmodel.predict(combined)\n",
    "# XGBRegressor 회귀분석 예측 qty생산\n",
    "XGBpredictions = XGBModel.predict(combined)\n",
    "# linearRegression 회귀분석 예측 qty생산\n",
    "linPred = linReg.predict(combined)\n",
    "# Ridge 회귀분석 예측 qty생산\n",
    "ridPred = ridge.predict(combined)\n",
    "# Lasso 회귀분석 예측 qty생산\n",
    "lassoPred = lasso.predict(combined)\n",
    "# OLS 회귀분석 예측 qty생산\n",
    "olsPred = olsModel.predict(combined)\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "result_df['week'] = gs_week_w['week']\n",
    "result_df['qty'] = gs_week_w.loc[:,'qty']\n",
    "\n",
    "# print(\"keras 신경망 predictions\",predictions.shape)\n",
    "# result_df['keras_qty'] = predictions\n",
    "\n",
    "# print(\"randomforest 예상\",RFpredicted.shape)\n",
    "result_df['rf_qty'] = RFpredicted\n",
    "\n",
    "# print(\"XGBpredictions\",XGBpredictions.shape)\n",
    "result_df['xgb_qty'] = XGBpredictions\n",
    "\n",
    "# print(\"linearRegression 예상\",RFpredicted.shape)\n",
    "result_df['lin_qty'] = linPred\n",
    "\n",
    "# print(\"Ridge 예상\",RFpredicted.shape)\n",
    "result_df['ridge_qty'] = ridPred\n",
    "\n",
    "# print(\"Lasso 예상\",RFpredicted.shape)\n",
    "result_df['lasso_qty'] = lassoPred\n",
    "\n",
    "# print(\"OLS 예상\",RFpredicted.shape)\n",
    "result_df['ols_qty'] = olsPred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph = result_df.loc[:,['week','qty','keras_qty','rf_qty','xgb_qty','lin_qty','ridge_qty','lasso_qty','ols_qty']]\n",
    "for_visual_col = ['week','temp','cloud','wind','lgt_time','snow','rain','PM10']\n",
    "df = pd.merge(df_graph, gs_week_w[for_visual_col], on='week', how='left')\n",
    "# df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2016, 온도\n",
    "# df_graph = df.loc[df.week <= 53]\n",
    "# plt.figure(figsize=(15.5, 10))\n",
    "# plt.scatter(df_graph.temp,df_graph.qty, ls='-', label='실제', color='green', alpha=0.5, lw=8)\n",
    "# plt.scatter(df_graph.temp,df_graph.keras_qty, ls='-', label='keras예측', color='r')\n",
    "# plt.scatter(df_graph.temp,df_graph.rf_qty, ls='-', label='rf예측', color='cyan')\n",
    "# plt.scatter(df_graph.temp,df_graph.xgb_qty, ls='-', label='xgb예측', color='b')\n",
    "# plt.scatter(df_graph.temp,df_graph.lin_qty, ls='-', label='linear예측')\n",
    "# plt.scatter(df_graph.temp,df_graph.ridge_qty, ls='-', label='ridge예측')\n",
    "# plt.scatter(df_graph.temp,df_graph.lasso_qty, ls='-', label='lasso예측',color='yellow')\n",
    "# plt.scatter(df_graph.temp,df_graph.ols_qty, ls='-', label='ols예측', color='violet')\n",
    "# plt.legend()\n",
    "# plt.title('{}년도 {} 판매량 실제/예측'.format( '2016',item ))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qty_columns = list(df_graph.columns)[1:9]\n",
    "weather_columns = list(df_graph.columns)[9:]\n",
    "print(qty_columns)\n",
    "print(weather_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_temp = pd.DataFrame()\n",
    "# x_temp['temp'] = list(range(-10,35,1))\n",
    "x_temp['temp'] = np.arange(-9,35,0.5)\n",
    "combined = x_temp\n",
    "# RandomForest 회귀분석 예측 qty생산\n",
    "RFpredicted = RFmodel.predict(combined)\n",
    "# XGBRegressor 회귀분석 예측 qty생산\n",
    "XGBpredictions = XGBModel.predict(combined)\n",
    "# linearRegression 회귀분석 예측 qty생산\n",
    "linPred = linReg.predict(combined)\n",
    "# Ridge 회귀분석 예측 qty생산\n",
    "ridPred = ridge.predict(combined)\n",
    "# Lasso 회귀분석 예측 qty생산\n",
    "lassoPred = lasso.predict(combined)\n",
    "# OLS 회귀분석 예측 qty생산\n",
    "olsPred = olsModel.predict(combined)\n",
    "\n",
    "# 2016~2018, 일조시간\n",
    "df_graph = df.copy()\n",
    "plt.figure(figsize=(15.5, 10))\n",
    "plt.scatter(df_graph[weather_columns[0]],df_graph['qty'], ls='-', color='k',label='실 판매량', s=100, alpha=0.7)\n",
    "plt.plot(x_temp, RFpredicted, label = 'rf')\n",
    "plt.plot(x_temp, XGBpredictions, label = 'xgb')\n",
    "plt.plot(x_temp, linPred, label = 'line')\n",
    "plt.plot(x_temp, ridPred, label = 'ridge')\n",
    "plt.plot(x_temp, lassoPred, label = 'lasso')\n",
    "plt.plot(x_temp, olsPred, label = 'ols')\n",
    "plt.plot()\n",
    "plt.legend()\n",
    "plt.title('{}년도 {} 판매량 실제/예측'.format( '2016~2018',item ))\n",
    "plt.xlabel(weather_columns[0])\n",
    "plt.ylabel('판매량 (단위 : 1개)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept_lin = linReg.intercept_\n",
    "coef_line = linReg.coef_\n",
    "# list_col\n",
    "linePredict = list()\n",
    "x_temp = list(range(-10,38,1))\n",
    "for temperature in x_temp:\n",
    "    linePredict.append(intercept_lin + coef_line[0]*temperature)\n",
    "\n",
    "    \n",
    "# 2016~2018, 일조시간\n",
    "df_graph = df.copy()\n",
    "plt.figure(figsize=(15.5, 10))\n",
    "plt.scatter(df_graph[weather_columns[0]],df_graph['qty'], ls='-', color='k',label='실 판매량', s=100, alpha=0.3)\n",
    "# for q_name in qty_columns:\n",
    "#     plt.plot(df_graph[weather_columns[0]],df_graph[q_name], ls='-', label=q_name)\n",
    "#     plt.scatter(df_graph.lgt_time,df_graph[q_name], ls='-', label='실제', color='green', alpha=0.5, lw=8)\n",
    "#     plt.scatter(df_graph.lgt_time,df_graph.keras_qty, ls='-', label='keras예측', color='r')\n",
    "#     plt.scatter(df_graph.lgt_time,df_graph.rf_qty, ls='-', label='rf예측', color='cyan')\n",
    "#     plt.scatter(df_graph.lgt_time,df_graph.xgb_qty, ls='-', label='xgb예측', color='b')\n",
    "#     plt.scatter(df_graph.lgt_time,df_graph.lin_qty, ls='-', label='linear예측')\n",
    "#     plt.scatter(df_graph.lgt_time,df_graph.ridge_qty, ls='-', label='ridge예측')\n",
    "#     plt.scatter(df_graph.lgt_time,df_graph.lasso_qty, ls='-', label='lasso예측',color='yellow')\n",
    "#     plt.scatter(df_graph.lgt_time,df_graph.ols_qty, ls='-', label='ols예측', color='violet')\n",
    "plt.plot(x_temp, linePredict, 'r--', label='linear회귀, 온도만')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('{}년도 {} 판매량 실제/예측'.format( '2016~2018',item ))\n",
    "plt.xlabel(weather_columns[0])\n",
    "plt.ylabel('판매량 (단위 : 1개)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시간의 경과에 따른 예측량 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2016\n",
    "df_graph = result_df.loc[result_df.week <=53]\n",
    "plt.figure(figsize=(15.5, 10))\n",
    "plt.plot(df_graph.week,df_graph.qty, ls='-', label='실제', color='green', alpha=0.5, lw=8)\n",
    "plt.plot(df_graph.week,df_graph.keras_qty, ls='-', label='keras예측', color='r')\n",
    "plt.plot(df_graph.week,df_graph.rf_qty, ls='-', label='rf예측', color='cyan')\n",
    "plt.plot(df_graph.week,df_graph.xgb_qty, ls='-', label='xgb예측', color='b')\n",
    "plt.plot(df_graph.week,df_graph.lin_qty, ls='-', label='linear예측')\n",
    "plt.plot(df_graph.week,df_graph.ridge_qty, ls='-', label='ridge예측')\n",
    "plt.plot(df_graph.week,df_graph.lasso_qty, ls='-', label='lasso예측',color='yellow')\n",
    "plt.plot(df_graph.week,df_graph.ols_qty, ls='-', label='ols예측', color='violet')\n",
    "plt.legend()\n",
    "plt.title('{}년도 {} 판매량 실제/예측'.format( '2016',item ))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2017\n",
    "df_graph = result_df.loc[(result_df.week >=53)&(result_df.week <=105)]\n",
    "plt.figure(figsize=(15.5, 10))\n",
    "plt.plot(df_graph.week,df_graph.qty, ls='-', label='실제', color='green', alpha=0.5, lw=8)\n",
    "plt.plot(df_graph.week,df_graph.keras_qty, ls='-', label='keras예측', color='r')\n",
    "plt.plot(df_graph.week,df_graph.rf_qty, ls='-', label='rf예측', color='cyan')\n",
    "plt.plot(df_graph.week,df_graph.xgb_qty, ls='-', label='xgb예측', color='b')\n",
    "plt.plot(df_graph.week,df_graph.lin_qty, ls='-', label='linear예측')\n",
    "plt.plot(df_graph.week,df_graph.ridge_qty, ls='-', label='ridge예측')\n",
    "plt.plot(df_graph.week,df_graph.lasso_qty, ls='-', label='lasso예측',color='yellow')\n",
    "plt.plot(df_graph.week,df_graph.ols_qty, ls='-', label='ols예측', color='violet')\n",
    "plt.title('{}년도 {} 판매량 실제/예측'.format( '2017',item ))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2018\n",
    "df_graph = result_df.loc[(result_df.week >=105)]\n",
    "plt.figure(figsize=(15.5, 10))\n",
    "plt.plot(df_graph.week,df_graph.qty, ls='-', label='실제', color='green', alpha=0.5, lw=8)\n",
    "plt.plot(df_graph.week,df_graph.keras_qty, ls='-', label='keras예측', color='r')\n",
    "plt.plot(df_graph.week,df_graph.rf_qty, ls='-', label='rf예측', color='cyan')\n",
    "plt.plot(df_graph.week,df_graph.xgb_qty, ls='-', label='xgb예측', color='b')\n",
    "plt.plot(df_graph.week,df_graph.lin_qty, ls='-', label='linear예측')\n",
    "plt.plot(df_graph.week,df_graph.ridge_qty, ls='-', label='ridge예측')\n",
    "plt.plot(df_graph.week,df_graph.lasso_qty, ls='-', label='lasso예측',color='yellow')\n",
    "plt.plot(df_graph.week,df_graph.ols_qty, ls='-', label='ols예측', color='violet')\n",
    "plt.title('{}년도 {} 판매량 실제/예측'.format( '2018',item ))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2016~2018\n",
    "plt.figure(figsize=(15.5, 10))\n",
    "plt.plot(result_df.week,result_df.qty, ls='-', label='실제', color='green', alpha=0.5, lw=8)\n",
    "plt.plot(result_df.week,result_df.keras_qty, ls='-', label='keras예측', color='r')\n",
    "plt.plot(result_df.week,result_df.rf_qty, ls='-', label='rf예측', color='cyan')\n",
    "plt.plot(result_df.week,result_df.xgb_qty, ls='-', label='xgb예측', color='b')\n",
    "plt.plot(df_graph.week,df_graph.lin_qty, ls='-', label='linear예측')\n",
    "plt.plot(df_graph.week,df_graph.ridge_qty, ls='-', label='ridge예측')\n",
    "plt.plot(df_graph.week,df_graph.lasso_qty, ls='-', label='lasso예측',color='yellow')\n",
    "plt.plot(df_graph.week,df_graph.ols_qty, ls='-', label='ols예측', color='violet')\n",
    "plt.title('{}년도 {} 판매량 실제/예측'.format( '2016~2018',item ))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score(v_true, v_pred):\n",
    "    ssr = np.sum(np.square(v_pred - np.mean(v_true)))\n",
    "    sst = np.sum(np.square(v_true - np.mean(v_true)))\n",
    "    return ( ssr / sst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checked_for = '2016~2017'\n",
    "combined = aaaaa.loc[:106,'temp':'PM10']\n",
    "target = aaaaa.loc[:106,'qty']\n",
    "\n",
    "# 가장 좋다고 판명된 설정을 이용하여, 예측qty생산\n",
    "predictions = NN_model.predict(combined)\n",
    "# RandomForest 회귀분석 예측 qty생산\n",
    "RFpredicted = RFmodel.predict(combined)\n",
    "# XGBRegressor 회귀분석 예측 qty생산\n",
    "XGBpredictions = XGBModel.predict(combined)\n",
    "# linearRegression 회귀분석 예측 qty생산\n",
    "linPred = linReg.predict(combined)\n",
    "# Ridge 회귀분석 예측 qty생산\n",
    "ridPred = ridge.predict(combined)\n",
    "# Lasso 회귀분석 예측 qty생산\n",
    "lassoPred = lasso.predict(combined)\n",
    "# OLS 회귀분석 예측 qty생산\n",
    "olsPred = olsModel.predict(combined)\n",
    "qty = target\n",
    "\n",
    "print(checked_for)\n",
    "print('RF R2값  \\t: ','{:<.5f}'.format(r2_score(qty, RFpredicted)) )\n",
    "print('XGB R2값  \\t: ','{:<.5f}'.format(r2_score(qty, XGBpredictions)) )\n",
    "print('KerasNN R2값\\t: ','{:<.5f}'.format(r2_score(qty,predictions )) )\n",
    "print('LinReg R2값\\t: ','{:<.5f}'.format(r2_score(qty, linPred)) )\n",
    "print('Ridge R2값  \\t: ','{:<.5f}'.format(r2_score(qty, ridPred)) )\n",
    "print('Lasso R2값\\t: ','{:<.5f}'.format(r2_score(qty, lassoPred )) )\n",
    "print('OLS R2값\\t: ','{:<.5f}'.format(r2_score(qty, olsPred )) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checked_for = '2018'\n",
    "combined = aaaaa.loc[106:,'temp':'PM10']\n",
    "target = aaaaa.loc[106:,'qty']\n",
    "\n",
    "# 가장 좋다고 판명된 설정을 이용하여, 예측qty생산\n",
    "predictions = NN_model.predict(combined)\n",
    "# RandomForest 회귀분석 예측 qty생산\n",
    "RFpredicted = RFmodel.predict(combined)\n",
    "# XGBRegressor 회귀분석 예측 qty생산\n",
    "XGBpredictions = XGBModel.predict(combined)\n",
    "# linearRegression 회귀분석 예측 qty생산\n",
    "linPred = linReg.predict(combined)\n",
    "# Ridge 회귀분석 예측 qty생산\n",
    "ridPred = ridge.predict(combined)\n",
    "# Lasso 회귀분석 예측 qty생산\n",
    "lassoPred = lasso.predict(combined)\n",
    "# OLS 회귀분석 예측 qty생산\n",
    "olsPred = olsModel.predict(combined)\n",
    "\n",
    "qty = target\n",
    "\n",
    "print(checked_for)\n",
    "print('RF R2값  \\t: ','{:<.5f}'.format(r2_score(qty, RFpredicted)) )\n",
    "print('XGB R2값  \\t: ','{:<.5f}'.format(r2_score(qty, XGBpredictions)) )\n",
    "print('KerasNN R2값\\t: ','{:<.5f}'.format(r2_score(qty,predictions )) )\n",
    "print('LinReg R2값\\t: ','{:<.5f}'.format(r2_score(qty, linPred)) )\n",
    "print('Ridge R2값  \\t: ','{:<.5f}'.format(r2_score(qty, ridPred)) )\n",
    "print('Lasso R2값\\t: ','{:<.5f}'.format(r2_score(qty, lassoPred )) )\n",
    "print('OLS R2값\\t: ','{:<.5f}'.format(r2_score(qty, olsPred )) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checked_for = '2016~2018'\n",
    "combined = aaaaa.loc[:,'temp':'PM10']\n",
    "target = aaaaa.loc[:,'qty']\n",
    "\n",
    "# 가장 좋다고 판명된 설정을 이용하여, 예측qty생산\n",
    "predictions = NN_model.predict(combined)\n",
    "# RandomForest 회귀분석 예측 qty생산\n",
    "RFpredicted = RFmodel.predict(combined)\n",
    "# XGBRegressor 회귀분석 예측 qty생산\n",
    "XGBpredictions = XGBModel.predict(combined)\n",
    "# linearRegression 회귀분석 예측 qty생산\n",
    "linPred = linReg.predict(combined)\n",
    "# Ridge 회귀분석 예측 qty생산\n",
    "ridPred = ridge.predict(combined)\n",
    "# Lasso 회귀분석 예측 qty생산\n",
    "lassoPred = lasso.predict(combined)\n",
    "# OLS 회귀분석 예측 qty생산\n",
    "olsPred = olsModel.predict(combined)\n",
    "\n",
    "qty = target\n",
    "\n",
    "print(checked_for)\n",
    "print('RF R2값  \\t: ','{:<.5f}'.format(r2_score(qty, RFpredicted)) )\n",
    "print('XGB R2값  \\t: ','{:<.5f}'.format(r2_score(qty, XGBpredictions)) )\n",
    "print('KerasNN R2값\\t: ','{:<.5f}'.format(r2_score(qty,predictions )) )\n",
    "print('LinReg R2값\\t: ','{:<.5f}'.format(r2_score(qty, linPred)) )\n",
    "print('Ridge R2값  \\t: ','{:<.5f}'.format(r2_score(qty, ridPred)) )\n",
    "print('Lasso R2값\\t: ','{:<.5f}'.format(r2_score(qty, lassoPred )) )\n",
    "print('OLS R2값\\t: ','{:<.5f}'.format(r2_score(qty, olsPred )) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/project/contest/data/result/'\n",
    "result_df.to_csv(path+item+'_'+grouped_by+'_predict(lowVIF07).csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날씨+sns와 아이스크림 을 사용하면 2017까진 적당하고 2018년도는 예측율 급락\n",
    "#  -> 과적합화 발생\n",
    "# 날씨와 아이스크림 판매량만 이용하니 ㅇㅋ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
